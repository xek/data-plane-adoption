<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
<head>
<meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="viewport" content="width=device-width, initial-scale=1.0"/>
<meta name="generator" content="Asciidoctor 2.0.23"/>
<title>Adopting a Red Hat OpenStack Platform 17.1 deployment</title>
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300,300italic,400,400italic,600,600italic%7CNoto+Serif:400,400italic,700,700italic%7CDroid+Sans+Mono:400,700"/>
<style>
/*! Asciidoctor default stylesheet | MIT License | https://asciidoctor.org */
/* Uncomment the following line when using as a custom stylesheet */
/* @import "https://fonts.googleapis.com/css?family=Open+Sans:300,300italic,400,400italic,600,600italic%7CNoto+Serif:400,400italic,700,700italic%7CDroid+Sans+Mono:400,700"; */
html{font-family:sans-serif;-webkit-text-size-adjust:100%}
a{background:none}
a:focus{outline:thin dotted}
a:active,a:hover{outline:0}
h1{font-size:2em;margin:.67em 0}
b,strong{font-weight:bold}
abbr{font-size:.9em}
abbr[title]{cursor:help;border-bottom:1px dotted #dddddf;text-decoration:none}
dfn{font-style:italic}
hr{height:0}
mark{background:#ff0;color:#000}
code,kbd,pre,samp{font-family:monospace;font-size:1em}
pre{white-space:pre-wrap}
q{quotes:"\201C" "\201D" "\2018" "\2019"}
small{font-size:80%}
sub,sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline}
sup{top:-.5em}
sub{bottom:-.25em}
img{border:0}
svg:not(:root){overflow:hidden}
figure{margin:0}
audio,video{display:inline-block}
audio:not([controls]){display:none;height:0}
fieldset{border:1px solid silver;margin:0 2px;padding:.35em .625em .75em}
legend{border:0;padding:0}
button,input,select,textarea{font-family:inherit;font-size:100%;margin:0}
button,input{line-height:normal}
button,select{text-transform:none}
button,html input[type=button],input[type=reset],input[type=submit]{-webkit-appearance:button;cursor:pointer}
button[disabled],html input[disabled]{cursor:default}
input[type=checkbox],input[type=radio]{padding:0}
button::-moz-focus-inner,input::-moz-focus-inner{border:0;padding:0}
textarea{overflow:auto;vertical-align:top}
table{border-collapse:collapse;border-spacing:0}
*,::before,::after{box-sizing:border-box}
html,body{font-size:100%}
body{background:#fff;color:rgba(0,0,0,.8);padding:0;margin:0;font-family:"Noto Serif","DejaVu Serif",serif;line-height:1;position:relative;cursor:auto;-moz-tab-size:4;-o-tab-size:4;tab-size:4;word-wrap:anywhere;-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased}
a:hover{cursor:pointer}
img,object,embed{max-width:100%;height:auto}
object,embed{height:100%}
img{-ms-interpolation-mode:bicubic}
.left{float:left!important}
.right{float:right!important}
.text-left{text-align:left!important}
.text-right{text-align:right!important}
.text-center{text-align:center!important}
.text-justify{text-align:justify!important}
.hide{display:none}
img,object,svg{display:inline-block;vertical-align:middle}
textarea{height:auto;min-height:50px}
select{width:100%}
.subheader,.admonitionblock td.content>.title,.audioblock>.title,.exampleblock>.title,.imageblock>.title,.listingblock>.title,.literalblock>.title,.stemblock>.title,.openblock>.title,.paragraph>.title,.quoteblock>.title,table.tableblock>.title,.verseblock>.title,.videoblock>.title,.dlist>.title,.olist>.title,.ulist>.title,.qlist>.title,.hdlist>.title{line-height:1.45;color:#7a2518;font-weight:400;margin-top:0;margin-bottom:.25em}
div,dl,dt,dd,ul,ol,li,h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6,pre,form,p,blockquote,th,td{margin:0;padding:0}
a{color:#2156a5;text-decoration:underline;line-height:inherit}
a:hover,a:focus{color:#1d4b8f}
a img{border:0}
p{line-height:1.6;margin-bottom:1.25em;text-rendering:optimizeLegibility}
p aside{font-size:.875em;line-height:1.35;font-style:italic}
h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{font-family:"Open Sans","DejaVu Sans",sans-serif;font-weight:300;font-style:normal;color:#ba3925;text-rendering:optimizeLegibility;margin-top:1em;margin-bottom:.5em;line-height:1.0125em}
h1 small,h2 small,h3 small,#toctitle small,.sidebarblock>.content>.title small,h4 small,h5 small,h6 small{font-size:60%;color:#e99b8f;line-height:0}
h1{font-size:2.125em}
h2{font-size:1.6875em}
h3,#toctitle,.sidebarblock>.content>.title{font-size:1.375em}
h4,h5{font-size:1.125em}
h6{font-size:1em}
hr{border:solid #dddddf;border-width:1px 0 0;clear:both;margin:1.25em 0 1.1875em}
em,i{font-style:italic;line-height:inherit}
strong,b{font-weight:bold;line-height:inherit}
small{font-size:60%;line-height:inherit}
code{font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;font-weight:400;color:rgba(0,0,0,.9)}
ul,ol,dl{line-height:1.6;margin-bottom:1.25em;list-style-position:outside;font-family:inherit}
ul,ol{margin-left:1.5em}
ul li ul,ul li ol{margin-left:1.25em;margin-bottom:0}
ul.circle{list-style-type:circle}
ul.disc{list-style-type:disc}
ul.square{list-style-type:square}
ul.circle ul:not([class]),ul.disc ul:not([class]),ul.square ul:not([class]){list-style:inherit}
ol li ul,ol li ol{margin-left:1.25em;margin-bottom:0}
dl dt{margin-bottom:.3125em;font-weight:bold}
dl dd{margin-bottom:1.25em}
blockquote{margin:0 0 1.25em;padding:.5625em 1.25em 0 1.1875em;border-left:1px solid #ddd}
blockquote,blockquote p{line-height:1.6;color:rgba(0,0,0,.85)}
@media screen and (min-width:768px){h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{line-height:1.2}
h1{font-size:2.75em}
h2{font-size:2.3125em}
h3,#toctitle,.sidebarblock>.content>.title{font-size:1.6875em}
h4{font-size:1.4375em}}
table{background:#fff;margin-bottom:1.25em;border:1px solid #dedede;word-wrap:normal}
table thead,table tfoot{background:#f7f8f7}
table thead tr th,table thead tr td,table tfoot tr th,table tfoot tr td{padding:.5em .625em .625em;font-size:inherit;color:rgba(0,0,0,.8);text-align:left}
table tr th,table tr td{padding:.5625em .625em;font-size:inherit;color:rgba(0,0,0,.8)}
table tr.even,table tr.alt{background:#f8f8f7}
table thead tr th,table tfoot tr th,table tbody tr td,table tr td,table tfoot tr td{line-height:1.6}
h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{line-height:1.2;word-spacing:-.05em}
h1 strong,h2 strong,h3 strong,#toctitle strong,.sidebarblock>.content>.title strong,h4 strong,h5 strong,h6 strong{font-weight:400}
.center{margin-left:auto;margin-right:auto}
.stretch{width:100%}
.clearfix::before,.clearfix::after,.float-group::before,.float-group::after{content:" ";display:table}
.clearfix::after,.float-group::after{clear:both}
:not(pre).nobreak{word-wrap:normal}
:not(pre).nowrap{white-space:nowrap}
:not(pre).pre-wrap{white-space:pre-wrap}
:not(pre):not([class^=L])>code{font-size:.9375em;font-style:normal!important;letter-spacing:0;padding:.1em .5ex;word-spacing:-.15em;background:#f7f7f8;border-radius:4px;line-height:1.45;text-rendering:optimizeSpeed}
pre{color:rgba(0,0,0,.9);font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;line-height:1.45;text-rendering:optimizeSpeed}
pre code,pre pre{color:inherit;font-size:inherit;line-height:inherit}
pre>code{display:block}
pre.nowrap,pre.nowrap pre{white-space:pre;word-wrap:normal}
em em{font-style:normal}
strong strong{font-weight:400}
.keyseq{color:rgba(51,51,51,.8)}
kbd{font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;display:inline-block;color:rgba(0,0,0,.8);font-size:.65em;line-height:1.45;background:#f7f7f7;border:1px solid #ccc;border-radius:3px;box-shadow:0 1px 0 rgba(0,0,0,.2),inset 0 0 0 .1em #fff;margin:0 .15em;padding:.2em .5em;vertical-align:middle;position:relative;top:-.1em;white-space:nowrap}
.keyseq kbd:first-child{margin-left:0}
.keyseq kbd:last-child{margin-right:0}
.menuseq,.menuref{color:#000}
.menuseq b:not(.caret),.menuref{font-weight:inherit}
.menuseq{word-spacing:-.02em}
.menuseq b.caret{font-size:1.25em;line-height:.8}
.menuseq i.caret{font-weight:bold;text-align:center;width:.45em}
b.button::before,b.button::after{position:relative;top:-1px;font-weight:400}
b.button::before{content:"[";padding:0 3px 0 2px}
b.button::after{content:"]";padding:0 2px 0 3px}
p a>code:hover{color:rgba(0,0,0,.9)}
#header,#content,#footnotes,#footer{width:100%;margin:0 auto;max-width:62.5em;*zoom:1;position:relative;padding-left:.9375em;padding-right:.9375em}
#header::before,#header::after,#content::before,#content::after,#footnotes::before,#footnotes::after,#footer::before,#footer::after{content:" ";display:table}
#header::after,#content::after,#footnotes::after,#footer::after{clear:both}
#content{margin-top:1.25em}
#content::before{content:none}
#header>h1:first-child{color:rgba(0,0,0,.85);margin-top:2.25rem;margin-bottom:0}
#header>h1:first-child+#toc{margin-top:8px;border-top:1px solid #dddddf}
#header>h1:only-child{border-bottom:1px solid #dddddf;padding-bottom:8px}
#header .details{border-bottom:1px solid #dddddf;line-height:1.45;padding-top:.25em;padding-bottom:.25em;padding-left:.25em;color:rgba(0,0,0,.6);display:flex;flex-flow:row wrap}
#header .details span:first-child{margin-left:-.125em}
#header .details span.email a{color:rgba(0,0,0,.85)}
#header .details br{display:none}
#header .details br+span::before{content:"\00a0\2013\00a0"}
#header .details br+span.author::before{content:"\00a0\22c5\00a0";color:rgba(0,0,0,.85)}
#header .details br+span#revremark::before{content:"\00a0|\00a0"}
#header #revnumber{text-transform:capitalize}
#header #revnumber::after{content:"\00a0"}
#content>h1:first-child:not([class]){color:rgba(0,0,0,.85);border-bottom:1px solid #dddddf;padding-bottom:8px;margin-top:0;padding-top:1rem;margin-bottom:1.25rem}
#toc{border-bottom:1px solid #e7e7e9;padding-bottom:.5em}
#toc>ul{margin-left:.125em}
#toc ul.sectlevel0>li>a{font-style:italic}
#toc ul.sectlevel0 ul.sectlevel1{margin:.5em 0}
#toc ul{font-family:"Open Sans","DejaVu Sans",sans-serif;list-style-type:none}
#toc li{line-height:1.3334;margin-top:.3334em}
#toc a{text-decoration:none}
#toc a:active{text-decoration:underline}
#toctitle{color:#7a2518;font-size:1.2em}
@media screen and (min-width:768px){#toctitle{font-size:1.375em}
body.toc2{padding-left:15em;padding-right:0}
body.toc2 #header>h1:nth-last-child(2){border-bottom:1px solid #dddddf;padding-bottom:8px}
#toc.toc2{margin-top:0!important;background:#f8f8f7;position:fixed;width:15em;left:0;top:0;border-right:1px solid #e7e7e9;border-top-width:0!important;border-bottom-width:0!important;z-index:1000;padding:1.25em 1em;height:100%;overflow:auto}
#toc.toc2 #toctitle{margin-top:0;margin-bottom:.8rem;font-size:1.2em}
#toc.toc2>ul{font-size:.9em;margin-bottom:0}
#toc.toc2 ul ul{margin-left:0;padding-left:1em}
#toc.toc2 ul.sectlevel0 ul.sectlevel1{padding-left:0;margin-top:.5em;margin-bottom:.5em}
body.toc2.toc-right{padding-left:0;padding-right:15em}
body.toc2.toc-right #toc.toc2{border-right-width:0;border-left:1px solid #e7e7e9;left:auto;right:0}}
@media screen and (min-width:1280px){body.toc2{padding-left:20em;padding-right:0}
#toc.toc2{width:20em}
#toc.toc2 #toctitle{font-size:1.375em}
#toc.toc2>ul{font-size:.95em}
#toc.toc2 ul ul{padding-left:1.25em}
body.toc2.toc-right{padding-left:0;padding-right:20em}}
#content #toc{border:1px solid #e0e0dc;margin-bottom:1.25em;padding:1.25em;background:#f8f8f7;border-radius:4px}
#content #toc>:first-child{margin-top:0}
#content #toc>:last-child{margin-bottom:0}
#footer{max-width:none;background:rgba(0,0,0,.8);padding:1.25em}
#footer-text{color:hsla(0,0%,100%,.8);line-height:1.44}
#content{margin-bottom:.625em}
.sect1{padding-bottom:.625em}
@media screen and (min-width:768px){#content{margin-bottom:1.25em}
.sect1{padding-bottom:1.25em}}
.sect1:last-child{padding-bottom:0}
.sect1+.sect1{border-top:1px solid #e7e7e9}
#content h1>a.anchor,h2>a.anchor,h3>a.anchor,#toctitle>a.anchor,.sidebarblock>.content>.title>a.anchor,h4>a.anchor,h5>a.anchor,h6>a.anchor{position:absolute;z-index:1001;width:1.5ex;margin-left:-1.5ex;display:block;text-decoration:none!important;visibility:hidden;text-align:center;font-weight:400}
#content h1>a.anchor::before,h2>a.anchor::before,h3>a.anchor::before,#toctitle>a.anchor::before,.sidebarblock>.content>.title>a.anchor::before,h4>a.anchor::before,h5>a.anchor::before,h6>a.anchor::before{content:"\00A7";font-size:.85em;display:block;padding-top:.1em}
#content h1:hover>a.anchor,#content h1>a.anchor:hover,h2:hover>a.anchor,h2>a.anchor:hover,h3:hover>a.anchor,#toctitle:hover>a.anchor,.sidebarblock>.content>.title:hover>a.anchor,h3>a.anchor:hover,#toctitle>a.anchor:hover,.sidebarblock>.content>.title>a.anchor:hover,h4:hover>a.anchor,h4>a.anchor:hover,h5:hover>a.anchor,h5>a.anchor:hover,h6:hover>a.anchor,h6>a.anchor:hover{visibility:visible}
#content h1>a.link,h2>a.link,h3>a.link,#toctitle>a.link,.sidebarblock>.content>.title>a.link,h4>a.link,h5>a.link,h6>a.link{color:#ba3925;text-decoration:none}
#content h1>a.link:hover,h2>a.link:hover,h3>a.link:hover,#toctitle>a.link:hover,.sidebarblock>.content>.title>a.link:hover,h4>a.link:hover,h5>a.link:hover,h6>a.link:hover{color:#a53221}
details,.audioblock,.imageblock,.literalblock,.listingblock,.stemblock,.videoblock{margin-bottom:1.25em}
details{margin-left:1.25rem}
details>summary{cursor:pointer;display:block;position:relative;line-height:1.6;margin-bottom:.625rem;outline:none;-webkit-tap-highlight-color:transparent}
details>summary::-webkit-details-marker{display:none}
details>summary::before{content:"";border:solid transparent;border-left:solid;border-width:.3em 0 .3em .5em;position:absolute;top:.5em;left:-1.25rem;transform:translateX(15%)}
details[open]>summary::before{border:solid transparent;border-top:solid;border-width:.5em .3em 0;transform:translateY(15%)}
details>summary::after{content:"";width:1.25rem;height:1em;position:absolute;top:.3em;left:-1.25rem}
.admonitionblock td.content>.title,.audioblock>.title,.exampleblock>.title,.imageblock>.title,.listingblock>.title,.literalblock>.title,.stemblock>.title,.openblock>.title,.paragraph>.title,.quoteblock>.title,table.tableblock>.title,.verseblock>.title,.videoblock>.title,.dlist>.title,.olist>.title,.ulist>.title,.qlist>.title,.hdlist>.title{text-rendering:optimizeLegibility;text-align:left;font-family:"Noto Serif","DejaVu Serif",serif;font-size:1rem;font-style:italic}
table.tableblock.fit-content>caption.title{white-space:nowrap;width:0}
.paragraph.lead>p,#preamble>.sectionbody>[class=paragraph]:first-of-type p{font-size:1.21875em;line-height:1.6;color:rgba(0,0,0,.85)}
.admonitionblock>table{border-collapse:separate;border:0;background:none;width:100%}
.admonitionblock>table td.icon{text-align:center;width:80px}
.admonitionblock>table td.icon img{max-width:none}
.admonitionblock>table td.icon .title{font-weight:bold;font-family:"Open Sans","DejaVu Sans",sans-serif;text-transform:uppercase}
.admonitionblock>table td.content{padding-left:1.125em;padding-right:1.25em;border-left:1px solid #dddddf;color:rgba(0,0,0,.6);word-wrap:anywhere}
.admonitionblock>table td.content>:last-child>:last-child{margin-bottom:0}
.exampleblock>.content{border:1px solid #e6e6e6;margin-bottom:1.25em;padding:1.25em;background:#fff;border-radius:4px}
.sidebarblock{border:1px solid #dbdbd6;margin-bottom:1.25em;padding:1.25em;background:#f3f3f2;border-radius:4px}
.sidebarblock>.content>.title{color:#7a2518;margin-top:0;text-align:center}
.exampleblock>.content>:first-child,.sidebarblock>.content>:first-child{margin-top:0}
.exampleblock>.content>:last-child,.exampleblock>.content>:last-child>:last-child,.exampleblock>.content .olist>ol>li:last-child>:last-child,.exampleblock>.content .ulist>ul>li:last-child>:last-child,.exampleblock>.content .qlist>ol>li:last-child>:last-child,.sidebarblock>.content>:last-child,.sidebarblock>.content>:last-child>:last-child,.sidebarblock>.content .olist>ol>li:last-child>:last-child,.sidebarblock>.content .ulist>ul>li:last-child>:last-child,.sidebarblock>.content .qlist>ol>li:last-child>:last-child{margin-bottom:0}
.literalblock pre,.listingblock>.content>pre{border-radius:4px;overflow-x:auto;padding:1em;font-size:.8125em}
@media screen and (min-width:768px){.literalblock pre,.listingblock>.content>pre{font-size:.90625em}}
@media screen and (min-width:1280px){.literalblock pre,.listingblock>.content>pre{font-size:1em}}
.literalblock pre,.listingblock>.content>pre:not(.highlight),.listingblock>.content>pre[class=highlight],.listingblock>.content>pre[class^="highlight "]{background:#f7f7f8}
.literalblock.output pre{color:#f7f7f8;background:rgba(0,0,0,.9)}
.listingblock>.content{position:relative}
.listingblock code[data-lang]::before{display:none;content:attr(data-lang);position:absolute;font-size:.75em;top:.425rem;right:.5rem;line-height:1;text-transform:uppercase;color:inherit;opacity:.5}
.listingblock:hover code[data-lang]::before{display:block}
.listingblock.terminal pre .command::before{content:attr(data-prompt);padding-right:.5em;color:inherit;opacity:.5}
.listingblock.terminal pre .command:not([data-prompt])::before{content:"$"}
.listingblock pre.highlightjs{padding:0}
.listingblock pre.highlightjs>code{padding:1em;border-radius:4px}
.listingblock pre.prettyprint{border-width:0}
.prettyprint{background:#f7f7f8}
pre.prettyprint .linenums{line-height:1.45;margin-left:2em}
pre.prettyprint li{background:none;list-style-type:inherit;padding-left:0}
pre.prettyprint li code[data-lang]::before{opacity:1}
pre.prettyprint li:not(:first-child) code[data-lang]::before{display:none}
table.linenotable{border-collapse:separate;border:0;margin-bottom:0;background:none}
table.linenotable td[class]{color:inherit;vertical-align:top;padding:0;line-height:inherit;white-space:normal}
table.linenotable td.code{padding-left:.75em}
table.linenotable td.linenos,pre.pygments .linenos{border-right:1px solid;opacity:.35;padding-right:.5em;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}
pre.pygments span.linenos{display:inline-block;margin-right:.75em}
.quoteblock{margin:0 1em 1.25em 1.5em;display:table}
.quoteblock:not(.excerpt)>.title{margin-left:-1.5em;margin-bottom:.75em}
.quoteblock blockquote,.quoteblock p{color:rgba(0,0,0,.85);font-size:1.15rem;line-height:1.75;word-spacing:.1em;letter-spacing:0;font-style:italic;text-align:justify}
.quoteblock blockquote{margin:0;padding:0;border:0}
.quoteblock blockquote::before{content:"\201c";float:left;font-size:2.75em;font-weight:bold;line-height:.6em;margin-left:-.6em;color:#7a2518;text-shadow:0 1px 2px rgba(0,0,0,.1)}
.quoteblock blockquote>.paragraph:last-child p{margin-bottom:0}
.quoteblock .attribution{margin-top:.75em;margin-right:.5ex;text-align:right}
.verseblock{margin:0 1em 1.25em}
.verseblock pre{font-family:"Open Sans","DejaVu Sans",sans-serif;font-size:1.15rem;color:rgba(0,0,0,.85);font-weight:300;text-rendering:optimizeLegibility}
.verseblock pre strong{font-weight:400}
.verseblock .attribution{margin-top:1.25rem;margin-left:.5ex}
.quoteblock .attribution,.verseblock .attribution{font-size:.9375em;line-height:1.45;font-style:italic}
.quoteblock .attribution br,.verseblock .attribution br{display:none}
.quoteblock .attribution cite,.verseblock .attribution cite{display:block;letter-spacing:-.025em;color:rgba(0,0,0,.6)}
.quoteblock.abstract blockquote::before,.quoteblock.excerpt blockquote::before,.quoteblock .quoteblock blockquote::before{display:none}
.quoteblock.abstract blockquote,.quoteblock.abstract p,.quoteblock.excerpt blockquote,.quoteblock.excerpt p,.quoteblock .quoteblock blockquote,.quoteblock .quoteblock p{line-height:1.6;word-spacing:0}
.quoteblock.abstract{margin:0 1em 1.25em;display:block}
.quoteblock.abstract>.title{margin:0 0 .375em;font-size:1.15em;text-align:center}
.quoteblock.excerpt>blockquote,.quoteblock .quoteblock{padding:0 0 .25em 1em;border-left:.25em solid #dddddf}
.quoteblock.excerpt,.quoteblock .quoteblock{margin-left:0}
.quoteblock.excerpt blockquote,.quoteblock.excerpt p,.quoteblock .quoteblock blockquote,.quoteblock .quoteblock p{color:inherit;font-size:1.0625rem}
.quoteblock.excerpt .attribution,.quoteblock .quoteblock .attribution{color:inherit;font-size:.85rem;text-align:left;margin-right:0}
p.tableblock:last-child{margin-bottom:0}
td.tableblock>.content{margin-bottom:1.25em;word-wrap:anywhere}
td.tableblock>.content>:last-child{margin-bottom:-1.25em}
table.tableblock,th.tableblock,td.tableblock{border:0 solid #dedede}
table.grid-all>*>tr>*{border-width:1px}
table.grid-cols>*>tr>*{border-width:0 1px}
table.grid-rows>*>tr>*{border-width:1px 0}
table.frame-all{border-width:1px}
table.frame-ends{border-width:1px 0}
table.frame-sides{border-width:0 1px}
table.frame-none>colgroup+*>:first-child>*,table.frame-sides>colgroup+*>:first-child>*{border-top-width:0}
table.frame-none>:last-child>:last-child>*,table.frame-sides>:last-child>:last-child>*{border-bottom-width:0}
table.frame-none>*>tr>:first-child,table.frame-ends>*>tr>:first-child{border-left-width:0}
table.frame-none>*>tr>:last-child,table.frame-ends>*>tr>:last-child{border-right-width:0}
table.stripes-all>*>tr,table.stripes-odd>*>tr:nth-of-type(odd),table.stripes-even>*>tr:nth-of-type(even),table.stripes-hover>*>tr:hover{background:#f8f8f7}
th.halign-left,td.halign-left{text-align:left}
th.halign-right,td.halign-right{text-align:right}
th.halign-center,td.halign-center{text-align:center}
th.valign-top,td.valign-top{vertical-align:top}
th.valign-bottom,td.valign-bottom{vertical-align:bottom}
th.valign-middle,td.valign-middle{vertical-align:middle}
table thead th,table tfoot th{font-weight:bold}
tbody tr th{background:#f7f8f7}
tbody tr th,tbody tr th p,tfoot tr th,tfoot tr th p{color:rgba(0,0,0,.8);font-weight:bold}
p.tableblock>code:only-child{background:none;padding:0}
p.tableblock{font-size:1em}
ol{margin-left:1.75em}
ul li ol{margin-left:1.5em}
dl dd{margin-left:1.125em}
dl dd:last-child,dl dd:last-child>:last-child{margin-bottom:0}
li p,ul dd,ol dd,.olist .olist,.ulist .ulist,.ulist .olist,.olist .ulist{margin-bottom:.625em}
ul.checklist,ul.none,ol.none,ul.no-bullet,ol.no-bullet,ol.unnumbered,ul.unstyled,ol.unstyled{list-style-type:none}
ul.no-bullet,ol.no-bullet,ol.unnumbered{margin-left:.625em}
ul.unstyled,ol.unstyled{margin-left:0}
li>p:empty:only-child::before{content:"";display:inline-block}
ul.checklist>li>p:first-child{margin-left:-1em}
ul.checklist>li>p:first-child>.fa-square-o:first-child,ul.checklist>li>p:first-child>.fa-check-square-o:first-child{width:1.25em;font-size:.8em;position:relative;bottom:.125em}
ul.checklist>li>p:first-child>input[type=checkbox]:first-child{margin-right:.25em}
ul.inline{display:flex;flex-flow:row wrap;list-style:none;margin:0 0 .625em -1.25em}
ul.inline>li{margin-left:1.25em}
.unstyled dl dt{font-weight:400;font-style:normal}
ol.arabic{list-style-type:decimal}
ol.decimal{list-style-type:decimal-leading-zero}
ol.loweralpha{list-style-type:lower-alpha}
ol.upperalpha{list-style-type:upper-alpha}
ol.lowerroman{list-style-type:lower-roman}
ol.upperroman{list-style-type:upper-roman}
ol.lowergreek{list-style-type:lower-greek}
.hdlist>table,.colist>table{border:0;background:none}
.hdlist>table>tbody>tr,.colist>table>tbody>tr{background:none}
td.hdlist1,td.hdlist2{vertical-align:top;padding:0 .625em}
td.hdlist1{font-weight:bold;padding-bottom:1.25em}
td.hdlist2{word-wrap:anywhere}
.literalblock+.colist,.listingblock+.colist{margin-top:-.5em}
.colist td:not([class]):first-child{padding:.4em .75em 0;line-height:1;vertical-align:top}
.colist td:not([class]):first-child img{max-width:none}
.colist td:not([class]):last-child{padding:.25em 0}
.thumb,.th{line-height:0;display:inline-block;border:4px solid #fff;box-shadow:0 0 0 1px #ddd}
.imageblock.left{margin:.25em .625em 1.25em 0}
.imageblock.right{margin:.25em 0 1.25em .625em}
.imageblock>.title{margin-bottom:0}
.imageblock.thumb,.imageblock.th{border-width:6px}
.imageblock.thumb>.title,.imageblock.th>.title{padding:0 .125em}
.image.left,.image.right{margin-top:.25em;margin-bottom:.25em;display:inline-block;line-height:0}
.image.left{margin-right:.625em}
.image.right{margin-left:.625em}
a.image{text-decoration:none;display:inline-block}
a.image object{pointer-events:none}
sup.footnote,sup.footnoteref{font-size:.875em;position:static;vertical-align:super}
sup.footnote a,sup.footnoteref a{text-decoration:none}
sup.footnote a:active,sup.footnoteref a:active,#footnotes .footnote a:first-of-type:active{text-decoration:underline}
#footnotes{padding-top:.75em;padding-bottom:.75em;margin-bottom:.625em}
#footnotes hr{width:20%;min-width:6.25em;margin:-.25em 0 .75em;border-width:1px 0 0}
#footnotes .footnote{padding:0 .375em 0 .225em;line-height:1.3334;font-size:.875em;margin-left:1.2em;margin-bottom:.2em}
#footnotes .footnote a:first-of-type{font-weight:bold;text-decoration:none;margin-left:-1.05em}
#footnotes .footnote:last-of-type{margin-bottom:0}
#content #footnotes{margin-top:-.625em;margin-bottom:0;padding:.75em 0}
div.unbreakable{page-break-inside:avoid}
.big{font-size:larger}
.small{font-size:smaller}
.underline{text-decoration:underline}
.overline{text-decoration:overline}
.line-through{text-decoration:line-through}
.aqua{color:#00bfbf}
.aqua-background{background:#00fafa}
.black{color:#000}
.black-background{background:#000}
.blue{color:#0000bf}
.blue-background{background:#0000fa}
.fuchsia{color:#bf00bf}
.fuchsia-background{background:#fa00fa}
.gray{color:#606060}
.gray-background{background:#7d7d7d}
.green{color:#006000}
.green-background{background:#007d00}
.lime{color:#00bf00}
.lime-background{background:#00fa00}
.maroon{color:#600000}
.maroon-background{background:#7d0000}
.navy{color:#000060}
.navy-background{background:#00007d}
.olive{color:#606000}
.olive-background{background:#7d7d00}
.purple{color:#600060}
.purple-background{background:#7d007d}
.red{color:#bf0000}
.red-background{background:#fa0000}
.silver{color:#909090}
.silver-background{background:#bcbcbc}
.teal{color:#006060}
.teal-background{background:#007d7d}
.white{color:#bfbfbf}
.white-background{background:#fafafa}
.yellow{color:#bfbf00}
.yellow-background{background:#fafa00}
span.icon>.fa{cursor:default}
a span.icon>.fa{cursor:inherit}
.admonitionblock td.icon [class^="fa icon-"]{font-size:2.5em;text-shadow:1px 1px 2px rgba(0,0,0,.5);cursor:default}
.admonitionblock td.icon .icon-note::before{content:"\f05a";color:#19407c}
.admonitionblock td.icon .icon-tip::before{content:"\f0eb";text-shadow:1px 1px 2px rgba(155,155,0,.8);color:#111}
.admonitionblock td.icon .icon-warning::before{content:"\f071";color:#bf6900}
.admonitionblock td.icon .icon-caution::before{content:"\f06d";color:#bf3400}
.admonitionblock td.icon .icon-important::before{content:"\f06a";color:#bf0000}
.conum[data-value]{display:inline-block;color:#fff!important;background:rgba(0,0,0,.8);border-radius:50%;text-align:center;font-size:.75em;width:1.67em;height:1.67em;line-height:1.67em;font-family:"Open Sans","DejaVu Sans",sans-serif;font-style:normal;font-weight:bold}
.conum[data-value] *{color:#fff!important}
.conum[data-value]+b{display:none}
.conum[data-value]::after{content:attr(data-value)}
pre .conum[data-value]{position:relative;top:-.125em}
b.conum *{color:inherit!important}
.conum:not([data-value]):empty{display:none}
dt,th.tableblock,td.content,div.footnote{text-rendering:optimizeLegibility}
h1,h2,p,td.content,span.alt,summary{letter-spacing:-.01em}
p strong,td.content strong,div.footnote strong{letter-spacing:-.005em}
p,blockquote,dt,td.content,td.hdlist1,span.alt,summary{font-size:1.0625rem}
p{margin-bottom:1.25rem}
.sidebarblock p,.sidebarblock dt,.sidebarblock td.content,p.tableblock{font-size:1em}
.exampleblock>.content{background:#fffef7;border-color:#e0e0dc;box-shadow:0 1px 4px #e0e0dc}
.print-only{display:none!important}
@page{margin:1.25cm .75cm}
@media print{*{box-shadow:none!important;text-shadow:none!important}
html{font-size:80%}
a{color:inherit!important;text-decoration:underline!important}
a.bare,a[href^="#"],a[href^="mailto:"]{text-decoration:none!important}
a[href^="http:"]:not(.bare)::after,a[href^="https:"]:not(.bare)::after{content:"(" attr(href) ")";display:inline-block;font-size:.875em;padding-left:.25em}
abbr[title]{border-bottom:1px dotted}
abbr[title]::after{content:" (" attr(title) ")"}
pre,blockquote,tr,img,object,svg{page-break-inside:avoid}
thead{display:table-header-group}
svg{max-width:100%}
p,blockquote,dt,td.content{font-size:1em;orphans:3;widows:3}
h2,h3,#toctitle,.sidebarblock>.content>.title{page-break-after:avoid}
#header,#content,#footnotes,#footer{max-width:none}
#toc,.sidebarblock,.exampleblock>.content{background:none!important}
#toc{border-bottom:1px solid #dddddf!important;padding-bottom:0!important}
body.book #header{text-align:center}
body.book #header>h1:first-child{border:0!important;margin:2.5em 0 1em}
body.book #header .details{border:0!important;display:block;padding:0!important}
body.book #header .details span:first-child{margin-left:0!important}
body.book #header .details br{display:block}
body.book #header .details br+span::before{content:none!important}
body.book #toc{border:0!important;text-align:left!important;padding:0!important;margin:0!important}
body.book #toc,body.book #preamble,body.book h1.sect0,body.book .sect1>h2{page-break-before:always}
.listingblock code[data-lang]::before{display:block}
#footer{padding:0 .9375em}
.hide-on-print{display:none!important}
.print-only{display:block!important}
.hide-for-print{display:none!important}
.show-for-print{display:inherit!important}}
@media amzn-kf8,print{#header>h1:first-child{margin-top:1.25rem}
.sect1{padding:0!important}
.sect1+.sect1{border:0}
#footer{background:none}
#footer-text{color:rgba(0,0,0,.6);font-size:.9em}}
@media amzn-kf8{#header,#content,#footnotes,#footer{padding:0}}
</style>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"/>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.3/styles/monokai.min.css"/>
</head>
<body class="book toc2 toc-left">
<div id="header">
<h1>Adopting a Red Hat OpenStack Platform 17.1 deployment</h1>
<div id="toc" class="toc2">
<div id="toctitle">Table of Contents</div>
<ul class="sectlevel1">
<li><a href="#rhoso-180-adoption-overview_assembly">Red&#160;Hat OpenStack Services on OpenShift 18.0 adoption overview</a>
<ul class="sectlevel2">
<li><a href="#adoption-limitations_planning">Adoption limitations</a></li>
<li><a href="#adoption-prerequisites_planning">Adoption prerequisites</a></li>
<li><a href="#adoption-guidelines_planning">Guidelines for planning the adoption</a></li>
<li><a href="#adoption-process-overview_planning">Adoption process overview</a></li>
<li><a href="#installing-the-systemd-container-package-on-compute-hosts_planning">Installing the <code>systemd-container</code> package on Compute hosts</a></li>
<li><a href="#identity-service-authentication_planning">Identity service authentication</a></li>
<li><a href="#configuring-network-for-RHOSO-deployment_planning">Configuring the network for the Red&#160;Hat OpenStack Services on OpenShift deployment</a>
<ul class="sectlevel3">
<li><a href="#retrieving-the-network-configuration_configuring-network">Retrieving the network configuration from your existing deployment</a></li>
<li><a href="#planning-your-ipam-configuration_configuring-network">Planning your IPAM configuration</a></li>
<li><a href="#configuring-isolated-networks_configuring-network">Configuring isolated networks</a></li>
</ul>
</li>
<li><a href="#storage-requirements_configuring-network">Storage requirements</a>
<ul class="sectlevel3">
<li><a href="#storage-driver-certification_storage-requirements">Storage driver certification</a></li>
<li><a href="#block-storage-requirements_storage-requirements">Block Storage service guidelines</a></li>
<li><a href="#block-storage-limitations_storage-requirements">Limitations for adopting the Block Storage service</a></li>
<li><a href="#openshift-preparation-for-block-storage-adoption_storage-requirements">RHOCP preparation for Block Storage service adoption</a></li>
<li><a href="#preparing-block-storage-by-customizing-configuration_storage-requirements">Converting the Block Storage service configuration</a></li>
<li><a href="#changes-to-cephFS-through-NFS_storage-requirements">Changes to CephFS through NFS</a></li>
</ul>
</li>
<li><a href="#red-hat-ceph-storage-prerequisites_configuring-network">Red Hat Ceph Storage prerequisites</a>
<ul class="sectlevel3">
<li><a href="#completing-prerequisites-for-migrating-ceph-monitoring-stack_ceph-prerequisites">Completing prerequisites for a Red Hat Ceph Storage cluster with monitoring stack components</a></li>
<li><a href="#completing-prerequisites-for-migrating-ceph-rgw_ceph-prerequisites">Completing prerequisites for Red Hat Ceph Storage RGW migration</a></li>
<li><a href="#completing-prerequisites-for-rbd-migration_ceph-prerequisites">Completing prerequisites for a Red Hat Ceph Storage RBD migration</a></li>
<li><a href="#creating-a-ceph-nfs-cluster_ceph-prerequisites">Creating an NFS Ganesha cluster</a></li>
</ul>
</li>
<li><a href="#comparing-configuration-files-between-deployments_configuring-network">Comparing configuration files between deployments</a></li>
</ul>
</li>
<li><a href="#migrating-tls-everywhere_configuring-network">Migrating TLS-e to the RHOSO deployment</a></li>
<li><a href="#migrating-databases-to-the-control-plane_configuring-network">Migrating databases to the control plane</a>
<ul class="sectlevel2">
<li><a href="#proc_retrieving-topology-specific-service-configuration_migrating-databases">Retrieving topology-specific service configuration</a></li>
<li><a href="#deploying-backend-services_migrating-databases">Deploying back-end services</a></li>
<li><a href="#configuring-a-ceph-backend_migrating-databases">Configuring a Red Hat Ceph Storage back end</a></li>
<li><a href="#stopping-openstack-services_migrating-databases">Stopping Red&#160;Hat OpenStack Platform services</a></li>
<li><a href="#migrating-databases-to-mariadb-instances_migrating-databases">Migrating databases to MariaDB instances</a></li>
<li><a href="#migrating-ovn-data_migrating-databases">Migrating OVN data</a></li>
</ul>
</li>
<li><a href="#adopting-openstack-control-plane-services_configuring-network">Adopting Red&#160;Hat OpenStack Platform control plane services</a>
<ul class="sectlevel2">
<li><a href="#adopting-the-identity-service_adopt-control-plane">Adopting the Identity service</a></li>
<li><a href="#adopting-the-key-manager-service_adopt-control-plane">Adopting the Key Manager service</a></li>
<li><a href="#adopting-the-networking-service_adopt-control-plane">Adopting the Networking service</a></li>
<li><a href="#adopting-the-object-storage-service_adopt-control-plane">Adopting the Object Storage service</a></li>
<li><a href="#adopting-the-image-service_adopt-control-plane">Adopting the Image service</a>
<ul class="sectlevel3">
<li><a href="#adopting-image-service-with-object-storage-backend_image-service">Adopting the Image service that is deployed with a Object Storage service back end</a></li>
<li><a href="#adopting-image-service-with-block-storage-backend_image-service">Adopting the Image service that is deployed with a Block Storage service back end</a></li>
<li><a href="#adopting-image-service-with-nfs-backend_image-service">Adopting the Image service that is deployed with an NFS back end</a></li>
<li><a href="#adopting-image-service-with-ceph-backend_image-service">Adopting the Image service that is deployed with a Red Hat Ceph Storage back end</a></li>
<li><a href="#verifying-the-image-service-adoption_image-service">Verifying the Image service adoption</a></li>
</ul>
</li>
<li><a href="#adopting-the-placement-service_adopt-control-plane">Adopting the Placement service</a></li>
<li><a href="#adopting-the-bare-metal-provisioning-service_adopt-control-plane">Adopting the Bare Metal Provisioning service</a>
<ul class="sectlevel3">
<li><a href="#con_bare-metal-provisioning-service-configurations_adopting-bare-metal-provisioning">Bare Metal Provisioning service configurations</a></li>
<li><a href="#deploying-the-bare-metal-provisioning-service_adopting-bare-metal-provisioning">Deploying the Bare Metal Provisioning service</a></li>
</ul>
</li>
<li><a href="#adopting-the-compute-service_adopt-control-plane">Adopting the Compute service</a></li>
<li><a href="#adopting-the-block-storage-service_adopt-control-plane">Adopting the Block Storage service</a></li>
<li><a href="#adopting-the-openstack-dashboard_adopt-control-plane">Adopting the Dashboard service</a></li>
<li><a href="#adopting-the-shared-file-systems-service_adopt-control-plane">Adopting the Shared File Systems service</a>
<ul class="sectlevel3">
<li><a href="#preparing-the-shared-file-systems-service-configuration_adopting-shared-file-systems">Guidelines for preparing the Shared File Systems service configuration</a></li>
<li><a href="#deploying-file-systems-service-control-plane_adopting-shared-file-systems">Deploying the Shared File Systems service on the control plane</a></li>
<li><a href="#decommissioning-RHOSP-standalone-Ceph-NFS-service_adopting-shared-file-systems">Decommissioning the Red&#160;Hat OpenStack Platform standalone Ceph NFS service</a></li>
</ul>
</li>
<li><a href="#adopting-the-orchestration-service_adopt-control-plane">Adopting the Orchestration service</a></li>
<li><a href="#adopting-the-loadbalancer-service_adopt-control-plane">Adopting the Load-balancing service</a></li>
<li><a href="#adopting-telemetry-services_adopt-control-plane">Adopting Telemetry services</a></li>
<li><a href="#adopting-autoscaling_adopt-control-plane">Adopting autoscaling services</a></li>
<li><a href="#pulling-configuration-from-tripleo-deployment_adopt-control-plane">Pulling the configuration from a director deployment</a></li>
<li><a href="#rolling-back-control-plane-adoption_adopt-control-plane">Rolling back the control plane adoption</a></li>
</ul>
</li>
<li><a href="#adopting-data-plane_adopt-control-plane">Adopting the data plane</a>
<ul class="sectlevel2">
<li><a href="#stopping-infrastructure-management-and-compute-services_data-plane">Stopping infrastructure management and Compute services</a></li>
<li><a href="#adopting-compute-services-to-the-data-plane_data-plane">Adopting Compute services to the RHOSO data plane</a></li>
<li><a href="#performing-a-fast-forward-upgrade-on-compute-services_data-plane">Performing a fast-forward upgrade on Compute services</a></li>
<li><a href="#adopting-networker-services-to-the-data-plane_data-plane">Adopting Networker services to the RHOSO data plane</a></li>
</ul>
</li>
<li><a href="#migrating-the-object-storage-service_adopt-control-plane">Migrating the Object Storage service to Red&#160;Hat OpenStack Services on OpenShift nodes</a>
<ul class="sectlevel2">
<li><a href="#migrating-object-storage-data-to-rhoso-nodes_migrate-object-storage-service">Migrating the Object Storage service data from RHOSP to RHOSO nodes</a></li>
<li><a href="#troubleshooting-object-storage-migration_migrate-object-storage-service">Troubleshooting the Object Storage service migration</a></li>
</ul>
</li>
<li><a href="#ceph-migration_adopt-control-plane">Migrating the Red Hat Ceph Storage cluster</a>
<ul class="sectlevel2">
<li><a href="#ceph-daemon-cardinality_migrating-ceph">Red Hat Ceph Storage daemon cardinality</a></li>
<li><a href="#migrating-ceph-monitoring_migrating-ceph">Migrating the monitoring stack component to new nodes within an existing Red Hat Ceph Storage cluster</a>
<ul class="sectlevel3">
<li><a href="#migrating-monitoring-stack-to-target-nodes_migrating-ceph-monitoring">Migrating the monitoring stack to the target nodes</a></li>
</ul>
</li>
<li><a href="#migrating-ceph-mds_migrating-ceph-monitoring">Migrating Red Hat Ceph Storage MDS to new nodes within the existing cluster</a></li>
<li><a href="#migrating-ceph-rgw_migrating-ceph-monitoring">Migrating Red Hat Ceph Storage RGW to external RHEL nodes</a>
<ul class="sectlevel3">
<li><a href="#migrating-the-rgw-backends_migrating-ceph-rgw">Migrating the Red Hat Ceph Storage RGW back ends</a></li>
<li><a href="#deploying-a-ceph-ingress-daemon_migrating-ceph-rgw">Deploying a Red Hat Ceph Storage ingress daemon</a></li>
<li><a href="#updating-the-object-storage-endpoints_migrating-ceph-rgw">Updating the Object Storage service endpoints</a></li>
</ul>
</li>
<li><a href="#migrating-ceph-rbd_migrating-ceph-monitoring">Migrating Red Hat Ceph Storage RBD to external RHEL nodes</a>
<ul class="sectlevel3">
<li><a href="#migrating-ceph-mgr-daemons-to-ceph-nodes_migrating-ceph-rbd">Migrating Ceph Manager daemons to Red Hat Ceph Storage nodes</a></li>
<li><a href="#migrating-mon-from-controller-nodes_migrating-ceph-rbd">Migrating Ceph Monitor daemons to Red Hat Ceph Storage nodes</a></li>
</ul>
</li>
<li><a href="#updating-the-cluster-dashboard-configuration_migrating-ceph-rbd">Updating the Red Hat Ceph Storage cluster Ceph Dashboard configuration</a></li>
</ul>
</li>
</ul>
</div>
</div>
<div id="content">
<div class="sect1">
<h2 id="rhoso-180-adoption-overview_assembly">Red&#160;Hat OpenStack Services on OpenShift 18.0 adoption overview</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Adoption is the process of migrating a Red&#160;Hat OpenStack Platform (RHOSP) 17.1 overcloud to a Red&#160;Hat OpenStack Services on OpenShift 18.0 data plane. To ensure that you understand the entire adoption process and how to sufficiently prepare your RHOSP environment, review the prerequisites, adoption process, and post-adoption tasks.</p>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
It is important to read the whole adoption guide before you start
the adoption. You should form an understanding of the procedure,
prepare the necessary configuration snippets for each service ahead of
time, and test the procedure in a representative test environment
before you adopt your main environment.
</td>
</tr>
</table>
</div>
<div class="sect2">
<h3 id="adoption-limitations_planning">Adoption limitations</h3>
<div class="paragraph">
<p>Before you proceed with the adoption, check which features are Technology Previews or unsupported.</p>
</div>
<div class="paragraph">
<div class="title">Technology Preview</div>
<p>The following features are Technology Previews and have not been tested within the context of the Red&#160;Hat OpenStack Services on OpenShift adoption:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>NFS Ganesha back end for Shared File Systems service (manila)</p>
</li>
<li>
<p>iSCSI, NFS, and FC-based drivers for Block Storage service (cinder)</p>
</li>
<li>
<p>Block Storage service back end for the Image Service (glance)</p>
</li>
<li>
<p>NFS back end for the Image service</p>
</li>
<li>
<p>Third-party drivers for the Shared File Systems service</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The following Compute service (nova) features are Technology Previews:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Compute hosts with <code>/var/lib/nova/instances</code> on NFS</p>
</li>
<li>
<p>NUMA-aware vswitches</p>
</li>
<li>
<p>PCI passthrough by flavor</p>
</li>
<li>
<p>SR-IOV trusted virtual functions</p>
</li>
<li>
<p>RX and TX queue sizes</p>
</li>
<li>
<p>vGPU</p>
</li>
<li>
<p>Virtio multiqueue</p>
</li>
<li>
<p>Emulated virtual Trusted Platform Module (vTPM)</p>
</li>
<li>
<p>UEFI</p>
</li>
<li>
<p>AMD SEV</p>
</li>
<li>
<p>Direct download from Rados Block Device (RBD)</p>
</li>
<li>
<p>File-backed memory</p>
</li>
<li>
<p>Defining a custom inventory of resources in a YAML file, <code>provider.yaml</code></p>
</li>
</ul>
</div>
<div class="paragraph">
<div class="title">Unsupported features</div>
<p>The adoption process does not support the following features:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>instanceHA</p>
</li>
<li>
<p>DCN</p>
</li>
<li>
<p>Designate</p>
</li>
<li>
<p>Load-balancing service (octavia)</p>
</li>
<li>
<p>BGP</p>
</li>
<li>
<p>Adopting a FIPS environment</p>
</li>
<li>
<p>The Key Manager service only supports the simple crypto plug-in</p>
</li>
<li>
<p>The Block Storage service only supports RBD back-end adoption</p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="adoption-prerequisites_planning">Adoption prerequisites</h3>
<div class="paragraph">
<p>Before you begin the adoption procedure, complete the following prerequisites:</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">Planning information</dt>
<dd>
<div class="ulist">
<ul>
<li>
<p>Review the <a href="#adoption-limitations_planning">Adoption limitations</a>.</p>
</li>
<li>
<p>Review the Red Hat OpenShift Container Platform (RHOCP) requirements, data plane node requirements, Compute node requirements, and so on. For more information, see <a href="https://docs.redhat.com/en/documentation/red_hat_openstack_services_on_openshift/18.0/html/planning_your_deployment/index">Planning your deployment</a>.</p>
</li>
<li>
<p>Review the adoption-specific networking requirements. For more information, see <a href="#configuring-network-for-RHOSO-deployment_planning">Configuring the network for the RHOSO deployment</a>.</p>
</li>
<li>
<p>Review the adoption-specific storage requirements. For more information, see <a href="#storage-requirements_configuring-network">Storage requirements</a>.</p>
</li>
<li>
<p>Review how to customize your deployed control plane with the services that are required for your environment. For more information, see <a href="https://docs.redhat.com/en/documentation/red_hat_openstack_services_on_openshift/18.0/html/customizing_the_red_hat_openstack_services_on_openshift_deployment/index">Customizing the Red Hat OpenStack Services on OpenShift deployment</a>.</p>
</li>
<li>
<p>Familiarize yourself with the following RHOCP concepts that are used during adoption:</p>
<div class="ulist">
<ul>
<li>
<p><a href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/html/nodes/overview-of-nodes">Overview of nodes</a></p>
</li>
<li>
<p><a href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/html/nodes/index#nodes-scheduler-node-selectors-about_nodes-scheduler-node-selectors">About node selectors</a></p>
</li>
<li>
<p><a href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/html/machine_configuration/index">Machine configuration overview</a></p>
</li>
</ul>
</div>
</li>
<li>
<p>Make sure to set the correct RHOSO project namespace in which to run commands.</p>
</li>
</ul>
</div>
</dd>
</dl>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">$ oc project openstack</code></pre>
</div>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">Back-up information</dt>
<dd>
<div class="ulist">
<ul>
<li>
<p>Back up your Red&#160;Hat OpenStack Platform (RHOSP) 17.1 environment by using one of the following options:</p>
<div class="ulist">
<ul>
<li>
<p>The Relax-and-Recover tool. For more information, see <a href="https://docs.redhat.com/en/documentation/red_hat_openstack_platform/17.1/html/backing_up_and_restoring_the_undercloud_and_control_plane_nodes/assembly_backing-up-the-undercloud-and-the-control-plane-nodes-using-the-relax-and-recover-tool_br-undercloud-ctlplane">Backing up the undercloud and the control plane nodes by using the Relax-and-Recover tool</a> in <em>Backing up and restoring the undercloud and control plane nodes</em>.</p>
</li>
<li>
<p>The Snapshot and Revert tool. For more information, see <a href="https://docs.redhat.com/en/documentation/red_hat_openstack_platform/17.1/html/backing_up_and_restoring_the_undercloud_and_control_plane_nodes/assembly_snapshot-and-revert-appendix_snapshot-and-revert-appendix">Backing up your Red Hat OpenStack Platform cluster by using the Snapshot and Revert tool</a> in <em>Backing up and restoring the undercloud and control plane nodes</em>.</p>
</li>
<li>
<p>A third-party backup and recovery tool. For more information about certified backup and recovery tools, see the <a href="https://catalog.redhat.com/">Red Hat Ecosystem Catalog</a>.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Back up the configuration files from the RHOSP services and director on your file system. For more information, see <a href="#pulling-configuration-from-tripleo-deployment_adopt-control-plane">Pulling the configuration from a director deployment</a>.</p>
</li>
</ul>
</div>
</dd>
<dt class="hdlist1">Compute</dt>
<dd>
<div class="ulist">
<ul>
<li>
<p>Upgrade your Compute nodes to Red Hat Enterprise Linux 9.2. For more information, see <a href="https://docs.redhat.com/en/documentation/red_hat_openstack_platform/17.1/html-single/framework_for_upgrades_16.2_to_17.1/index#upgrading-compute-nodes_upgrading-the-compute-node-operating-system">Upgrading all Compute nodes to RHEL 9.2</a> in <em>Framework for upgrades (16.2 to 17.1)</em>.</p>
</li>
<li>
<p>Perform a minor update to the latest RHOSP version. For more information, see <a href="https://docs.redhat.com/en/documentation/red_hat_openstack_platform/17.1/html/performing_a_minor_update_of_red_hat_openstack_platform/index">Performing a minor update of Red Hat OpenStack Platform</a>.</p>
</li>
<li>
<p>Install the <code>systemd-container</code> package on your Compute hosts. For more information, see <a href="#installing-the-systemd-container-package-on-compute-hosts_planning">Installing the <code>systemd-container</code> package on Compute hosts</a>.</p>
</li>
</ul>
</div>
</dd>
<dt class="hdlist1">ML2/OVS</dt>
<dd>
<div class="ulist">
<ul>
<li>
<p>If you use the Modular Layer 2 plug-in with Open vSwitch mechanism driver (ML2/OVS), migrate it to the Modular Layer 2 plug-in with Open Virtual Networking (ML2/OVN) mechanism driver. For more information, see <a href="https://docs.redhat.com/en/documentation/red_hat_openstack_platform/17.1/html/migrating_to_the_ovn_mechanism_driver/index">Migrating to the OVN mechanism driver</a>.</p>
</li>
</ul>
</div>
</dd>
<dt class="hdlist1">Tools</dt>
<dd>
<div class="ulist">
<ul>
<li>
<p>Install the <code>oc</code> command line tool on your workstation.</p>
</li>
<li>
<p>Install the <code>podman</code> command line tool on your workstation.</p>
</li>
</ul>
</div>
</dd>
<dt class="hdlist1">RHOSP 17.1 release</dt>
<dd>
<div class="ulist">
<ul>
<li>
<p>The RHOSP 17.1 cloud is updated to the latest minor version of the 17.1 release.</p>
</li>
</ul>
</div>
</dd>
<dt class="hdlist1">RHOSP 17.1 hosts</dt>
<dd>
<div class="ulist">
<ul>
<li>
<p>All control plane and data plane hosts of the RHOSP 17.1 cloud are up and running, and continue to run throughout the adoption procedure.</p>
</li>
</ul>
</div>
</dd>
</dl>
</div>
</div>
<div class="sect2">
<h3 id="adoption-guidelines_planning">Guidelines for planning the adoption</h3>
<div class="paragraph">
<p>When planning to adopt a Red&#160;Hat OpenStack Services on OpenShift (RHOSO) 18.0 environment, consider the scope of the change. An adoption is similar in scope to a data center upgrade. Different firmware levels, hardware vendors, hardware profiles, networking interfaces, storage interfaces, and so on affect the adoption process and can cause changes in behavior during the adoption.</p>
</div>
<div class="paragraph">
<p>Review the following guidelines to adequately plan for the adoption and increase the chance that you complete the adoption successfully:</p>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
All commands in the adoption documentation are examples. Do not copy and paste the commands without understanding what the commands do.
</td>
</tr>
</table>
</div>
<div class="ulist">
<ul>
<li>
<p>To minimize the risk of an adoption failure, reduce the number of environmental differences between the staging environment and the production sites.</p>
</li>
<li>
<p>If the staging environment is not representative of the production sites, or a staging environment is not available, then you must plan to include contingency time in case the adoption fails.</p>
</li>
<li>
<p>Review your custom Red&#160;Hat OpenStack Platform (RHOSP) service configuration at every major release.</p>
<div class="ulist">
<ul>
<li>
<p>Every major release upgrades through multiple OpenStack releases.</p>
</li>
<li>
<p>Each major release might deprecate configuration options or change the format of the configuration.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Prepare a Method of Procedure (MOP) that is specific to your environment to reduce the risk of variance or omitted steps when running the adoption process.</p>
</li>
<li>
<p>You can use representative hardware in a staging environment to prepare a MOP and validate any content changes.</p>
<div class="ulist">
<ul>
<li>
<p>Include a cross-section of firmware versions, additional interface or device hardware, and any additional software in the representative staging environment to ensure that it is broadly representative of the variety that is present in the production environments.</p>
</li>
<li>
<p>Ensure that you validate any Red Hat Enterprise Linux update or upgrade in the representative staging environment.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Use Satellite for localized and version-pinned RPM content where your data plane nodes are located.</p>
</li>
<li>
<p>In the production environment, use the content that you tested in the staging environment.</p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="adoption-process-overview_planning">Adoption process overview</h3>
<div class="paragraph">
<p>Familiarize yourself with the steps of the adoption process and the optional post-adoption tasks.</p>
</div>
<div class="olist arabic">
<div class="title">Main adoption process</div>
<ol class="arabic">
<li>
<p><a href="#migrating-tls-everywhere_configuring-network">Migrate TLS everywhere (TLS-e) to the Red Hat OpenStack Services on OpenShift (RHOSO) deployment</a>.</p>
</li>
<li>
<p><a href="#migrating-databases-to-the-control-plane_configuring-network">Migrate your existing databases to the new control plane</a>.</p>
</li>
<li>
<p><a href="#adopting-openstack-control-plane-services_configuring-network">Adopt your Red Hat OpenStack Platform 17.1 control plane services to the new RHOSO 18.0 deployment</a>.</p>
</li>
<li>
<p><a href="#adopting-data-plane_adopt-control-plane">Adopt the RHOSO 18.0 data plane</a>.</p>
</li>
<li>
<p><a href="#migrating-the-object-storage-service_adopt-control-plane">Migrate the Object Storage service (swift) to the RHOSO nodes</a>.</p>
</li>
<li>
<p><a href="#ceph-migration_adopt-control-plane">Migrate the Red Hat Ceph Storage cluster</a>.</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p><a href="#migrating-ceph-monitoring_migrating-ceph">Migrate the monitoring stack component to new nodes within an existing Red Hat Ceph Storage cluster</a>.</p>
</li>
<li>
<p><a href="#migrating-ceph-mds_migrating-ceph-monitoring">Migrate Red Hat Ceph Storage MDS to new nodes within the existing cluster</a>.</p>
</li>
<li>
<p><a href="#migrating-ceph-rgw_migrating-ceph-monitoring">Migrate Red Hat Ceph Storage RGW to external RHEL nodes</a>.</p>
</li>
<li>
<p><a href="#migrating-ceph-rbd_migrating-ceph-monitoring">Migrate Red Hat Ceph Storage RBD to external RHEL nodes</a>.</p>
</li>
</ol>
</div>
</li>
</ol>
</div>
<div class="ulist">
<div class="title">Post-adoption tasks</div>
<ul>
<li>
<p>Optional: Run tempest to verify that the entire adoption process is working properly. For more information, see <a href="https://docs.redhat.com/en/documentation/red_hat_openstack_services_on_openshift/18.0/html/validating_and_troubleshooting_the_deployed_cloud/index">Validating and troubleshooting the deployed cloud</a>.</p>
</li>
<li>
<p>Optional: Perform a minor update from RHEL 9.2 to 9.4. You can perform a minor update any time after you complete the adoption procedure. For more information, see <a href="https://docs.redhat.com/en/documentation/red_hat_openstack_services_on_openshift/18.0/html/updating_your_environment_to_the_latest_maintenance_release/index">Updating your environment to the latest maintenance release</a>.</p>
</li>
<li>
<p>Optional: Verify that you migrated all services from the Controller nodes, and then power off the nodes. If any services are still running in the Controller nodes, such as Open Virtual Networking (ML2/OVN), Object Storage service (swift), or Red Hat Ceph Storage, do not power off the nodes.</p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="installing-the-systemd-container-package-on-compute-hosts_planning">Installing the <code>systemd-container</code> package on Compute hosts</h3>
<div class="paragraph">
<p>Before you adopt the Red&#160;Hat OpenStack Services on OpenShift (RHOSO) data plane, you must install the <code>systemd-container</code> package on all the hypervisors on your Compute hosts. This procedure must be performed on one Compute host at a time.</p>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>If your Compute host is running a virtual machine, live migrate the virtual machine from the host. For more information about live migration, see <a href="https://docs.redhat.com/en/documentation/red_hat_openstack_platform/17.1/html/performing_a_minor_update_of_red_hat_openstack_platform/assembly_rebooting-the-overcloud_keeping-updated#proc_rebooting-compute-nodes_rebooting-the-overcloud">Rebooting Compute nodes</a> in <em>Performing a minor update of Red Hat OpenStack Platform</em>.</p>
</li>
<li>
<p>Install the <code>systemd-container</code> on the host:</p>
<div class="ulist">
<ul>
<li>
<p>If you upgraded your environment from an earlier version of Red&#160;Hat OpenStack Platform, reboot the Compute host to automatically install the <code>systemd-container</code>.</p>
</li>
<li>
<p>If you deployed a new RHOSO environment, install the <code>systemd-container</code> manually by using the following command. Rebooting the Compute host is not required:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo dnf -y install systemd-container</pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
If your Compute host is not running a virtual machine, you can install the <code>systemd-container</code> automatically or manually.
</td>
</tr>
</table>
</div>
</li>
</ul>
</div>
</li>
<li>
<p>Repeat this procedure on each hypervisor one by one.</p>
</li>
</ol>
</div>
</div>
<div class="sect2">
<h3 id="identity-service-authentication_planning">Identity service authentication</h3>
<div class="paragraph">
<p>If you have custom policies enabled, contact Red Hat Support before adopting a director OpenStack deployment. You must complete the following steps for adoption:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Remove custom policies.</p>
</li>
<li>
<p>Run the adoption.</p>
</li>
<li>
<p>Re-add custom policies by using the new SRBAC syntax.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>After you adopt a director-based OpenStack deployment to a Red&#160;Hat OpenStack Services on OpenShift deployment, the Identity service performs user authentication and authorization by using Secure RBAC (SRBAC). If SRBAC is already enabled, then there is no change to how you perform operations. If SRBAC is disabled, then adopting a director-based OpenStack deployment might change how you perform operations due to changes in API access policies.</p>
</div>
<div class="paragraph">
<p>For more information on SRBAC, see <a href="https://docs.redhat.com/en/documentation/red_hat_openstack_services_on_openshift/18.0/html/performing_security_operations/assembly_srbac-in-rhoso_performing-security-services#assembly_srbac-in-rhoso_performing-security-services">Secure role based access control in Red Hat OpenStack Services on OpenShift</a> in <em>Performing security operations</em>.</p>
</div>
</div>
<div class="sect2">
<h3 id="configuring-network-for-RHOSO-deployment_planning">Configuring the network for the Red&#160;Hat OpenStack Services on OpenShift deployment</h3>
<div class="paragraph">
<p>When you adopt a new Red&#160;Hat OpenStack Services on OpenShift (RHOSO) deployment, you must align the network
configuration with the adopted cluster to maintain connectivity for existing
workloads.</p>
</div>
<div class="paragraph">
<p>Perform the following tasks to incorporate the existing network configuration:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Configure Red Hat OpenShift Container Platform (RHOCP) worker nodes to align VLAN tags and IP Address Management (IPAM) configuration with the existing deployment.</p>
</li>
<li>
<p>Configure control plane services to use compatible IP ranges for service and load-balancing IP addresses.</p>
</li>
<li>
<p>Configure data plane nodes to use corresponding compatible configuration for VLAN tags and IPAM.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>When configuring nodes and services, the general approach is as follows:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>For IPAM, you can either reuse subnet ranges from the existing deployment or, if there is a shortage of free IP addresses in existing subnets, define new ranges for the new control plane services. If you define new ranges, you configure IP routing between the old and new ranges. For more information, see <a href="#planning-your-ipam-configuration_configuring-network">Planning your IPAM configuration</a>.</p>
</li>
<li>
<p>For VLAN tags, always reuse the configuration from the existing deployment.</p>
</li>
</ul>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
For more information about the network architecture and configuration, see
<a href="https://docs.redhat.com/en/documentation/red_hat_openstack_services_on_openshift/18.0/html/deploying_red_hat_openstack_services_on_openshift/assembly_preparing-rhoso-networks">Preparing networks for Red Hat OpenStack Services on OpenShift</a> in <em>Deploying Red Hat OpenStack Services on OpenShift</em> and <a href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/html/networking/about-networking">About networking</a> in <em>Networking</em>.
</td>
</tr>
</table>
</div>
<div class="sect3">
<h4 id="retrieving-the-network-configuration_configuring-network">Retrieving the network configuration from your existing deployment</h4>
<div class="paragraph">
<p>You must determine which isolated networks are defined in your existing
deployment. After you retrieve your network configuration, you have the following information:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>A list of isolated networks that are used in the existing deployment.</p>
</li>
<li>
<p>For each of the isolated networks, the VLAN tag and IP ranges used for
dynamic address allocation.</p>
</li>
<li>
<p>A list of existing IP address allocations that are used in the environment.
When reusing the existing subnet ranges to host the new control plane
services, these addresses are excluded from the corresponding allocation
pools.</p>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Find the network configuration in the <code>network_data.yaml</code> file. For example:</p>
<div class="listingblock">
<div class="content">
<pre>- name: InternalApi
  mtu: 1500
  vip: true
  vlan: 20
  name_lower: internal_api
  dns_domain: internal.mydomain.tld.
  service_net_map_replace: internal
  subnets:
    internal_api_subnet:
      ip_subnet: '172.17.0.0/24'
      allocation_pools: [{'start': '172.17.0.4', 'end': '172.17.0.250'}]</pre>
</div>
</div>
</li>
<li>
<p>Retrieve the VLAN tag that is used in the <code>vlan</code> key and the IP range in the
<code>ip_subnet</code> key for each isolated network from the <code>network_data.yaml</code> file.
When reusing subnet ranges from the existing deployment for the new control
plane services, the ranges are split into separate pools for control
plane services and load-balancer IP addresses.</p>
</li>
<li>
<p>Use the <code>tripleo-ansible-inventory.yaml</code> file to determine the list of IP addresses that are already consumed in the adopted environment. For each listed host in the file, make a note of the IP and VIP addresses that are consumed by the node. For example:</p>
<div class="listingblock">
<div class="content">
<pre>Standalone:
  hosts:
    standalone:
      ...
      internal_api_ip: 172.17.0.100
    ...
  ...
standalone:
  children:
    Standalone: {}
  vars:
    ...
    internal_api_vip: 172.17.0.2
    ...</pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
In this example, the <code>172.17.0.2</code> and <code>172.17.0.100</code> values are
consumed and are not available for the new control plane services until the adoption is complete.
</td>
</tr>
</table>
</div>
</li>
<li>
<p>Repeat this procedure for each isolated network and each host in the
configuration.</p>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="planning-your-ipam-configuration_configuring-network">Planning your IPAM configuration</h4>
<div class="paragraph">
<p>In a Red&#160;Hat OpenStack Services on OpenShift (RHOSO) deployment, each service that is deployed on the Red Hat OpenShift Container Platform (RHOCP)
worker nodes requires an IP address from the IP Address Management (IPAM) pool.
In a Red&#160;Hat OpenStack Platform (RHOSP) deployment, all services that are
hosted on a Controller node share the same IP address.</p>
</div>
<div class="paragraph">
<p>The RHOSO control plane has different requirements for the number of IP
addresses that are made available for services. Depending on the size of the IP
ranges that are used in the existing RHOSO deployment, you might reuse
these ranges for the RHOSO control plane.</p>
</div>
<div class="paragraph">
<p>The total number of IP addresses that are required for the new control plane services in each isolated network is calculated as the sum of the following:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The number of RHOCP worker nodes. Each worker node requires 1 IP address in the <code>NodeNetworkConfigurationPolicy</code> custom resource (CR).</p>
</li>
<li>
<p>The number of IP addresses required for the data plane nodes. Each node requires an IP address from the <code>NetConfig</code> CRs.</p>
</li>
<li>
<p>The number of IP addresses required for control plane services. Each service requires an IP address from the <code>NetworkAttachmentDefinition</code> CRs. This number depends on the number of replicas for each service.</p>
</li>
<li>
<p>The number of IP addresses required for load balancer IP addresses. Each service requires a Virtual IP address from the <code>IPAddressPool</code> CRs.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>For example, a simple single worker node RHOCP deployment
with Red Hat OpenShift Local has the following IP ranges defined for the <code>internalapi</code> network:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>1 IP address for the single worker node</p>
</li>
<li>
<p>1 IP address for the data plane node</p>
</li>
<li>
<p><code>NetworkAttachmentDefinition</code> CRs for control plane services:
<code>X.X.X.30-X.X.X.70</code> (41 addresses)</p>
</li>
<li>
<p><code>IPAllocationPool</code> CRs for load balancer IPs: <code>X.X.X.80-X.X.X.90</code> (11
addresses)</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>This example shows a total of 54 IP addresses allocated to the <code>internalapi</code>
allocation pools.</p>
</div>
<div class="paragraph">
<p>The requirements might differ depending on the list of RHOSP services
to be deployed, their replica numbers, and the number of RHOCP worker nodes and data plane nodes.</p>
</div>
<div class="paragraph">
<p>Additional IP addresses might be required in future RHOSP releases, so you must plan for some extra capacity for each of the allocation pools that are used in the new environment.</p>
</div>
<div class="paragraph">
<p>After you determine the required IP pool size for the new deployment, you can choose to define new IP address ranges or reuse your existing IP address ranges. Regardless of the scenario, the VLAN tags in the existing deployment are reused in the new deployment. Ensure that the VLAN tags are properly retained in the new configuration. For more information, see <a href="#configuring-isolated-networks_configuring-network">Configuring isolated networks</a>.</p>
</div>
<div class="sect4">
<h5 id="using-new-subnet-ranges_ipam-configuration">Configuring new subnet ranges</h5>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
If you are using IPv6, you can reuse existing subnet ranges in most cases. For more information about existing subnet ranges, see <a href="#reusing-existing-subnet-ranges_ipam-configuration">Reusing existing subnet ranges</a>.
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>You can define new IP ranges for control plane services that belong to a different subnet that is not used in the existing cluster. Then you configure link local IP routing between the existing and new subnets to enable existing and new service deployments to communicate. This involves using the director mechanism on a pre-adopted cluster to configure additional link local routes. This enables the data plane deployment to reach out to Red&#160;Hat OpenStack Platform (RHOSP) nodes by using the existing subnet addresses. You can use new subnet ranges with any existing subnet configuration, and when the existing cluster subnet ranges do not have enough free IP addresses for the new control plane services.</p>
</div>
<div class="paragraph">
<p>You must size the new subnet appropriately to accommodate the new control
plane services. There are no specific requirements for the
existing deployment allocation pools that are already consumed by the RHOSP environment.</p>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
Defining a new subnet for Storage and Storage management is not supported because Compute service (nova) and Red Hat Ceph Storage do not allow modifying those networks during adoption.
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>In the following procedure, you configure <code>NetworkAttachmentDefinition</code> custom resources (CRs) to use a different subnet from what is configured in the <code>network_config</code> section of the <code>OpenStackDataPlaneNodeSet</code> CR for the same networks. The new range in the <code>NetworkAttachmentDefinition</code> CR is used for control plane services, while the existing range in the <code>OpenStackDataPlaneNodeSet</code> CR is used to manage IP Address Management (IPAM) for data plane nodes.</p>
</div>
<div class="paragraph">
<p>The values that are used in the following procedure are examples. Use values that are specific to your configuration.</p>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Configure link local routes on the existing deployment nodes for the control plane subnets. This is done through director configuration:</p>
<div class="listingblock">
<div class="content">
<pre>network_config:
  - type: ovs_bridge
    name: br-ctlplane
    routes:
    - ip_netmask: 0.0.0.0/0
      next_hop: 192.168.1.1
    - ip_netmask: 172.31.0.0/24 <i class="conum" data-value="1"></i><b>(1)</b>
      next_hop: 192.168.1.100 <i class="conum" data-value="2"></i><b>(2)</b></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>The new control plane subnet.</td>
</tr>
<tr>
<td><i class="conum" data-value="2"></i><b>2</b></td>
<td>The control plane IP address of the existing data plane node.
<div class="paragraph">
<p>Repeat this configuration for other networks that need to use different subnets for the new and existing parts of the deployment.</p>
</div></td>
</tr>
</table>
</div>
</li>
<li>
<p>Apply the new configuration to every RHOSP node:</p>
<div class="listingblock">
<div class="content">
<pre>(undercloud)$ openstack overcloud network provision \
 --output  &lt;deployment_file&gt; \
[--templates &lt;templates_directory&gt;]/home/stack/templates/&lt;networks_definition_file&gt;</pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre>(undercloud)$ openstack overcloud node provision \
 --stack &lt;stack&gt; \
 --network-config \
 --output &lt;deployment_file&gt; \
[--templates &lt;templates_directory&gt;]/home/stack/templates/&lt;node_definition_file&gt;</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Optional: Include the <code>--templates</code> option to use your own templates instead of the default templates located in <code>/usr/share/openstack-tripleo-heat-templates</code>. Replace <code>&lt;templates_directory&gt;</code> with the path to the directory that contains your templates.</p>
</li>
<li>
<p>Replace <code>&lt;stack&gt;</code> with the name of the stack for which the bare-metal nodes are provisioned. If not specified, the default is <code>overcloud</code>.</p>
</li>
<li>
<p>Include the <code>--network-config</code> optional argument to provide the network definitions to the <code>cli-overcloud-node-network-config.yaml</code> Ansible playbook. The <code>cli-overcloud-node-network-config.yaml</code> playbook uses the <code>os-net-config</code> tool to apply the network configuration on the deployed nodes. If you do not use <code>--network-config</code> to provide the network definitions, then you must configure the <code>{{role.name}}NetworkConfigTemplate</code> parameters in your <code>network-environment.yaml</code> file, otherwise the default network definitions are used.</p>
</li>
<li>
<p>Replace <code>&lt;deployment_file&gt;</code> with the name of the heat environment file to generate for inclusion in the deployment command, for example <code>/home/stack/templates/overcloud-baremetal-deployed.yaml</code>.</p>
</li>
<li>
<p>Replace <code>&lt;node_definition_file&gt;</code> with the name of your node definition file, for example, <code>overcloud-baremetal-deploy.yaml</code>. Ensure that the <code>network_config_update</code> variable is set to <code>true</code> in the node definition file.</p>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Network configuration changes are not applied by default to avoid
the risk of network disruption. You must enforce the changes by setting the
<code>StandaloneNetworkConfigUpdate: true</code> in the director configuration files.
</td>
</tr>
</table>
</div>
</li>
</ul>
</div>
</li>
<li>
<p>Confirm that there are new link local routes to the new subnet on each node. For example:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml"># ip route | grep 172
172.31.0.0/24 via 192.168.122.100 dev br-ctlplane</code></pre>
</div>
</div>
</li>
<li>
<p>You also must configure link local routes to existing deployment on Red&#160;Hat OpenStack Services on OpenShift (RHOSO) worker nodes. This is achieved by adding <code>routes</code> entries to the <code>NodeNetworkConfigurationPolicy</code> CRs for each network. For example:</p>
<div class="listingblock">
<div class="content">
<pre>  - destination: 192.168.122.0/24 <i class="conum" data-value="1"></i><b>(1)</b>
    next-hop-interface: ospbr <i class="conum" data-value="2"></i><b>(2)</b></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>The original subnet of the isolated network on the data plane.</td>
</tr>
<tr>
<td><i class="conum" data-value="2"></i><b>2</b></td>
<td>The Red Hat OpenShift Container Platform (RHOCP) worker network interface that corresponds to the isolated network on the data plane.
<div class="paragraph">
<p>As a result, the following route is added to your RHOCP nodes:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml"># ip route | grep 192
192.168.122.0/24 dev ospbr proto static scope link</code></pre>
</div>
</div></td>
</tr>
</table>
</div>
</li>
<li>
<p>Later, during the data plane adoption, in the <code>network_config</code> section of the <code>OpenStackDataPlaneNodeSet</code> CR, add the same link local routes for the new control plane subnet ranges. For example:</p>
<div class="listingblock">
<div class="content">
<pre>  nodeTemplate:
    ansible:
      ansibleUser: root
      ansibleVars:
        additional_ctlplane_host_routes:
        - ip_netmask: 172.31.0.0/24
          next_hop: '{{ ctlplane_ip }}'
        edpm_network_config_template: |
          network_config:
          - type: ovs_bridge
            routes: {{ ctlplane_host_routes + additional_ctlplane_host_routes }}
            ...</pre>
</div>
</div>
</li>
<li>
<p>List the IP addresses that are used for the data plane nodes in the existing deployment as <code>ansibleHost</code> and <code>fixedIP</code>. For example:</p>
<div class="listingblock">
<div class="content">
<pre>  nodes:
    standalone:
      ansible:
        ansibleHost: 192.168.122.100
        ansibleUser: ""
      hostName: standalone
      networks:
      - defaultRoute: true
        fixedIP: 192.168.122.100
        name: ctlplane
        subnetName: subnet1</pre>
</div>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
Do not change RHOSP node IP addresses during the adoption process. List previously used IP addresses in the <code>fixedIP</code> fields for each node entry in the <code>nodes</code> section of the <code>OpenStackDataPlaneNodeSet</code> CR.
</td>
</tr>
</table>
</div>
</li>
<li>
<p>Expand the SSH range for the firewall configuration to include both subnets to allow SSH access to data plane nodes from both subnets:</p>
<div class="listingblock">
<div class="content">
<pre>  edpm_sshd_allowed_ranges:
  - 192.168.122.0/24
  - 172.31.0.0/24</pre>
</div>
</div>
<div class="paragraph">
<p>This provides SSH access from the new subnet to the RHOSP nodes as well as the RHOSP subnets.</p>
</div>
</li>
<li>
<p>Set <code>edpm_network_config_update: true</code> to enforce the changes that you are applying to the nodes.</p>
</li>
</ol>
</div>
</div>
<div class="sect4">
<h5 id="reusing-existing-subnet-ranges_ipam-configuration">Reusing existing subnet ranges</h5>
<div class="paragraph">
<p>You can reuse existing subnet ranges if they have enough IP addresses to allocate to the new control plane services. You configure the new control plane services to use the same subnet as you used in the Red&#160;Hat OpenStack Platform (RHOSP) environment, and configure the allocation pools that are used by the new services to exclude IP addresses that are already allocated to existing cluster nodes. By reusing existing subnets, you avoid additional link local route configuration between the existing and new subnets.</p>
</div>
<div class="paragraph">
<p>If your existing subnets do not have enough IP addresses in the existing subnet ranges for the new control plane services, you must create new subnet ranges. For more information, see <a href="#using-new-subnet-ranges_ipam-configuration">Using new subnet ranges</a>.</p>
</div>
<div class="paragraph">
<p>No special routing configuration is required to reuse subnet ranges. However, you must ensure that the IP addresses that are consumed by RHOSP services do not overlap with the new allocation pools configured for Red&#160;Hat OpenStack Services on OpenShift control plane services.</p>
</div>
<div class="paragraph">
<p>If you are especially constrained by the size of the existing subnet, you may
have to apply elaborate exclusion rules when defining allocation pools for the
new control plane services. For more information, see <a href="#configuring-isolated-networks_configuring-network">Configuring isolated networks</a>.</p>
</div>
</div>
</div>
<div class="sect3">
<h4 id="configuring-isolated-networks_configuring-network">Configuring isolated networks</h4>
<div class="paragraph">
<p>Before you begin replicating your existing VLAN and IPAM configuration in the Red&#160;Hat OpenStack Services on OpenShift (RHOSO) environment, you must have the following IP address allocations for the new control plane services:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>1 IP address for each isolated network on each Red Hat OpenShift Container Platform (RHOCP) worker node. You configure these IP addresses in the <code>NodeNetworkConfigurationPolicy</code> custom resources (CRs) for the RHOCP worker nodes. For more information, see <a href="#configuring-openshift-worker-nodes_isolated-networks">Configuring RHOCP worker nodes</a>.</p>
</li>
<li>
<p>1 IP range for each isolated network for the data plane nodes. You configure these ranges in the <code>NetConfig</code> CRs for the data plane nodes. For more information, see <a href="#configuring-data-plane-nodes_isolated-networks">Configuring data plane nodes</a>.</p>
</li>
<li>
<p>1 IP range for each isolated network for control plane services. These ranges
enable pod connectivity for isolated networks in the <code>NetworkAttachmentDefinition</code> CRs. For more information, see <a href="#configuring-networking-for-control-plane-services_isolated-networks">Configuring the networking for control plane services</a>.</p>
</li>
<li>
<p>1 IP range for each isolated network for load balancer IP addresses. These IP ranges define load balancer IP addresses for MetalLB in the <code>IPAddressPool</code> CRs. For more information, see <a href="#configuring-networking-for-control-plane-services_isolated-networks">Configuring the networking for control plane services</a>.</p>
</li>
</ul>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
The exact list and configuration of isolated networks in the following procedures should reflect the actual Red&#160;Hat OpenStack Platform environment. The number of isolated networks might differ from the examples used in the procedures. The IPAM scheme might also differ. Only the parts of the configuration that are relevant to configuring networks are shown. The values that are used in the following procedures are examples. Use values that are specific to your configuration.
</td>
</tr>
</table>
</div>
<div class="sect4">
<h5 id="configuring-openshift-worker-nodes_isolated-networks">Configuring isolated networks on RHOCP worker nodes</h5>
<div class="paragraph">
<p>To connect service pods to isolated networks on Red Hat OpenShift Container Platform (RHOCP) worker nodes that run Red&#160;Hat OpenStack Platform services, physical network configuration on the hypervisor is required.</p>
</div>
<div class="paragraph">
<p>This configuration is managed by the NMState operator, which uses <code>NodeNetworkConfigurationPolicy</code> custom resources (CRs) to define the desired network configuration for the nodes.</p>
</div>
<div class="ulist">
<div class="title">Procedure</div>
<ul>
<li>
<p>For each RHOCP worker node, define a <code>NodeNetworkConfigurationPolicy</code> CR that describes the desired network configuration. For example:</p>
<div class="listingblock">
<div class="content">
<pre>apiVersion: v1
items:
- apiVersion: nmstate.io/v1
  kind: NodeNetworkConfigurationPolicy
  spec:
    desiredState:
      interfaces:
      - description: internalapi vlan interface
        ipv4:
          address:
          - ip: 172.17.0.10
            prefix-length: 24
          dhcp: false
          enabled: true
        ipv6:
          enabled: false
        name: enp6s0.20
        state: up
        type: vlan
        vlan:
          base-iface: enp6s0
          id: 20
          reorder-headers: true
      - description: storage vlan interface
        ipv4:
          address:
          - ip: 172.18.0.10
            prefix-length: 24
          dhcp: false
          enabled: true
        ipv6:
          enabled: false
        name: enp6s0.21
        state: up
        type: vlan
        vlan:
          base-iface: enp6s0
          id: 21
          reorder-headers: true
      - description: tenant vlan interface
        ipv4:
          address:
          - ip: 172.19.0.10
            prefix-length: 24
          dhcp: false
          enabled: true
        ipv6:
          enabled: false
        name: enp6s0.22
        state: up
        type: vlan
        vlan:
          base-iface: enp6s0
          id: 22
          reorder-headers: true
    nodeSelector:
      kubernetes.io/hostname: ocp-worker-0
      node-role.kubernetes.io/worker: ""</pre>
</div>
</div>
</li>
</ul>
</div>
</div>
<div class="sect4">
<h5 id="configuring-networking-for-control-plane-services_isolated-networks">Configuring isolated networks on control plane services</h5>
<div class="paragraph">
<p>After the NMState operator creates the desired hypervisor network configuration for isolated networks, you must configure the Red&#160;Hat OpenStack Platform (RHOSP) services to use the configured interfaces. You define a <code>NetworkAttachmentDefinition</code> custom resource (CR) for each isolated network. In some clusters, these CRs are managed by the Cluster Network Operator, in which case you use <code>Network</code> CRs instead. For more information, see
<a href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/html/networking/cluster-network-operator#nw-cluster-network-operator_cluster-network-operator">Cluster Network Operator</a> in <em>Networking</em>.</p>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Define a <code>NetworkAttachmentDefinition</code> CR for each isolated network.
For example:</p>
<div class="listingblock">
<div class="content">
<pre>apiVersion: k8s.cni.cncf.io/v1
kind: NetworkAttachmentDefinition
metadata:
  name: internalapi
spec:
  config: |
    {
      "cniVersion": "0.3.1",
      "name": "internalapi",
      "type": "macvlan",
      "master": "enp6s0.20",
      "ipam": {
        "type": "whereabouts",
        "range": "172.17.0.0/24",
        "range_start": "172.17.0.20",
        "range_end": "172.17.0.50"
      }
    }</pre>
</div>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
Ensure that the interface name and IPAM range match the configuration that you used in the <code>NodeNetworkConfigurationPolicy</code> CRs.
</td>
</tr>
</table>
</div>
</li>
<li>
<p>Optional: When reusing existing IP ranges, you can exclude part of the range that is used in the existing deployment by using the <code>exclude</code> parameter in the <code>NetworkAttachmentDefinition</code> pool. For example:</p>
<div class="listingblock">
<div class="content">
<pre>apiVersion: k8s.cni.cncf.io/v1
kind: NetworkAttachmentDefinition
metadata:
  name: internalapi
spec:
  config: |
    {
      "cniVersion": "0.3.1",
      "name": "internalapi",
      "type": "macvlan",
      "master": "enp6s0.20",
      "ipam": {
        "type": "whereabouts",
        "range": "172.17.0.0/24",
        "range_start": "172.17.0.20", <i class="conum" data-value="1"></i><b>(1)</b>
        "range_end": "172.17.0.50", <i class="conum" data-value="2"></i><b>(2)</b>
        "exclude": [ <i class="conum" data-value="3"></i><b>(3)</b>
          "172.17.0.24/32",
          "172.17.0.44/31"
        ]
      }
    }</pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>Defines the start of the IP range.</td>
</tr>
<tr>
<td><i class="conum" data-value="2"></i><b>2</b></td>
<td>Defines the end of the IP range.</td>
</tr>
<tr>
<td><i class="conum" data-value="3"></i><b>3</b></td>
<td>Excludes part of the IP range. This example excludes IP addresses <code>172.17.0.24/32</code> and <code>172.17.0.44/31</code> from the allocation pool.</td>
</tr>
</table>
</div>
</li>
<li>
<p>If your RHOSP services require load balancer IP addresses, define the pools for these services in an <code>IPAddressPool</code> CR. For example:</p>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
The load balancer IP addresses belong to the same IP range as the control plane services, and are managed by MetalLB. This pool should also be aligned with the RHOSP configuration.
</td>
</tr>
</table>
</div>
<div class="listingblock">
<div class="content">
<pre>- apiVersion: metallb.io/v1beta1
  kind: IPAddressPool
  spec:
    addresses:
    - 172.17.0.60-172.17.0.70</pre>
</div>
</div>
<div class="paragraph">
<p>Define <code>IPAddressPool</code> CRs for each isolated network that requires load
balancer IP addresses.</p>
</div>
</li>
<li>
<p>Optional: When reusing existing IP ranges, you can exclude part of the range by listing multiple entries in the <code>addresses</code> section of the <code>IPAddressPool</code>. For example:</p>
<div class="listingblock">
<div class="content">
<pre>- apiVersion: metallb.io/v1beta1
  kind: IPAddressPool
  spec:
    addresses:
    - 172.17.0.60-172.17.0.64
    - 172.17.0.66-172.17.0.70</pre>
</div>
</div>
<div class="paragraph">
<p>The example above would exclude the <code>172.17.0.65</code> address from the allocation
pool.</p>
</div>
</li>
</ol>
</div>
</div>
<div class="sect4">
<h5 id="configuring-data-plane-nodes_isolated-networks">Configuring isolated networks on data plane nodes</h5>
<div class="paragraph">
<p>Data plane nodes are configured by the OpenStack Operator and your <code>OpenStackDataPlaneNodeSet</code> custom resources (CRs). The <code>OpenStackDataPlaneNodeSet</code> CRs define your desired network configuration for the nodes.</p>
</div>
<div class="paragraph">
<p>Your Red&#160;Hat OpenStack Services on OpenShift (RHOSO) network configuration should reflect the existing Red&#160;Hat OpenStack Platform (RHOSP) network setup. You must pull the <code>network_data.yaml</code> files from each RHOSP node and reuse them when you define the <code>OpenStackDataPlaneNodeSet</code> CRs. The format of the configuration does not change, so you can put network templates under <code>edpm_network_config_template</code> variables, either for all nodes or for each node.</p>
</div>
<div class="paragraph">
<p>To ensure that the latest network configuration is used during the data plane adoption, you should also set <code>edpm_network_config_update: true</code> in the <code>nodeTemplate</code> field of the <code>OpenStackDataPlaneNodeSet</code> CR.</p>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Configure a <code>NetConfig</code> CR with your desired VLAN tags and IPAM configuration. For example:</p>
<div class="listingblock">
<div class="content">
<pre>apiVersion: network.openstack.org/v1beta1
kind: NetConfig
metadata:
  name: netconfig
spec:
  networks: <i class="conum" data-value="1"></i><b>(1)</b>
  - name: internalapi
    dnsDomain: internalapi.example.com
    subnets:
    - name: subnet1
      allocationRanges:
      - end: 172.17.0.250
        start: 172.17.0.100
      cidr: 172.17.0.0/24
      vlan: 20
  - name: storage
    dnsDomain: storage.example.com
    subnets:
    - name: subnet1
      allocationRanges:
      - end: 172.18.0.250
        start: 172.18.0.100
      cidr: 172.18.0.0/24
      vlan: 21
  - name: tenant
    dnsDomain: tenant.example.com
    subnets:
    - name: subnet1
      allocationRanges:
      - end: 172.19.0.250
        start: 172.19.0.100
      cidr: 172.19.0.0/24
      vlan: 22</pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>The <code>networks</code> composition must match the source cloud configuration to avoid data plane connectivity downtime.</td>
</tr>
</table>
</div>
</li>
<li>
<p>Optional: In the <code>NetConfig</code> CR, list multiple ranges for the <code>allocationRanges</code> field to exclude some of the IP addresses, for example, to accommodate IP addresses that are already consumed by the adopted environment:</p>
<div class="listingblock">
<div class="content">
<pre>apiVersion: network.openstack.org/v1beta1
kind: NetConfig
metadata:
  name: netconfig
spec:
  networks:
  - name: internalapi
    dnsDomain: internalapi.example.com
    subnets:
    - name: subnet1
      allocationRanges:
      - end: 172.17.0.199
        start: 172.17.0.100
      - end: 172.17.0.250
        start: 172.17.0.201
      cidr: 172.17.0.0/24
      vlan: 20</pre>
</div>
</div>
<div class="paragraph">
<p>This example excludes the <code>172.17.0.200</code> address from the pool.</p>
</div>
</li>
</ol>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="storage-requirements_configuring-network">Storage requirements</h3>
<div class="paragraph">
<p>Storage in a Red&#160;Hat OpenStack Platform (RHOSP) deployment refers to the following types:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The storage that is needed for the service to run</p>
</li>
<li>
<p>The storage that the service manages</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Before you can deploy the services in Red&#160;Hat OpenStack Services on OpenShift (RHOSO), you must review the storage requirements, plan your Red Hat OpenShift Container Platform (RHOCP) node selection, prepare your RHOCP nodes, and so on.</p>
</div>
<div class="sect3">
<h4 id="storage-driver-certification_storage-requirements">Storage driver certification</h4>
<div class="paragraph">
<p>Before you adopt your Red&#160;Hat OpenStack Platform 17.1 deployment to a Red&#160;Hat OpenStack Services on OpenShift (RHOSO) 18.0 deployment, confirm that your deployed storage drivers are certified for use with RHOSO 18.0.</p>
</div>
<div class="paragraph">
<p>For information on software certified for use with RHOSO 18.0, see the <a href="https://catalog.redhat.com/search?searchType=software&amp;certified_versions=Red%20Hat%20OpenStack%20Services%20on%20OpenShift%2018&amp;p=1">Red Hat Ecosystem Catalog</a>.</p>
</div>
</div>
<div class="sect3">
<h4 id="block-storage-requirements_storage-requirements">Block Storage service guidelines</h4>
<div class="paragraph">
<p>Prepare to adopt your Block Storage service (cinder):</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Take note of the Block Storage service back ends that you use.</p>
</li>
<li>
<p>Determine all the transport protocols that the Block Storage service back ends use, such as
RBD, iSCSI, FC, NFS, NVMe-TCP, and so on. You must consider them when you place the Block Storage services and when the right storage transport-related binaries are running on the Red Hat OpenShift Container Platform (RHOCP) nodes. For more information about each storage transport protocol, see <a href="#openshift-preparation-for-block-storage-adoption_storage-requirements">RHOCP preparation for Block Storage service adoption</a>.</p>
</li>
<li>
<p>Use a Block Storage service volume service to deploy each Block Storage service volume back end.</p>
<div class="paragraph">
<p>For example, you have an LVM back end, a Ceph back end, and two entries in <code>cinderVolumes</code>, and you cannot set global defaults for all volume services. You must define a service for each of them:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: core.openstack.org/v1beta1
kind: OpenStackControlPlane
metadata:
  name: openstack
spec:
  cinder:
    enabled: true
    template:
      cinderVolumes:
        lvm:
          customServiceConfig: |
            [DEFAULT]
            debug = True
            [lvm]
&lt; . . . &gt;
        ceph:
          customServiceConfig: |
            [DEFAULT]
            debug = True
            [ceph]
&lt; . . . &gt;</code></pre>
</div>
</div>
<div class="admonitionblock warning">
<table>
<tr>
<td class="icon">
<i class="fa icon-warning" title="Warning"></i>
</td>
<td class="content">
Check that all configuration options are still valid for RHOSO 18.0 version. Configuration options might be deprecated, removed, or added. This applies to both back-end driver-specific configuration options and other generic options.
</td>
</tr>
</table>
</div>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="block-storage-limitations_storage-requirements">Limitations for adopting the Block Storage service</h4>
<div class="paragraph">
<p>Before you begin the Block Storage service (cinder) adoption, review the following limitations:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>There is no global <code>nodeSelector</code> option for all Block Storage service volumes. You must specify the <code>nodeSelector</code> for each back end.</p>
</li>
<li>
<p>There are no global <code>customServiceConfig</code> or <code>customServiceConfigSecrets</code> options for all Block Storage service volumes. You must specify these options for each back end.</p>
</li>
<li>
<p>Support for Block Storage service back ends that require kernel modules that are not included in Red Hat Enterprise Linux is not tested in Red&#160;Hat OpenStack Services on OpenShift (RHOSO).</p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="openshift-preparation-for-block-storage-adoption_storage-requirements">RHOCP preparation for Block Storage service adoption</h4>
<div class="paragraph">
<p>Before you deploy Red&#160;Hat OpenStack Platform (RHOSP) in Red Hat OpenShift Container Platform (RHOCP) nodes, ensure that the networks are ready, that you decide which RHOCP nodes to restrict, and that you make any necessary changes to the RHOCP nodes.</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">Node selection</dt>
<dd>
<p>You might need to restrict the RHOCP nodes where the Block Storage service volume and backup services run.</p>
<div class="paragraph">
<p>An example of when you need to restrict nodes for a specific Block Storage service is when you deploy the Block Storage service with the LVM driver. In that scenario, the LVM data where the volumes are stored only exists in a specific host, so you need to pin the Block Storage-volume service to that specific RHOCP node. Running the service on any other RHOCP node does not work. You cannot use the RHOCP host node name to restrict the LVM back end. You need to identify the LVM back end by using a unique label, an existing label, or a new label:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>$ oc label nodes worker0 lvm=cinder-volumes</pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: core.openstack.org/v1beta1
kind: OpenStackControlPlane
metadata:
  name: openstack
spec:
  secret: osp-secret
  storageClass: local-storage
  cinder:
    enabled: true
    template:
      cinderVolumes:
        lvm-iscsi:
          nodeSelector:
            lvm: cinder-volumes
&lt; . . . &gt;</code></pre>
</div>
</div>
<div class="paragraph">
<p>For more information about node selection, see <a href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/html/nodes/index#nodes-scheduler-node-selectors-about_nodes-scheduler-node-selectors">About node selectors</a>.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>If your nodes do not have enough local disk space for temporary images, you can use a remote NFS location by setting the extra volumes feature, <code>extraMounts</code>.</p>
</div>
</td>
</tr>
</table>
</div>
</dd>
<dt class="hdlist1">Transport protocols</dt>
<dd>
<p>Some changes to the storage transport protocols might be required for RHOCP:</p>
<div class="ulist">
<ul>
<li>
<p>If you use a <code>MachineConfig</code> to make changes to RHOCP nodes, the nodes reboot.</p>
</li>
<li>
<p>Check the back-end sections that are listed in the <code>enabled_backends</code> configuration option in your <code>cinder.conf</code> file to determine the enabled storage back-end sections.</p>
</li>
<li>
<p>Depending on the back end, you can find the transport protocol by viewing the <code>volume_driver</code> or <code>target_protocol</code> configuration options.</p>
</li>
<li>
<p>The <code>icssid</code> service, <code>multipathd</code> service, and <code>NVMe-TCP</code> kernel modules start automatically on data plane nodes.</p>
<div class="dlist">
<dl>
<dt class="hdlist1">NFS</dt>
<dd>
<div class="ulist">
<ul>
<li>
<p>RHOCP connects to NFS back ends without additional changes.</p>
</li>
</ul>
</div>
</dd>
<dt class="hdlist1">Rados Block Device and Red Hat Ceph Storage</dt>
<dd>
<div class="ulist">
<ul>
<li>
<p>RHOCP connects to Red Hat Ceph Storage back ends without additional changes. You must provide credentials and configuration files to the services.</p>
</li>
</ul>
</div>
</dd>
<dt class="hdlist1">iSCSI</dt>
<dd>
<div class="ulist">
<ul>
<li>
<p>To connect to iSCSI volumes, the iSCSI initiator must run on the
RHOCP hosts where the volume and backup services run. The Linux Open iSCSI initiator does not support network namespaces, so you must only run one instance of the service for the normal RHOCP usage, as well as
the RHOCP CSI plugins and the RHOSP services.</p>
</li>
<li>
<p>If you are not already running <code>iscsid</code> on the RHOCP nodes, then you must apply a <code>MachineConfig</code>. For example:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: worker
    service: cinder
  name: 99-master-cinder-enable-iscsid
spec:
  config:
    ignition:
      version: 3.2.0
    systemd:
      units:
      - enabled: true
        name: iscsid.service</code></pre>
</div>
</div>
</li>
<li>
<p>If you use labels to restrict the nodes where the Block Storage services run, you must use a <code>MachineConfigPool</code> to limit the effects of the
<code>MachineConfig</code> to the nodes where your services might run. For more information, see <a href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/html/nodes/index#nodes-scheduler-node-selectors-about_nodes-scheduler-node-selectors">About node selectors</a>.</p>
</li>
<li>
<p>If you are using a single node deployment to test the process, replace <code>worker</code> with <code>master</code> in the <code>MachineConfig</code>.</p>
</li>
<li>
<p>For production deployments that use iSCSI volumes, configure multipathing for better I/O.</p>
</li>
</ul>
</div>
</dd>
<dt class="hdlist1">FC</dt>
<dd>
<div class="ulist">
<ul>
<li>
<p>The Block Storage service volume and Block Storage service backup services must run in an RHOCP host that has host bus adapters (HBAs). If some nodes do not have HBAs, then use labels to restrict where these services run. For more information, see <a href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/html/nodes/index#nodes-scheduler-node-selectors-about_nodes-scheduler-node-selectors">About node selectors</a>.</p>
</li>
<li>
<p>If you have virtualized RHOCP clusters that use FC you need to expose the host HBAs inside the virtual machine.</p>
</li>
<li>
<p>For production deployments that use FC volumes, configure multipathing for better I/O.</p>
</li>
</ul>
</div>
</dd>
<dt class="hdlist1">NVMe-TCP</dt>
<dd>
<div class="ulist">
<ul>
<li>
<p>To connect to NVMe-TCP volumes, load NVMe-TCP kernel modules on the RHOCP hosts.</p>
</li>
<li>
<p>If you do not already load the <code>nvme-fabrics</code> module on the RHOCP nodes where the volume and backup services are going to run, then you must apply a <code>MachineConfig</code>. For example:</p>
<div class="listingblock">
<div class="content">
<pre>apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: worker
    service: cinder
  name: 99-master-cinder-load-nvme-fabrics
spec:
  config:
    ignition:
      version: 3.2.0
    storage:
      files:
        - path: /etc/modules-load.d/nvme_fabrics.conf
          overwrite: false
          # Mode must be decimal, this is 0644
          mode: 420
          user:
            name: root
          group:
            name: root
          contents:
            # Source can be a http, https, tftp, s3, gs, or data as defined in rfc2397.
            # This is the rfc2397 text/plain string format
            source: data:,nvme-fabrics</pre>
</div>
</div>
</li>
<li>
<p>If you use labels to restrict the nodes where Block Storage
services run, use a <code>MachineConfigPool</code> to limit the effects of the <code>MachineConfig</code> to the nodes where your services run. For more information, see <a href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/html/nodes/index#nodes-scheduler-node-selectors-about_nodes-scheduler-node-selectors">About node selectors</a>.</p>
</li>
<li>
<p>If you use a single node deployment to test the process, replace <code>worker</code> with <code>master</code> in the <code>MachineConfig</code>.</p>
</li>
<li>
<p>Only load the <code>nvme-fabrics</code> module because it loads the transport-specific modules, such as TCP, RDMA, or FC, as needed.</p>
</li>
<li>
<p>For production deployments that use NVMe-TCP volumes, use multipathing for better I/O. For NVMe-TCP volumes, RHOCP uses native multipathing, called ANA.</p>
</li>
<li>
<p>After the RHOCP nodes reboot and load the <code>nvme-fabrics</code> module, you can confirm that the operating system is configured and that it supports ANA by checking the host:</p>
<div class="listingblock">
<div class="content">
<pre>$ cat /sys/module/nvme_core/parameters/multipath</pre>
</div>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
ANA does not use the Linux Multipathing Device Mapper, but RHOCP requires <code>multipathd</code> to run on Compute nodes for the Compute service (nova) to be able to use multipathing. Multipathing is automatically configured on data plane nodes when they are provisioned.
</td>
</tr>
</table>
</div>
</li>
</ul>
</div>
</dd>
<dt class="hdlist1">Multipathing</dt>
<dd>
<div class="ulist">
<ul>
<li>
<p>Use multipathing for iSCSI and FC protocols. To configure multipathing on these protocols, you perform the following tasks:</p>
<div class="ulist">
<ul>
<li>
<p>Prepare the RHOCP hosts</p>
</li>
<li>
<p>Configure the Block Storage services</p>
</li>
<li>
<p>Prepare the Compute service nodes</p>
</li>
<li>
<p>Configure the Compute service</p>
</li>
</ul>
</div>
</li>
<li>
<p>To prepare the RHOCP hosts, ensure that the Linux Multipath Device Mapper is configured and running on the RHOCP hosts by using <code>MachineConfig</code>. For example:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml"># Includes the /etc/multipathd.conf contents and the systemd unit changes
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: worker
    service: cinder
  name: 99-master-cinder-enable-multipathd
spec:
  config:
    ignition:
      version: 3.2.0
    storage:
      files:
        - path: /etc/multipath.conf
          overwrite: false
          # Mode must be decimal, this is 0600
          mode: 384
          user:
            name: root
          group:
            name: root
          contents:
            # Source can be a http, https, tftp, s3, gs, or data as defined in rfc2397.
            # This is the rfc2397 text/plain string format
            source: data:,defaults%20%7B%0A%20%20user_friendly_names%20no%0A%20%20recheck_wwid%20yes%0A%20%20skip_kpartx%20yes%0A%20%20find_multipaths%20yes%0A%7D%0A%0Ablacklist%20%7B%0A%7D
    systemd:
      units:
      - enabled: true
        name: multipathd.service</code></pre>
</div>
</div>
</li>
<li>
<p>If you use labels to restrict the nodes where Block Storage services run, you need to use a <code>MachineConfigPool</code> to limit the effects of the <code>MachineConfig</code> to only the nodes where your services run. For more information, see <a href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/html/nodes/index#nodes-scheduler-node-selectors-about_nodes-scheduler-node-selectors">About node selectors</a>.</p>
</li>
<li>
<p>If you are using a single node deployment to test the process, replace <code>worker</code> with <code>master</code> in the <code>MachineConfig</code>.</p>
</li>
<li>
<p>Cinder volume and backup are configured by default to use multipathing.</p>
</li>
</ul>
</div>
</dd>
</dl>
</div>
</li>
</ul>
</div>
</dd>
</dl>
</div>
</div>
<div class="sect3">
<h4 id="preparing-block-storage-by-customizing-configuration_storage-requirements">Converting the Block Storage service configuration</h4>
<div class="paragraph">
<p>In your previous deployment, you use the same <code>cinder.conf</code> file for all the services. To prepare your Block Storage service (cinder) configuration for adoption, split this single-file configuration into individual configurations for each Block Storage service service. Review the following information to guide you in coverting your previous configuration:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Determine what part of the configuration is generic for all the Block Storage services and remove anything that would change when deployed in Red Hat OpenShift Container Platform (RHOCP), such as the <code>connection</code> in the <code>[database]</code> section, the <code>transport_url</code> and <code>log_dir</code> in the <code>[DEFAULT]</code> sections, the whole <code>[coordination]</code> and <code>[barbican]</code> sections. The remaining generic configuration goes into the <code>customServiceConfig</code> option, or a <code>Secret</code> custom resource (CR) and is then used in the <code>customServiceConfigSecrets</code> section, at the <code>cinder: template:</code> level.</p>
</li>
<li>
<p>Determine if there is a scheduler-specific configuration and add it to the <code>customServiceConfig</code> option in <code>cinder: template: cinderScheduler</code>.</p>
</li>
<li>
<p>Determine if there is an API-specific configuration and add it to the <code>customServiceConfig</code> option in <code>cinder: template: cinderAPI</code>.</p>
</li>
<li>
<p>If the Block Storage service backup is deployed, add the Block Storage service backup configuration options to <code>customServiceConfig</code> option, or to a <code>Secret</code> CR that you can add to <code>customServiceConfigSecrets</code> section at the <code>cinder: template:
cinderBackup:</code> level. Remove the <code>host</code> configuration in the <code>[DEFAULT]</code> section to support multiple replicas later.</p>
</li>
<li>
<p>Determine the individual volume back-end configuration for each of the drivers. The configuration is in the specific driver section, and it includes the <code>[backend_defaults]</code> section and FC zoning sections if you use them. The Block Storage service operator does not support a global <code>customServiceConfig</code> option for all volume services. Each back end has its own section under <code>cinder: template: cinderVolumes</code>, and the configuration goes in the <code>customServiceConfig</code> option or in a <code>Secret</code> CR and is then used in the <code>customServiceConfigSecrets</code> section.</p>
</li>
<li>
<p>If any of the Block Storage service volume drivers require a custom vendor image, find the location of the image in the <a href="https://catalog.redhat.com/search?searchType=software">Red Hat Ecosystem Catalog</a>, and create or modify an <code>OpenStackVersion</code> CR to specify the custom image by using the key from the <code>cinderVolumes</code> section.</p>
<div class="paragraph">
<p>For example, if you have the following configuration:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">spec:
  cinder:
    enabled: true
    template:
      cinderVolume:
        pure:
          customServiceConfigSecrets:
            - openstack-cinder-pure-cfg
&lt; . . . &gt;</code></pre>
</div>
</div>
<div class="paragraph">
<p>Then the <code>OpenStackVersion</code> CR that describes the container image for that back end looks like the following example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: core.openstack.org/v1beta1
kind: OpenStackVersion
metadata:
  name: openstack
spec:
  customContainerImages:
    cinderVolumeImages:
      pure: registry.connect.redhat.com/purestorage/openstack-cinder-volume-pure-rhosp-18-0'</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
The name of the <code>OpenStackVersion</code> must match the name of your <code>OpenStackControlPlane</code> CR.
</td>
</tr>
</table>
</div>
</li>
<li>
<p>If your Block Storage services use external files, for example, for a custom policy, or to store credentials or SSL certificate authority bundles to connect to a storage array, make those files available to the right containers. Use <code>Secrets</code> or <code>ConfigMap</code> to store the information in RHOCP and then in the <code>extraMounts</code> key. For example, for Red Hat Ceph Storage credentials that are stored in a <code>Secret</code> called <code>ceph-conf-files</code>, you patch the top-level <code>extraMounts</code> key in the <code>OpenstackControlPlane</code> CR:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">spec:
  extraMounts:
  - extraVol:
    - extraVolType: Ceph
      mounts:
      - mountPath: /etc/ceph
        name: ceph
        readOnly: true
      propagation:
      - CinderVolume
      - CinderBackup
      - Glance
      volumes:
      - name: ceph
        projected:
          sources:
          - secret:
              name: ceph-conf-files</code></pre>
</div>
</div>
</li>
<li>
<p>For a service-specific file, such as the API policy, you add the configuration
on the service itself. In the following example, you include the <code>CinderAPI</code>
configuration that references the policy you are adding from a <code>ConfigMap</code>
called <code>my-cinder-conf</code> that has a <code>policy</code> key with the contents of the policy:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">spec:
  cinder:
    enabled: true
    template:
      cinderAPI:
        customServiceConfig: |
           [oslo_policy]
           policy_file=/etc/cinder/api/policy.yaml
      extraMounts:
      - extraVol:
        - extraVolType: Ceph
          mounts:
          - mountPath: /etc/cinder/api
            name: policy
            readOnly: true
          propagation:
          - CinderAPI
          volumes:
          - name: policy
            projected:
              sources:
              - configMap:
                  name: my-cinder-conf
                  items:
                    - key: policy
                      path: policy.yaml</code></pre>
</div>
</div>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="changes-to-cephFS-through-NFS_storage-requirements">Changes to CephFS through NFS</h4>
<div class="paragraph">
<p>Before you begin the adoption, review the following information to understand the changes to CephFS through NFS between Red&#160;Hat OpenStack Platform (RHOSP) 17.1 and Red&#160;Hat OpenStack Services on OpenShift (RHOSO) 18.0:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>If the RHOSP 17.1 deployment uses CephFS through NFS as a back end for Shared File Systems service (manila), you cannot directly import the <code>ceph-nfs</code> service on the RHOSP Controller nodes into RHOSO 18.0. In RHOSO 18.0, the Shared File Systems service only supports using a clustered NFS service that is directly managed on the Red Hat Ceph Storage cluster. Adoption with the <code>ceph-nfs</code> service involves a data path disruption to existing NFS clients.</p>
</li>
<li>
<p>On RHOSP 17.1, Pacemaker controls the high availability of the <code>ceph-nfs</code> service. This service is assigned a Virtual IP (VIP) address that is also managed by Pacemaker. The VIP is typically created on an isolated <code>StorageNFS</code> network. The Controller nodes have ordering and collocation constraints established between this VIP, <code>ceph-nfs</code>, and the Shared File Systems service (manila) share manager service. Prior to adopting Shared File Systems service, you must adjust the Pacemaker ordering and collocation constraints to separate the share manager service. This establishes <code>ceph-nfs</code> with its VIP as an isolated, standalone NFS service that you can decommission after completing the RHOSO adoption.</p>
</li>
<li>
<p>In Red Hat Ceph Storage 7, a native clustered Ceph NFS service has to be deployed on the Red Hat Ceph Storage cluster by using the Ceph Orchestrator prior to adopting the Shared File Systems service. This NFS service eventually replaces the standalone NFS service from RHOSP 17.1 in your deployment. When the Shared File Systems service is adopted into the RHOSO 18.0 environment, it establishes all the existing exports and client restrictions on the new clustered Ceph NFS service. Clients can continue to read and write data on existing NFS shares, and are not affected until the old standalone NFS service is decommissioned. After the service is decommissioned, you can re-mount the same share from the new clustered Ceph NFS service during a scheduled downtime.</p>
</li>
<li>
<p>To ensure that NFS users are not required to make any networking changes to their existing workloads, assign an IP address from the same isolated <code>StorageNFS</code> network to the clustered Ceph NFS service. NFS users only need to discover and re-mount their shares by using new export paths. When the adoption is complete, RHOSO users can query the Shared File Systems service API to list the export locations on existing shares to identify the preferred paths to mount these shares. These preferred paths correspond to the new clustered Ceph NFS service in contrast to other non-preferred export paths that continue to be displayed until the old isolated, standalone NFS service is decommissioned.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>For more information on setting up a clustered NFS service, see <a href="#creating-a-ceph-nfs-cluster_ceph-prerequisites">Creating an NFS Ganesha cluster</a>.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="red-hat-ceph-storage-prerequisites_configuring-network">Red Hat Ceph Storage prerequisites</h3>
<div class="paragraph">
<p>Before you migrate your Red Hat Ceph Storage cluster daemons from your Controller nodes, complete the following tasks in your Red&#160;Hat OpenStack Platform 17.1 environment:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Upgrade your Red Hat Ceph Storage cluster to release 7. For more information, see <a href="https://docs.redhat.com/en/documentation/red_hat_openstack_platform/17.1/html-single/framework_for_upgrades_16.2_to_17.1/index#assembly_ceph-6-to-7_upgrade_post-upgrade-external-ceph">Upgrading Red Hat Ceph Storage 6 to 7</a> in <em>Framework for upgrades (16.2 to 17.1)</em>.</p>
</li>
<li>
<p>Your Red Hat Ceph Storage 7 deployment is managed by <code>cephadm</code>.</p>
</li>
<li>
<p>The undercloud is still available, and the nodes and networks are managed by director.</p>
</li>
<li>
<p>If you use an externally deployed Red Hat Ceph Storage cluster, you must recreate a <code>ceph-nfs</code> cluster in the target nodes as well as propogate the <code>StorageNFS</code> network.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Complete the prerequisites for your specific Red Hat Ceph Storage environment:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><a href="#completing-prerequisites-for-migrating-ceph-monitoring-stack_ceph-prerequisites">Red Hat Ceph Storage with monitoring stack components</a></p>
</li>
<li>
<p><a href="#completing-prerequisites-for-migrating-ceph-rgw_ceph-prerequisites">Red Hat Ceph Storage RGW</a></p>
</li>
<li>
<p><a href="#completing-prerequisites-for-rbd-migration_ceph-prerequisites">Red Hat Ceph Storage RBD</a></p>
</li>
<li>
<p><a href="#creating-a-ceph-nfs-cluster_ceph-prerequisites">NFS Ganesha</a></p>
</li>
</ul>
</div>
<div class="sect3">
<h4 id="completing-prerequisites-for-migrating-ceph-monitoring-stack_ceph-prerequisites">Completing prerequisites for a Red Hat Ceph Storage cluster with monitoring stack components</h4>
<div class="paragraph">
<p>Complete the following prerequisites before you migrate a Red Hat Ceph Storage cluster with monitoring stack components.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
In addition to updating the container images related to the monitoring stack, you must update the configuration entry related to the <code>container_image_base</code>. This has an impact on all the Red Hat Ceph Storage daemons that rely on the undercloud images.
New daemons are deployed by using the new image registry location that is configured in the Red Hat Ceph Storage cluster.
</td>
</tr>
</table>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Gather the current status of the monitoring stack. Verify that
the hosts have no <code>monitoring</code> label, or <code>grafana</code>, <code>prometheus</code>, or <code>alertmanager</code>, in cases of a per daemons placement evaluation:</p>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
The entire relocation process is driven by <code>cephadm</code> and relies on labels to be
assigned to the target nodes, where the daemons are scheduled.
For more information about assigning labels to nodes, review the Red Hat Knowledgebase article <a href="https://access.redhat.com/articles/1548993">Red Hat Ceph Storage: Supported configurations</a>.
</td>
</tr>
</table>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">[tripleo-admin@controller-0 ~]$ sudo cephadm shell -- ceph orch host ls

HOST                    	ADDR       	LABELS                 	STATUS
cephstorage-0.redhat.local  192.168.24.11  osd mds
cephstorage-1.redhat.local  192.168.24.12  osd mds
cephstorage-2.redhat.local  192.168.24.47  osd mds
controller-0.redhat.local   192.168.24.35  _admin mon mgr
controller-1.redhat.local   192.168.24.53  mon _admin mgr
controller-2.redhat.local   192.168.24.10  mon _admin mgr
6 hosts in cluster</code></pre>
</div>
</div>
<div class="paragraph">
<p>Confirm that the cluster is healthy and that both <code>ceph orch ls</code> and
<code>ceph orch ps</code> return the expected number of deployed daemons.</p>
</div>
</li>
<li>
<p>Review and update the container image registry:</p>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
If you run the Red Hat Ceph Storage externalization procedure after you migrate the Red&#160;Hat OpenStack Platform control plane, update the container images in the Red Hat Ceph Storage cluster configuration. The current container images point to the undercloud registry, which might not be available anymore. Because the undercloud is not available after adoption is complete, replace the undercloud-provided images with an alternative registry.
</td>
</tr>
</table>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">$ ceph config dump
...
...
mgr   advanced  mgr/cephadm/container_image_alertmanager    undercloud-0.ctlplane.redhat.local:8787/rh-osbs/openshift-ose-prometheus-alertmanager:v4.10
mgr   advanced  mgr/cephadm/container_image_base            undercloud-0.ctlplane.redhat.local:8787/rh-osbs/rhceph
mgr   advanced  mgr/cephadm/container_image_grafana         undercloud-0.ctlplane.redhat.local:8787/rh-osbs/grafana:latest
mgr   advanced  mgr/cephadm/container_image_node_exporter   undercloud-0.ctlplane.redhat.local:8787/rh-osbs/openshift-ose-prometheus-node-exporter:v4.10
mgr   advanced  mgr/cephadm/container_image_prometheus      undercloud-0.ctlplane.redhat.local:8787/rh-osbs/openshift-ose-prometheus:v4.10</code></pre>
</div>
</div>
</li>
<li>
<p>Remove the undercloud container images:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">$ cephadm shell -- ceph config rm mgr mgr/cephadm/container_image_base
for i in prometheus grafana alertmanager node_exporter; do
    cephadm shell -- ceph config rm mgr mgr/cephadm/container_image_$i
done</code></pre>
</div>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="completing-prerequisites-for-migrating-ceph-rgw_ceph-prerequisites">Completing prerequisites for Red Hat Ceph Storage RGW migration</h4>
<div class="paragraph">
<p>Complete the following prerequisites before you begin the Ceph Object Gateway (RGW) migration.</p>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Check the current status of the Red Hat Ceph Storage nodes:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">(undercloud) [stack@undercloud-0 ~]$ metalsmith list


    +------------------------+    +----------------+
    | IP Addresses           |    |  Hostname      |
    +------------------------+    +----------------+
    | ctlplane=192.168.24.25 |    | cephstorage-0  |
    | ctlplane=192.168.24.10 |    | cephstorage-1  |
    | ctlplane=192.168.24.32 |    | cephstorage-2  |
    | ctlplane=192.168.24.28 |    | compute-0      |
    | ctlplane=192.168.24.26 |    | compute-1      |
    | ctlplane=192.168.24.43 |    | controller-0   |
    | ctlplane=192.168.24.7  |    | controller-1   |
    | ctlplane=192.168.24.41 |    | controller-2   |
    +------------------------+    +----------------+</code></pre>
</div>
</div>
</li>
<li>
<p>Log in to <code>controller-0</code> and check the Pacemaker status to identify important information for the RGW migration:</p>
<div class="listingblock">
<div class="content">
<pre>Full List of Resources:
  * ip-192.168.24.46	(ocf:heartbeat:IPaddr2):     	Started controller-0
  * ip-10.0.0.103   	(ocf:heartbeat:IPaddr2):     	Started controller-1
  * ip-172.17.1.129 	(ocf:heartbeat:IPaddr2):     	Started controller-2
  * ip-172.17.3.68  	(ocf:heartbeat:IPaddr2):     	Started controller-0
  * ip-172.17.4.37  	(ocf:heartbeat:IPaddr2):     	Started controller-1
  * Container bundle set: haproxy-bundle

[undercloud-0.ctlplane.redhat.local:8787/rh-osbs/rhosp17-openstack-haproxy:pcmklatest]:
    * haproxy-bundle-podman-0   (ocf:heartbeat:podman):  Started controller-2
    * haproxy-bundle-podman-1   (ocf:heartbeat:podman):  Started controller-0
    * haproxy-bundle-podman-2   (ocf:heartbeat:podman):  Started controller-1</pre>
</div>
</div>
</li>
<li>
<p>Identify the ranges of the storage networks. The following is an example and the values might differ in your environment:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">[heat-admin@controller-0 ~]$ ip -o -4 a

1: lo	inet 127.0.0.1/8 scope host lo\   	valid_lft forever preferred_lft forever
2: enp1s0	inet 192.168.24.45/24 brd 192.168.24.255 scope global enp1s0\   	valid_lft forever preferred_lft forever
2: enp1s0	inet 192.168.24.46/32 brd 192.168.24.255 scope global enp1s0\   	valid_lft forever preferred_lft forever
7: br-ex	inet 10.0.0.122/24 brd 10.0.0.255 scope global br-ex\   	valid_lft forever preferred_lft forever <i class="conum" data-value="1"></i><b>(1)</b>
8: vlan70	inet 172.17.5.22/24 brd 172.17.5.255 scope global vlan70\   	valid_lft forever preferred_lft forever
8: vlan70	inet 172.17.5.94/32 brd 172.17.5.255 scope global vlan70\   	valid_lft forever preferred_lft forever
9: vlan50	inet 172.17.2.140/24 brd 172.17.2.255 scope global vlan50\   	valid_lft forever preferred_lft forever
10: vlan30	inet 172.17.3.73/24 brd 172.17.3.255 scope global vlan30\   	valid_lft forever preferred_lft forever <i class="conum" data-value="2"></i><b>(2)</b>
10: vlan30	inet 172.17.3.68/32 brd 172.17.3.255 scope global vlan30\   	valid_lft forever preferred_lft forever
11: vlan20	inet 172.17.1.88/24 brd 172.17.1.255 scope global vlan20\   	valid_lft forever preferred_lft forever
12: vlan40	inet 172.17.4.24/24 brd 172.17.4.255 scope global vlan40\   	valid_lft forever preferred_lft forever</code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td><code>br-ex</code> represents the External Network, where in the current
environment, HAProxy has the front-end Virtual IP (VIP) assigned.</td>
</tr>
<tr>
<td><i class="conum" data-value="2"></i><b>2</b></td>
<td><code>vlan30</code> represents the Storage Network, where the new RGW instances should be started on the Red Hat Ceph Storage nodes.</td>
</tr>
</table>
</div>
</li>
<li>
<p>Identify the network that you previously had in HAProxy and propagate it through director to the Red Hat Ceph Storage nodes. Use this network to reserve a new VIP that is owned by Red Hat Ceph Storage as the entry point for the RGW service.</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>Log in to <code>controller-0</code> and find the <code>ceph_rgw</code> section in the current HAProxy configuration:</p>
<div class="listingblock">
<div class="content">
<pre>$ less /var/lib/config-data/puppet-generated/haproxy/etc/haproxy/haproxy.cfg
...
...
listen ceph_rgw
  bind 10.0.0.103:8080 transparent
  bind 172.17.3.68:8080 transparent
  mode http
  balance leastconn
  http-request set-header X-Forwarded-Proto https if { ssl_fc }
  http-request set-header X-Forwarded-Proto http if !{ ssl_fc }
  http-request set-header X-Forwarded-Port %[dst_port]
  option httpchk GET /swift/healthcheck
  option httplog
  option forwardfor
  server controller-0.storage.redhat.local 172.17.3.73:8080 check fall 5 inter 2000 rise 2
  server controller-1.storage.redhat.local 172.17.3.146:8080 check fall 5 inter 2000 rise 2
  server controller-2.storage.redhat.local 172.17.3.156:8080 check fall 5 inter 2000 rise 2</pre>
</div>
</div>
</li>
<li>
<p>Confirm that the network is used as an HAProxy front end. The following example shows that <code>controller-0</code> exposes the services by using the external network, which is absent from the Red Hat Ceph Storage nodes. You must propagate the external network through director:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">[controller-0]$ ip -o -4 a

...
7: br-ex	inet 10.0.0.106/24 brd 10.0.0.255 scope global br-ex\   	valid_lft forever preferred_lft forever
...</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
If the target nodes are not managed by director, you cannot use this procedure to configure the network. An administrator must manually configure all the required networks.
</td>
</tr>
</table>
</div>
</li>
</ol>
</div>
</li>
<li>
<p>Propagate the HAProxy front-end network to Red Hat Ceph Storage nodes.</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>In the NIC template that you use to define the <code>ceph-storage</code> network interfaces, add the new config section in the Red Hat Ceph Storage network configuration template file, for example, <code>/home/stack/composable_roles/network/nic-configs/ceph-storage.j2</code>:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">---
network_config:
- type: interface
  name: nic1
  use_dhcp: false
  dns_servers: {{ ctlplane_dns_nameservers }}
  addresses:
  - ip_netmask: {{ ctlplane_ip }}/{{ ctlplane_cidr }}
  routes: {{ ctlplane_host_routes }}
- type: vlan
  vlan_id: {{ storage_mgmt_vlan_id }}
  device: nic1
  addresses:
  - ip_netmask: {{ storage_mgmt_ip }}/{{ storage_mgmt_cidr }}
  routes: {{ storage_mgmt_host_routes }}
- type: interface
  name: nic2
  use_dhcp: false
  defroute: false
- type: vlan
  vlan_id: {{ storage_vlan_id }}
  device: nic2
  addresses:
  - ip_netmask: {{ storage_ip }}/{{ storage_cidr }}
  routes: {{ storage_host_routes }}
- type: ovs_bridge
  name: {{ neutron_physical_bridge_name }}
  dns_servers: {{ ctlplane_dns_nameservers }}
  domain: {{ dns_search_domains }}
  use_dhcp: false
  addresses:
  - ip_netmask: {{ external_ip }}/{{ external_cidr }}
  routes: {{ external_host_routes }}
  members: []
  - type: interface
    name: nic3
    primary: true</code></pre>
</div>
</div>
</li>
<li>
<p>Add the External Network to the bare metal file, for example, <code>/home/stack/composable_roles/network/baremetal_deployment.yaml</code> that is used by <code>metalsmith</code>:</p>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Ensure that <em>network_config_update</em> is enabled for network propagation to the target nodes when <code>os-net-config</code> is triggered.
</td>
</tr>
</table>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">- name: CephStorage
  count: 3
  hostname_format: cephstorage-%index%
  instances:
  - hostname: cephstorage-0
  name: ceph-0
  - hostname: cephstorage-1
  name: ceph-1
  - hostname: cephstorage-2
  name: ceph-2
  defaults:
  profile: ceph-storage
  network_config:
      template: /home/stack/composable_roles/network/nic-configs/ceph-storage.j2
      network_config_update: true
  networks:
  - network: ctlplane
      vif: true
  - network: storage
  - network: storage_mgmt
  - network: external</code></pre>
</div>
</div>
</li>
<li>
<p>Configure the new network on the bare metal nodes:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">(undercloud) [stack@undercloud-0]$

openstack overcloud node provision
   -o overcloud-baremetal-deployed-0.yaml \
   --stack overcloud \
   --network-config -y \
  $PWD/composable_roles/network/baremetal_deployment.yaml</code></pre>
</div>
</div>
</li>
<li>
<p>Verify that the new network is configured on the Red Hat Ceph Storage nodes:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">[root@cephstorage-0 ~]# ip -o -4 a

1: lo	inet 127.0.0.1/8 scope host lo\   	valid_lft forever preferred_lft forever
2: enp1s0	inet 192.168.24.54/24 brd 192.168.24.255 scope global enp1s0\   	valid_lft forever preferred_lft forever
11: vlan40	inet 172.17.4.43/24 brd 172.17.4.255 scope global vlan40\   	valid_lft forever preferred_lft forever
12: vlan30	inet 172.17.3.23/24 brd 172.17.3.255 scope global vlan30\   	valid_lft forever preferred_lft forever
14: br-ex	inet 10.0.0.133/24 brd 10.0.0.255 scope global br-ex\   	valid_lft forever preferred_lft forever</code></pre>
</div>
</div>
</li>
</ol>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="completing-prerequisites-for-rbd-migration_ceph-prerequisites">Completing prerequisites for a Red Hat Ceph Storage RBD migration</h4>
<div class="paragraph">
<p>Complete the following prerequisites before you begin the Red Hat Ceph Storage Rados Block Device (RBD) migration.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The target CephStorage or ComputeHCI nodes are configured to have both <code>storage</code> and <code>storage_mgmt</code> networks. This ensures that you can use both Red Hat Ceph Storage public and cluster networks from the same node. From Red&#160;Hat OpenStack Platform 17.1 and later you do not have to run a stack update.</p>
</li>
<li>
<p>NFS Ganesha is migrated from a director deployment to <code>cephadm</code>. For more information, see <a href="#creating-a-ceph-nfs-cluster_ceph-prerequisites">Creating an NFS Ganesha
cluster</a>.</p>
</li>
<li>
<p>Ceph Metadata Server, monitoring stack, Ceph Object Gateway, and any other daemon that is deployed on Controller nodes.</p>
</li>
<li>
<p>The daemons distribution follows the cardinality constraints that are
described in <a href="https://access.redhat.com/articles/1548993">Red Hat Ceph
Storage: Supported configurations</a>.</p>
</li>
<li>
<p>The Red Hat Ceph Storage cluster is healthy, and the <code>ceph -s</code> command returns <code>HEALTH_OK</code>.</p>
</li>
<li>
<p>Run <code>os-net-config</code> on the bare metal node and configure additional networks:</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>If target nodes are <code>CephStorage</code>, ensure that the network is defined in the
bare metal file for the <code>CephStorage</code> nodes, for example, <code>/home/stack/composable_roles/network/baremetal_deployment.yaml</code>:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">- name: CephStorage
count: 2
instances:
- hostname: oc0-ceph-0
name: oc0-ceph-0
- hostname: oc0-ceph-1
name: oc0-ceph-1
defaults:
networks:
- network: ctlplane
vif: true
- network: storage_cloud_0
subnet: storage_cloud_0_subnet
- network: storage_mgmt_cloud_0
subnet: storage_mgmt_cloud_0_subnet
network_config:
template: templates/single_nic_vlans/single_nic_vlans_storage.j2</code></pre>
</div>
</div>
</li>
<li>
<p>Add the missing network:</p>
<div class="listingblock">
<div class="content">
<pre>$ openstack overcloud node provision \
-o overcloud-baremetal-deployed-0.yaml --stack overcloud-0 \
/--network-config -y --concurrency 2 /home/stack/metalsmith-0.yaml</pre>
</div>
</div>
</li>
<li>
<p>Verify that the storage network is configured on the target nodes:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">(undercloud) [stack@undercloud ~]$ ssh heat-admin@192.168.24.14 ip -o -4 a
1: lo    inet 127.0.0.1/8 scope host lo\       valid_lft forever preferred_lft forever
5: br-storage    inet 192.168.24.14/24 brd 192.168.24.255 scope global br-storage\       valid_lft forever preferred_lft forever
6: vlan1    inet 192.168.24.14/24 brd 192.168.24.255 scope global vlan1\       valid_lft forever preferred_lft forever
7: vlan11    inet 172.16.11.172/24 brd 172.16.11.255 scope global vlan11\       valid_lft forever preferred_lft forever
8: vlan12    inet 172.16.12.46/24 brd 172.16.12.255 scope global vlan12\       valid_lft forever preferred_lft forever</code></pre>
</div>
</div>
</li>
</ol>
</div>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="creating-a-ceph-nfs-cluster_ceph-prerequisites">Creating an NFS Ganesha cluster</h4>
<div class="paragraph">
<p>If you use CephFS through NFS with the Shared File Systems service (manila), you must create a new clustered NFS service on the Red Hat Ceph Storage cluster. This service replaces the standalone, Pacemaker-controlled <code>ceph-nfs</code> service that you use in Red&#160;Hat OpenStack Platform (RHOSP) 17.1.</p>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Identify the Red Hat Ceph Storage nodes to deploy the new clustered NFS service, for example, <code>cephstorage-0</code>, <code>cephstorage-1</code>, <code>cephstorage-2</code>.</p>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
You must deploy this service on the <code>StorageNFS</code> isolated network so that you can mount your existing shares through the new NFS export locations.
You can deploy the new clustered NFS service on your existing CephStorage nodes or HCI nodes, or on new hardware that you enrolled in the Red Hat Ceph Storage cluster.
</td>
</tr>
</table>
</div>
</li>
<li>
<p>If you deployed your Red Hat Ceph Storage nodes with director, propagate the <code>StorageNFS</code> network to the target nodes where the <code>ceph-nfs</code> service is deployed.</p>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
If the target nodes are not managed by director, you cannot use this procedure to configure the network. An administrator must manually configure all the required networks.
</td>
</tr>
</table>
</div>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>Identify the node definition file, <code>overcloud-baremetal-deploy.yaml</code>, that is used in the RHOSP environment.
For more information about identifying the <code>overcloud-baremetal-deploy.yaml</code> file, see <a href="https://docs.redhat.com/en/documentation/red_hat_openstack_services_on_openshift/18.0/html/customizing_the_red_hat_openstack_services_on_openshift_deployment/index#assembly_customizing-overcloud-networks">Customizing overcloud networks</a> in <em>Customizing the Red Hat OpenStack Services on OpenShift deployment</em>.</p>
</li>
<li>
<p>Edit the networks that are associated with the Red Hat Ceph Storage nodes to include the <code>StorageNFS</code> network:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">- name: CephStorage
  count: 3
  hostname_format: cephstorage-%index%
  instances:
  - hostname: cephstorage-0
    name: ceph-0
  - hostname: cephstorage-1
    name: ceph-1
  - hostname: cephstorage-2
    name: ceph-2
  defaults:
    profile: ceph-storage
    network_config:
      template: /home/stack/network/nic-configs/ceph-storage.j2
      network_config_update: true
    networks:
    - network: ctlplane
      vif: true
    - network: storage
    - network: storage_mgmt
    - network: storage_nfs</code></pre>
</div>
</div>
</li>
<li>
<p>Edit the network configuration template file, for example, <code>/home/stack/network/nic-configs/ceph-storage.j2</code>, for the Red Hat Ceph Storage nodes
to include an interface that connects to the <code>StorageNFS</code> network:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">- type: vlan
  device: nic2
  vlan_id: {{ storage_nfs_vlan_id }}
  addresses:
  - ip_netmask: {{ storage_nfs_ip }}/{{ storage_nfs_cidr }}
  routes: {{ storage_nfs_host_routes }}</code></pre>
</div>
</div>
</li>
<li>
<p>Update the Red Hat Ceph Storage nodes:</p>
<div class="listingblock">
<div class="content">
<pre>$ openstack overcloud node provision \
    --stack overcloud   \
    --network-config -y  \
    -o overcloud-baremetal-deployed-storage_nfs.yaml \
    --concurrency 2 \
    /home/stack/network/baremetal_deployment.yaml</pre>
</div>
</div>
<div class="paragraph">
<p>When the update is complete, ensure that a new interface is created in theRed Hat Ceph Storage nodes and that they are tagged with the VLAN that is associated with <code>StorageNFS</code>.</p>
</div>
</li>
</ol>
</div>
</li>
<li>
<p>Identify the IP address from the <code>StorageNFS</code> network to use as the Virtual IP
address (VIP) for the Ceph NFS service:</p>
<div class="listingblock">
<div class="content">
<pre>$ openstack port list -c "Fixed IP Addresses" --network storage_nfs</pre>
</div>
</div>
</li>
<li>
<p>In a running <code>cephadm</code> shell, identify the hosts for the NFS service:</p>
<div class="listingblock">
<div class="content">
<pre>$ ceph orch host ls</pre>
</div>
</div>
</li>
<li>
<p>Label each host that you identified. Repeat this command for each host that you want to label:</p>
<div class="listingblock">
<div class="content">
<pre>$ ceph orch host label add &lt;hostname&gt; nfs</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Replace <code>&lt;hostname&gt;</code> with the name of the host that you identified.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Create the NFS cluster:</p>
<div class="listingblock">
<div class="content">
<pre>$ ceph nfs cluster create cephfs \
    "label:nfs" \
    --ingress \
    --virtual-ip=&lt;VIP&gt; \
    --ingress-mode=haproxy-protocol</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Replace <code>&lt;VIP&gt;</code> with the VIP for the Ceph NFS service.</p>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
You must set the <code>ingress-mode</code> argument to <code>haproxy-protocol</code>. No other
ingress-mode is supported. This ingress mode allows you to enforce client
restrictions through the Shared File Systems service.
For more information on deploying the clustered Ceph NFS service, see the
<a href="https://access.redhat.com/documentation/en-us/red_hat_ceph_storage/7/html/operations_guide/index#management-of-nfs-ganesha-gateway-using-the-ceph-orchestrator">Management of NFS-Ganesha gateway using the Ceph Orchestrator (Limited Availability)</a> in <em>Red Hat Ceph Storage 7 Operations Guide</em>.
</td>
</tr>
</table>
</div>
</li>
</ul>
</div>
</li>
<li>
<p>Check the status of the NFS cluster:</p>
<div class="listingblock">
<div class="content">
<pre>$ ceph nfs cluster ls
$ ceph nfs cluster info cephfs</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
</div>
<div class="sect2">
<h3 id="comparing-configuration-files-between-deployments_configuring-network">Comparing configuration files between deployments</h3>
<div class="paragraph">
<p>To help you manage the configuration for your director and Red&#160;Hat OpenStack Platform (RHOSP) services, you can compare the configuration files between your director deployment and the Red&#160;Hat OpenStack Services on OpenShift (RHOSO) cloud by using the os-diff tool.</p>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>Golang is installed and configured on your environment:</p>
<div class="listingblock">
<div class="content">
<pre>dnf install -y golang-github-openstack-k8s-operators-os-diff</pre>
</div>
</div>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Configure the <code>/etc/os-diff/os-diff.cfg</code> file and the <code>/etc/os-diff/ssh.config</code> file according to your environment. To allow os-diff to connect to your clouds and pull files from the services that you describe in the <code>config.yaml</code> file, you must set the following options in the <code>os-diff.cfg</code> file:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">[Default]

local_config_dir=/tmp/
service_config_file=config.yaml

[Tripleo]

ssh_cmd=ssh -F ssh.config <i class="conum" data-value="1"></i><b>(1)</b>
director_host=standalone <i class="conum" data-value="2"></i><b>(2)</b>
container_engine=podman
connection=ssh
remote_config_path=/tmp/tripleo
local_config_path=/tmp/

[Openshift]

ocp_local_config_path=/tmp/ocp
connection=local
ssh_cmd=""</code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>Instructs os-diff to access your director host through SSH. The default value is <code>ssh -F ssh.config</code>. However, you can set the value without an ssh.config file, for example, <code>ssh -i /home/user/.ssh/id_rsa stack@my.undercloud.local</code>.</td>
</tr>
<tr>
<td><i class="conum" data-value="2"></i><b>2</b></td>
<td>The host to use to access your cloud, and the podman/docker binary is installed and allowed to interact with the running containers. You can leave this key blank.</td>
</tr>
</table>
</div>
</li>
<li>
<p>If you use a host file to connect to your cloud, configure the <code>ssh.config</code> file to allow os-diff to access your RHOSP environment, for example:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">Host *
    IdentitiesOnly yes

Host virthost
    Hostname virthost
    IdentityFile ~/.ssh/id_rsa
    User root
    StrictHostKeyChecking no
    UserKnownHostsFile=/dev/null


Host standalone
    Hostname standalone
    IdentityFile &lt;path to SSH key&gt;
    User root
    StrictHostKeyChecking no
    UserKnownHostsFile=/dev/null

Host crc
    Hostname crc
    IdentityFile ~/.ssh/id_rsa
    User stack
    StrictHostKeyChecking no
    UserKnownHostsFile=/dev/null</code></pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Replace <code>&lt;path to SSH key&gt;</code> with the path to your SSH key. You must provide a value for <code>IdentityFile</code> to get full working access to your RHOSP environment.</p>
</li>
</ul>
</div>
</li>
<li>
<p>If you use an inventory file to connect to your cloud, generate the <code>ssh.config</code> file from your Ansible inventory, for example, <code>tripleo-ansible-inventory.yaml</code> file:</p>
<div class="listingblock">
<div class="content">
<pre>$ os-diff configure -i tripleo-ansible-inventory.yaml -o ssh.config --yaml</pre>
</div>
</div>
</li>
</ol>
</div>
<div class="ulist">
<div class="title">Verification</div>
<ul>
<li>
<p>Test your connection:</p>
<div class="listingblock">
<div class="content">
<pre>$ ssh -F ssh.config standalone</pre>
</div>
</div>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="migrating-tls-everywhere_configuring-network">Migrating TLS-e to the RHOSO deployment</h2>
<div class="sectionbody">
<div class="paragraph">
<p>If you enabled TLS everywhere (TLS-e) in your Red&#160;Hat OpenStack Platform (RHOSP) 17.1 deployment, you must migrate TLS-e to the Red&#160;Hat OpenStack Services on OpenShift (RHOSO) deployment.</p>
</div>
<div class="paragraph">
<p>The RHOSO deployment uses the cert-manager operator to issue, track, and renew the certificates. In the following procedure, you extract the CA signing certificate from the FreeIPA instance that you use to provide the certificates in the RHOSP environment, and then import them into cert-manager in the RHOSO environment. As a result, you minimize the disruption on the Compute nodes because you do not need to install a new chain of trust.</p>
</div>
<div class="paragraph">
<p>You then decommission the previous FreeIPA node and no longer use it to issue certificates. This might not be possible if you use the IPA server to issue certificates for non-RHOSP systems.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="ulist">
<ul>
<li>
<p>The following procedure was reproduced on a FreeIPA 4.10.1 server. The location of the files and directories might change depending on the version.</p>
</li>
<li>
<p>If the signing keys are stored in an hardware security module (HSM) instead of an NSS shared database (NSSDB), and the keys are retrievable, special HSM utilities might be required.</p>
</li>
</ul>
</div>
</td>
</tr>
</table>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>Your RHOSP deployment is using TLS-e.</p>
</li>
<li>
<p>Ensure that the back-end services on the new deployment are not started yet.</p>
</li>
<li>
<p>Define the following shell variables. The values are examples and refer to a single-node standalone director deployment. Replace these example values with values that are correct for your environment:</p>
<div class="listingblock">
<div class="content">
<pre>IPA_SSH="ssh -i &lt;path_to_ssh_key&gt; root@&lt;freeipa-server-ip-address&gt;"</pre>
</div>
</div>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>To locate the CA certificate and key, list all the certificates inside your NSSDB:</p>
<div class="listingblock">
<div class="content">
<pre>$ IPA_SSH certutil -L -d /etc/pki/pki-tomcat/alias</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>The <code>-L</code> option lists all certificates.</p>
</li>
<li>
<p>The <code>-d</code> option specifies where the certificates are stored.</p>
<div class="paragraph">
<p>The command produces an output similar to the following example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>Certificate Nickname                                         Trust Attributes
                                                             SSL,S/MIME,JAR/XPI

caSigningCert cert-pki-ca                                    CTu,Cu,Cu
ocspSigningCert cert-pki-ca                                  u,u,u
Server-Cert cert-pki-ca                                      u,u,u
subsystemCert cert-pki-ca                                    u,u,u
auditSigningCert cert-pki-ca                                 u,u,Pu</pre>
</div>
</div>
</li>
</ul>
</div>
</li>
<li>
<p>Export the certificate and key from the <code>/etc/pki/pki-tomcat/alias</code> directory. The following example uses the <code>caSigningCert cert-pki-ca</code> certificate:</p>
<div class="listingblock">
<div class="content">
<pre>$IPA_SSH pk12util -o /tmp/freeipa.p12 -n 'caSigningCert\ cert-pki-ca' -d /etc/pki/pki-tomcat/alias -k /etc/pki/pki-tomcat/alias/pwdfile.txt -w /etc/pki/pki-tomcat/alias/pwdfile.txt</pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>The command generates a P12 file with both the certificate and the key. The <code>/etc/pki/pki-tomcat/alias/pwdfile.txt</code> file contains the password that protects the key. You can use the password to both extract the key and generate the new file, <code>/tmp/freeipa.p12</code>. You can also choose another password. If you choose a different password for the new file, replace the parameter of the <code>-w</code> option, or use the <code>-W</code> option followed by the password, in clear text.</p>
</div>
<div class="paragraph">
<p>With that file, you can also get the certificate and the key by using the <code>openssl pkcs12</code> command.</p>
</div>
</td>
</tr>
</table>
</div>
</li>
<li>
<p>Create the secret that contains the root CA:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc create secret generic rootca-internal</pre>
</div>
</div>
</li>
<li>
<p>Import the certificate and the key from FreeIPA:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc patch secret rootca-internal -p="{\"data\":{\"ca.crt\": \"`$IPA_SSH openssl pkcs12 -in /tmp/freeipa.p12 -passin file:/etc/pki/pki-tomcat/alias/pwdfile.txt -nokeys | openssl x509 | base64 -w 0`\"}}"

$ oc patch secret rootca-internal -p="{\"data\":{\"tls.crt\": \"`$IPA_SSH openssl pkcs12 -in /tmp/freeipa.p12 -passin file:/etc/pki/pki-tomcat/alias/pwdfile.txt -nokeys | openssl x509 | base64 -w 0`\"}}"

$ oc patch secret rootca-internal -p="{\"data\":{\"tls.key\": \"`$IPA_SSH openssl pkcs12 -in /tmp/freeipa.p12 -passin file:/etc/pki/pki-tomcat/alias/pwdfile.txt -nocerts -noenc | openssl rsa | base64 -w 0`\"}}"</pre>
</div>
</div>
</li>
<li>
<p>Create the cert-manager issuer and reference the secret:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">$ oc apply -f - &lt;&lt;EOF
apiVersion: cert-manager.io/v1
kind: Issuer
metadata:
  name: rootca-internal
  labels:
    osp-rootca-issuer-public: ""
    osp-rootca-issuer-internal: ""
    osp-rootca-issuer-libvirt: ""
    osp-rootca-issuer-ovn: ""
spec:
  ca:
    secretName: rootca-internal
EOF</code></pre>
</div>
</div>
</li>
<li>
<p>Delete the previously created p12 files:</p>
<div class="listingblock">
<div class="content">
<pre>$IPA_SSH rm /tmp/freeipa.p12</pre>
</div>
</div>
</li>
</ol>
</div>
<div class="ulist">
<div class="title">Verification</div>
<ul>
<li>
<p>Verify that the necessary resources are created:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc get issuers</pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre>$ oc get secret rootca-internal -o yaml</pre>
</div>
</div>
</li>
</ul>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
After the adoption is complete, the cert-manager operator issues new certificates and updates the secrets with the new certificates. As a result, the pods on the control plane automatically restart in order to obtain the new certificates. On the data plane, you must manually initiate a new deployment and restart certain processes to use the new certificates. The old certificates remain active until both the control plane and data plane obtain the new certificates.
</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect1">
<h2 id="migrating-databases-to-the-control-plane_configuring-network">Migrating databases to the control plane</h2>
<div class="sectionbody">
<div class="paragraph">
<p>To begin creating the control plane, enable back-end services and import the databases from your original Red&#160;Hat OpenStack Platform 17.1 deployment.</p>
</div>
<div class="sect2">
<h3 id="proc_retrieving-topology-specific-service-configuration_migrating-databases">Retrieving topology-specific service configuration</h3>
<div class="paragraph">
<p>Before you migrate your databases to the Red&#160;Hat OpenStack Services on OpenShift (RHOSO) control plane, retrieve the topology-specific service configuration from your Red&#160;Hat OpenStack Platform (RHOSP) environment. You need this configuration for the following reasons:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>To check your current database for inaccuracies</p>
</li>
<li>
<p>To ensure that you have the data you need before the migration</p>
</li>
<li>
<p>To compare your RHOSP database with the adopted RHOSO database</p>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Prerequisites</div>
<ol class="arabic">
<li>
<p>Define the following shell variables. Replace the example values with values that are correct for your environment:</p>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
If you use IPv6, define the <code>SOURCE_MARIADB_IP</code> value without brackets. For example, <code>SOURCE_MARIADB_IP=fd00:bbbb::2</code>.
</td>
</tr>
</table>
</div>
<div class="listingblock">
<div class="content">
<pre>$ PASSWORD_FILE="$HOME/overcloud-passwords.yaml"
$ MARIADB_IMAGE=registry.redhat.io/rhoso/openstack-mariadb-rhel9:18.0
$ declare -A TRIPLEO_PASSWORDS
$ CELLS="default cell1 cell2"
$ for CELL in $(echo $CELLS); do
&gt;    TRIPLEO_PASSWORDS[$CELL]="$PASSWORD_FILE"
&gt; done
$ declare -A SOURCE_DB_ROOT_PASSWORD
$ for CELL in $(echo $CELLS); do
&gt;     SOURCE_DB_ROOT_PASSWORD[$CELL]=$(cat ${TRIPLEO_PASSWORDS[$CELL]} | grep ' MysqlRootPassword:' | awk -F ': ' '{ print $2; }')
&gt; done</pre>
</div>
</div>
</li>
<li>
<p>Define the following shell variables. Replace the example values with values that are correct for your environment:</p>
<div class="listingblock">
<div class="content">
<pre>$ MARIADB_CLIENT_ANNOTATIONS='--annotations=k8s.v1.cni.cncf.io/networks=internalapi'
$ MARIADB_RUN_OVERRIDES="$MARIADB_CLIENT_ANNOTATIONS"

$ CONTROLLER1_SSH="ssh -i *&lt;path to SSH key&gt;* root@*&lt;node IP&gt;*"

$ declare -A SOURCE_MARIADB_IP
$ SOURCE_MARIADB_IP[default]=*&lt;galera cluster VIP&gt;*
$ SOURCE_MARIADB_IP[cell1]=*&lt;galera cell1 cluster VIP&gt;*
$ SOURCE_MARIADB_IP[cell2]=*&lt;galera cell2 cluster VIP&gt;*
# ...</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Provide <code>CONTROLLER1_SSH</code> settings with SSH connection details for any non-cell Controller of the source director cloud.</p>
</li>
<li>
<p>For each cell that is defined in <code>CELLS</code>, replace <code>SOURCE_MARIADB_IP[*]= ...</code>, with the records lists for the cell names and VIP addresses of MariaDB Galera clusters, including the cells, of the source director cloud.</p>
</li>
<li>
<p>To get the values for <code>SOURCE_MARIADB_IP</code>, query the puppet-generated configurations in a Controller
and CellController
node:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo grep -rI 'listen mysql' -A10 /var/lib/config-data/puppet-generated/ | grep bind</pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
The source cloud always uses the same password for cells databases. For that reason, the same passwords file is used for all cells stacks. However, split-stack topology allows using different passwords files for each stack.
</td>
</tr>
</table>
</div>
</li>
</ul>
</div>
</li>
</ol>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Export the shell variables for the following outputs and test the connection to the RHOSP database:</p>
<div class="listingblock">
<div class="content">
<pre>$ unset PULL_OPENSTACK_CONFIGURATION_DATABASES
$ declare -xA PULL_OPENSTACK_CONFIGURATION_DATABASES
$ for CELL in $(echo $CELLS); do
&gt;     PULL_OPENSTACK_CONFIGURATION_DATABASES[$CELL]=$(oc run mariadb-client-1-$CELL ${MARIADB_RUN_OVERRIDES} -q --image ${MARIADB_IMAGE} -i --rm --restart=Never -- \
&gt;         mysql -rsh "${SOURCE_MARIADB_IP[$CELL]}" -uroot -p"${SOURCE_DB_ROOT_PASSWORD[$CELL]}" -e 'SHOW databases;')
&gt; done</pre>
</div>
</div>
<div class="paragraph">
<p>If the connection is successful, the expected output is nothing.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
The <code>nova</code>, <code>nova_api</code>, and <code>nova_cell0</code> databases are included in the same database host for the main overcloud Orchestration service (heat) stack.
Additional cells use the <code>nova</code> database of their local Galera clusters.
</td>
</tr>
</table>
</div>
</li>
<li>
<p>Run <code>mysqlcheck</code> on the RHOSP database to check for inaccuracies:</p>
<div class="listingblock">
<div class="content">
<pre>$ unset PULL_OPENSTACK_CONFIGURATION_MYSQLCHECK_NOK
$ declare -xA PULL_OPENSTACK_CONFIGURATION_MYSQLCHECK_NOK
$ run_mysqlcheck() {
&gt;     PULL_OPENSTACK_CONFIGURATION_MYSQLCHECK_NOK=$(oc run mariadb-client-2-$1 ${MARIADB_RUN_OVERRIDES} -q --image ${MARIADB_IMAGE} -i --rm --restart=Never -- \
&gt;         mysqlcheck --all-databases -h ${SOURCE_MARIADB_IP[$CELL]} -u root -p"${SOURCE_DB_ROOT_PASSWORD[$CELL]}" | grep -v OK)
&gt; }
$ for CELL in $(echo $CELLS); do
&gt;     run_mysqlcheck $CELL
&gt; done
$ if [ "$PULL_OPENSTACK_CONFIGURATION_MYSQLCHECK_NOK" != "" ]; then
&gt;     for CELL in $(echo $CELLS); do
&gt;         MYSQL_UPGRADE=$(oc run mariadb-client-3 ${MARIADB_RUN_OVERRIDES} -q --image ${MARIADB_IMAGE} -i --rm --restart=Never -- \
&gt;             mysql_upgrade -v -h ${SOURCE_MARIADB_IP[$CELL]} -u root -p"${SOURCE_DB_ROOT_PASSWORD[$CELL]}")
&gt;         # rerun mysqlcheck to check if problem is resolved
&gt;         run_mysqlcheck
&gt;     done
&gt; fi</pre>
</div>
</div>
</li>
<li>
<p>Get the Compute service (nova) cell mappings:</p>
<div class="listingblock">
<div class="content">
<pre>export PULL_OPENSTACK_CONFIGURATION_NOVADB_MAPPED_CELLS=$(oc run mariadb-client-1 ${MARIADB_RUN_OVERRIDES} -q --image ${MARIADB_IMAGE} -i --rm --restart=Never -- \
    mysql -rsh "${SOURCE_MARIADB_IP['default]}" -uroot -p"${SOURCE_DB_ROOT_PASSWORD['default']}" nova_api -e \
    'select uuid,name,transport_url,database_connection,disabled from cell_mappings;')</pre>
</div>
</div>
</li>
<li>
<p>Get the hostnames of the registered Compute services:</p>
<div class="listingblock">
<div class="content">
<pre>$ unset PULL_OPENSTACK_CONFIGURATION_NOVA_COMPUTE_HOSTNAMES
$ declare -xA PULL_OPENSTACK_CONFIGURATION_NOVA_COMPUTE_HOSTNAMES
$ for CELL in $(echo $CELLS); do
&gt;     PULL_OPENSTACK_CONFIGURATION_NOVA_COMPUTE_HOSTNAMES[$CELL]=$(oc run mariadb-client-4-$CELL ${MARIADB_RUN_OVERRIDES} -q --image ${MARIADB_IMAGE} -i --rm --restart=Never -- \
&gt;         mysql -rsh "${SOURCE_MARIADB_IP[$CELL]}" -uroot -p"${SOURCE_DB_ROOT_PASSWORD[$CELL]}" -e \
&gt;             "select host from nova.services where services.binary='nova-compute' and deleted=0;")
&gt; done</pre>
</div>
</div>
</li>
<li>
<p>Get the list of the mapped Compute service cells:</p>
<div class="listingblock">
<div class="content">
<pre>export PULL_OPENSTACK_CONFIGURATION_NOVAMANAGE_CELL_MAPPINGS=$($CONTROLLER1_SSH sudo podman exec -it nova_conductor nova-manage cell_v2 list_cells)</pre>
</div>
</div>
</li>
<li>
<p>Store the exported variables for future use:</p>
<div class="listingblock">
<div class="content">
<pre>$ unset SRIOV_AGENTS
$ declare -xA SRIOV_AGENTS <i class="conum" data-value="1"></i><b>(1)</b>
$ for CELL in $(echo $CELLS); do
&gt;     RCELL=$CELL
&gt;     [ "$CELL" = "$DEFAULT_CELL_NAME" ] &amp;&amp; RCELL=default
&gt;     cat &gt; ~/.source_cloud_exported_variables_$CELL &lt;&lt; EOF
&gt; unset PULL_OPENSTACK_CONFIGURATION_DATABASES
&gt; unset PULL_OPENSTACK_CONFIGURATION_MYSQLCHECK_NOK
&gt; unset PULL_OPENSTACK_CONFIGURATION_NOVA_COMPUTE_HOSTNAMES
&gt; declare -xA PULL_OPENSTACK_CONFIGURATION_DATABASES
&gt; declare -xA PULL_OPENSTACK_CONFIGURATION_MYSQLCHECK_NOK
&gt; declare -xA PULL_OPENSTACK_CONFIGURATION_NOVA_COMPUTE_HOSTNAMES
&gt; PULL_OPENSTACK_CONFIGURATION_DATABASES[$CELL]="$(oc run mariadb-client-5-$CELL ${MARIADB_RUN_OVERRIDES} -q --image ${MARIADB_IMAGE} -i --rm --restart=Never -- \
&gt;     mysql -rsh ${SOURCE_MARIADB_IP[$RCELL]} -uroot -p${SOURCE_DB_ROOT_PASSWORD[$RCELL]} -e 'SHOW databases;')"
&gt; PULL_OPENSTACK_CONFIGURATION_MYSQLCHECK_NOK[$CELL]="$(oc run mariadb-client-6-$CELL ${MARIADB_RUN_OVERRIDES} -q --image ${MARIADB_IMAGE} -i --rm --restart=Never -- \
&gt;     mysqlcheck --all-databases -h ${SOURCE_MARIADB_IP[$RCELL]} -u root -p${SOURCE_DB_ROOT_PASSWORD[$RCELL]} | grep -v OK)"
&gt; PULL_OPENSTACK_CONFIGURATION_NOVA_COMPUTE_HOSTNAMES[$CELL]="$(oc run mariadb-client-7-$CELL ${MARIADB_RUN_OVERRIDES} -q --image ${MARIADB_IMAGE} -i --rm --restart=Never -- \
&gt;     mysql -rsh ${SOURCE_MARIADB_IP[$RCELL]} -uroot -p${SOURCE_DB_ROOT_PASSWORD[$RCELL]} -e \
&gt;     "select host from nova.services where services.binary='nova-compute' and deleted=0;")"
&gt; if [ "$RCELL" = "default" ]; then
&gt;     PULL_OPENSTACK_CONFIGURATION_NOVADB_MAPPED_CELLS="$(oc run mariadb-client-2 ${MARIADB_RUN_OVERRIDES} -q --image ${MARIADB_IMAGE} -i --rm --restart=Never -- \
&gt;         mysql -rsh ${SOURCE_MARIADB_IP[$RCELL]} -uroot -p${SOURCE_DB_ROOT_PASSWORD[$RCELL]} nova_api -e \
&gt;             'select uuid,name,transport_url,database_connection,disabled from cell_mappings;')"
&gt;     PULL_OPENSTACK_CONFIGURATION_NOVAMANAGE_CELL_MAPPINGS="$($CONTROLLER1_SSH sudo podman exec -it nova_conductor nova-manage cell_v2 list_cells)"
&gt; fi
&gt; EOF
&gt; done
$ chmod 0600 ~/.source_cloud_exported_variables*</pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>If <code>neutron-sriov-nic-agent</code> agents are running in your RHOSP deployment, get the configuration to use for the data plane adoption.</td>
</tr>
</table>
</div>
</li>
</ol>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
This configuration and the exported values are required later, during the data plane adoption post-checks. After the RHOSP control plane services are shut down, if any of the exported values are lost, re-running the <code>export</code> command fails because the control plane services are no longer running on the source cloud, and the data cannot be retrieved. To avoid data loss, preserve the exported values in an environment file before shutting down the control plane services.
</td>
</tr>
</table>
</div>
</div>
<div class="sect2">
<h3 id="deploying-backend-services_migrating-databases">Deploying back-end services</h3>
<div class="paragraph">
<p>Create the <code>OpenStackControlPlane</code> custom resource (CR) with the basic back-end services deployed, and disable all the Red&#160;Hat OpenStack Platform (RHOSP) services. This CR is the foundation of the control plane.</p>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>The cloud that you want to adopt is running, and it is on the latest minor version of RHOSP 17.1.</p>
</li>
<li>
<p>All control plane and data plane hosts of the source cloud are running, and continue to run throughout the adoption procedure.</p>
</li>
<li>
<p>The <code>openstack-operator</code> is deployed, but <code>OpenStackControlPlane</code> is
not deployed.</p>
</li>
<li>
<p>Install the OpenStack Operators. For more information, see <a href="https://docs.redhat.com/en/documentation/red_hat_openstack_services_on_openshift/18.0/html-single/deploying_red_hat_openstack_services_on_openshift/index#assembly_installing-and-preparing-the-Operators">Installing and preparing the Operators</a> in <em>Deploying Red Hat OpenStack Services on OpenShift</em>.</p>
</li>
<li>
<p>If you enabled TLS everywhere (TLS-e) on the RHOSP environment, you must copy the <code>tls</code> root CA from the RHOSP environment to the <code>rootca-internal</code> issuer.</p>
</li>
<li>
<p>There are free PVs available for Galera and RabbitMQ.</p>
</li>
<li>
<p>Set the desired admin password for the control plane deployment. This can
be the admin password from your original deployment or a different password:</p>
<div class="listingblock">
<div class="content">
<pre>ADMIN_PASSWORD=SomePassword</pre>
</div>
</div>
<div class="paragraph">
<p>To use the existing RHOSP deployment password:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>declare -A TRIPLEO_PASSWORDS
TRIPLEO_PASSWORDS[default]="$HOME/overcloud-passwords.yaml"
ADMIN_PASSWORD=$(cat ${TRIPLEO_PASSWORDS[default]} | grep ' AdminPassword:' | awk -F ': ' '{ print $2; }')</pre>
</div>
</div>
</li>
<li>
<p>Set the service password variables to match the original deployment.
Database passwords can differ in the control plane environment, but
you must synchronize the service account passwords.</p>
<div class="paragraph">
<p>For example, in developer environments with director Standalone, the passwords can be extracted:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>AODH_PASSWORD=$(cat ${TRIPLEO_PASSWORDS[default]} | grep ' AodhPassword:' | awk -F ': ' '{ print $2; }')
BARBICAN_PASSWORD=$(cat ${TRIPLEO_PASSWORDS[default]} | grep ' BarbicanPassword:' | awk -F ': ' '{ print $2; }')
CEILOMETER_METERING_SECRET=$(cat ${TRIPLEO_PASSWORDS[default]} | grep ' CeilometerMeteringSecret:' | awk -F ': ' '{ print $2; }')
CEILOMETER_PASSWORD=$(cat ${TRIPLEO_PASSWORDS[default]} | grep ' CeilometerPassword:' | awk -F ': ' '{ print $2; }')
CINDER_PASSWORD=$(cat ${TRIPLEO_PASSWORDS[default]} | grep ' CinderPassword:' | awk -F ': ' '{ print $2; }')
GLANCE_PASSWORD=$(cat ${TRIPLEO_PASSWORDS[default]} | grep ' GlancePassword:' | awk -F ': ' '{ print $2; }')
HEAT_AUTH_ENCRYPTION_KEY=$(cat ${TRIPLEO_PASSWORDS[default]} | grep ' HeatAuthEncryptionKey:' | awk -F ': ' '{ print $2; }')
HEAT_PASSWORD=$(cat ${TRIPLEO_PASSWORDS[default]} | grep ' HeatPassword:' | awk -F ': ' '{ print $2; }')
HEAT_STACK_DOMAIN_ADMIN_PASSWORD=$(cat ${TRIPLEO_PASSWORDS[default]} | grep ' HeatStackDomainAdminPassword:' | awk -F ': ' '{ print $2; }')
IRONIC_PASSWORD=$(cat ${TRIPLEO_PASSWORDS[default]} | grep ' IronicPassword:' | awk -F ': ' '{ print $2; }')
MANILA_PASSWORD=$(cat ${TRIPLEO_PASSWORDS[default]} | grep ' ManilaPassword:' | awk -F ': ' '{ print $2; }')
NEUTRON_PASSWORD=$(cat ${TRIPLEO_PASSWORDS[default]} | grep ' NeutronPassword:' | awk -F ': ' '{ print $2; }')
NOVA_PASSWORD=$(cat ${TRIPLEO_PASSWORDS[default]} | grep ' NovaPassword:' | awk -F ': ' '{ print $2; }')
OCTAVIA_PASSWORD=$(cat ${TRIPLEO_PASSWORDS[default]} | grep ' OctaviaPassword:' | awk -F ': ' '{ print $2; }')
PLACEMENT_PASSWORD=$(cat ${TRIPLEO_PASSWORDS[default]} | grep ' PlacementPassword:' | awk -F ': ' '{ print $2; }')
SWIFT_PASSWORD=$(cat ${TRIPLEO_PASSWORDS[default]} | grep ' SwiftPassword:' | awk -F ': ' '{ print $2; }')</pre>
</div>
</div>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Ensure that you are using the Red Hat OpenShift Container Platform (RHOCP) namespace where you want the
control plane to be deployed:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc project openstack</pre>
</div>
</div>
</li>
<li>
<p>Create the RHOSP secret. For more information, see <a href="https://docs.redhat.com/en/documentation/red_hat_openstack_services_on_openshift/18.0/html-single/deploying_red_hat_openstack_services_on_openshift/index#proc_providing-secure-access-to-the-RHOSO-services_preparing">Providing secure access to the Red Hat OpenStack Services on OpenShift services</a> in <em>Deploying Red Hat OpenStack Services on OpenShift</em>.</p>
</li>
<li>
<p>If the <code>$ADMIN_PASSWORD</code> is different than the password you set
in <code>osp-secret</code>, amend the <code>AdminPassword</code> key in the <code>osp-secret</code>:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc set data secret/osp-secret "AdminPassword=$ADMIN_PASSWORD"</pre>
</div>
</div>
</li>
<li>
<p>Set service account passwords in <code>osp-secret</code> to match the service
account passwords from the original deployment:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc set data secret/osp-secret "AodhPassword=$AODH_PASSWORD"
$ oc set data secret/osp-secret "BarbicanPassword=$BARBICAN_PASSWORD"
$ oc set data secret/osp-secret "CeilometerPassword=$CEILOMETER_PASSWORD"
$ oc set data secret/osp-secret "CinderPassword=$CINDER_PASSWORD"
$ oc set data secret/osp-secret "GlancePassword=$GLANCE_PASSWORD"
$ oc set data secret/osp-secret "HeatAuthEncryptionKey=$HEAT_AUTH_ENCRYPTION_KEY"
$ oc set data secret/osp-secret "HeatPassword=$HEAT_PASSWORD"
$ oc set data secret/osp-secret "HeatStackDomainAdminPassword=$HEAT_STACK_DOMAIN_ADMIN_PASSWORD"
$ oc set data secret/osp-secret "IronicPassword=$IRONIC_PASSWORD"
$ oc set data secret/osp-secret "IronicInspectorPassword=$IRONIC_PASSWORD"
$ oc set data secret/osp-secret "ManilaPassword=$MANILA_PASSWORD"
$ oc set data secret/osp-secret "MetadataSecret=$METADATA_SECRET"
$ oc set data secret/osp-secret "NeutronPassword=$NEUTRON_PASSWORD"
$ oc set data secret/osp-secret "NovaPassword=$NOVA_PASSWORD"
$ oc set data secret/osp-secret "OctaviaPassword=$OCTAVIA_PASSWORD"
$ oc set data secret/osp-secret "PlacementPassword=$PLACEMENT_PASSWORD"
$ oc set data secret/osp-secret "SwiftPassword=$SWIFT_PASSWORD"</pre>
</div>
</div>
</li>
<li>
<p>Deploy the <code>OpenStackControlPlane</code> CR. Ensure that you only enable the DNS, Galera, Memcached, and RabbitMQ services. All other services must
be disabled:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">$ oc apply -f - &lt;&lt;EOF
apiVersion: core.openstack.org/v1beta1
kind: OpenStackControlPlane
metadata:
  name: openstack
spec:
  secret: osp-secret
  storageClass: &lt;storage_class&gt; <i class="conum" data-value="1"></i><b>(1)</b>

  barbican:
    enabled: false
    template:
      barbicanAPI: {}
      barbicanWorker: {}
      barbicanKeystoneListener: {}

  cinder:
    enabled: false
    template:
      cinderAPI: {}
      cinderScheduler: {}
      cinderBackup: {}
      cinderVolumes: {}

  dns:
    template:
      override:
        service:
          metadata:
            annotations:
              metallb.universe.tf/address-pool: ctlplane
              metallb.universe.tf/allow-shared-ip: ctlplane
              metallb.universe.tf/loadBalancerIPs: 192.168.122.80 <i class="conum" data-value="2"></i><b>(2)</b>

          spec:
            type: LoadBalancer
      options:
      - key: server
        values:
        - 192.168.122.1
      replicas: 1

  glance:
    enabled: false
    template:
      glanceAPIs: {}

  heat:
    enabled: false
    template: {}

  horizon:
    enabled: false
    template: {}

  ironic:
    enabled: false
    template:
      ironicConductors: []

  keystone:
    enabled: false
    template: {}

  manila:
    enabled: false
    template:
      manilaAPI: {}
      manilaScheduler: {}
      manilaShares: {}

  galera:
    enabled: true
    templates:
      openstack:
        secret: osp-secret
        replicas: 3
        storageRequest: 5G
      openstack-cell1: <i class="conum" data-value="3"></i><b>(3)</b>
        secret: osp-secret
        replicas: 3
        storageRequest: 5G
      openstack-cell2:
        secret: osp-secret
        replicas: 1
        storageRequest: 5G
      openstack-cell3:
        secret: osp-secret
        replicas: 1
        storageRequest: 5G
  memcached:
    enabled: true
    templates:
      memcached:
        replicas: 3

  neutron:
    enabled: false
    template: {}

  nova:
    enabled: false
    template: {}

  ovn:
    enabled: false
    template:
      ovnController:
        networkAttachment: tenant
        nodeSelector:
          node: non-existing-node-name
      ovnNorthd:
        replicas: 0
      ovnDBCluster:
        ovndbcluster-nb:
          replicas: 3
          dbType: NB
          networkAttachment: internalapi
        ovndbcluster-sb:
          replicas: 3
          dbType: SB
          networkAttachment: internalapi

  placement:
    enabled: false
    template: {}

  rabbitmq:
    templates:
      rabbitmq:
        override:
          service:
            metadata:
              annotations:
                metallb.universe.tf/address-pool: internalapi
                metallb.universe.tf/loadBalancerIPs: 172.17.0.85
            spec:
              type: LoadBalancer
      rabbitmq-cell1:
        persistence:
          storage: 1G
        override:
          service:
            metadata:
              annotations:
                metallb.universe.tf/address-pool: internalapi
                metallb.universe.tf/loadBalancerIPs: 172.17.0.86

            spec:
              type: LoadBalancer
      rabbitmq-cell2:
        persistence:
          storage: 1G
        override:
          service:
            metadata:
              annotations:
                metallb.universe.tf/address-pool: internalapi
                metallb.universe.tf/loadBalancerIPs: 172.17.0.87
            spec:
              type: LoadBalancer
      rabbitmq-cell3:
        persistence:
          storage: 1G
        override:
          service:
            metadata:
              annotations:
                metallb.universe.tf/address-pool: internalapi
                metallb.universe.tf/loadBalancerIPs: 172.17.0.88
            spec:
              type: LoadBalancer
  telemetry:
    enabled: false
  tls: <i class="conum" data-value="4"></i><b>(4)</b>
    podLevel:
      enabled: false
    ingress:
      enabled: false
  swift:
    enabled: false
    template:
      swiftRing:
        ringReplicas: 1
      swiftStorage:
        replicas: 0
      swiftProxy:
        replicas: 1
EOF</code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>Select an existing <code>&lt;storage_class&gt;</code> in your RHOCP cluster.</td>
</tr>
<tr>
<td><i class="conum" data-value="2"></i><b>2</b></td>
<td>Replace <code>&lt;loadBalancer_IP&gt;</code> with the LoadBalancer IP address.</td>
</tr>
<tr>
<td><i class="conum" data-value="3"></i><b>3</b></td>
<td>This example provides the required infrastructure database and messaging services for 3 Compute cells named <code>cell1</code>, <code>cell2</code>, and <code>cell3</code>. Adjust the values for fields such as <code>replicas</code>, <code>storage</code>, or <code>storageRequest</code>, for each Compute cell as needed.</td>
</tr>
<tr>
<td><i class="conum" data-value="4"></i><b>4</b></td>
<td>If you enabled TLS-e in your RHOSP environment, in the <code>spec:tls</code> section set <code>tls</code> to the following:
<div class="listingblock">
<div class="content">
<pre>spec:
  ...
  tls:
    podLevel:
      enabled: true
      internal:
        ca:
          customIssuer: rootca-internal
      libvirt:
        ca:
          customIssuer: rootca-internal
      ovn:
        ca:
          customIssuer: rootca-internal
    ingress:
      ca:
        customIssuer: rootca-internal
      enabled: true</pre>
</div>
</div></td>
</tr>
</table>
</div>
</li>
</ol>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>If you use IPv6, change the load balancer IPs to the IPs in your environment, for example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>...
metallb.universe.tf/allow-shared-ip: ctlplane
metallb.universe.tf/loadBalancerIPs: fd00:aaaa::80
...
metallb.universe.tf/address-pool: internalapi
metallb.universe.tf/loadBalancerIPs: fd00:bbbb::85
...
metallb.universe.tf/address-pool: internalapi
metallb.universe.tf/loadBalancerIPs: fd00:bbbb::86</pre>
</div>
</div>
</td>
</tr>
</table>
</div>
<div class="ulist">
<div class="title">Verification</div>
<ul>
<li>
<p>Verify that the Galera and RabbitMQ status is <code>Running</code> for all defined cells:</p>
<div class="listingblock">
<div class="content">
<pre>$ RENAMED_CELLS="cell1 cell2 cell3"
$ oc get pod openstack-galera-0 -o jsonpath='{.status.phase}{"\n"}'
$ oc get pod rabbitmq-server-0 -o jsonpath='{.status.phase}{"\n"}'
$ for CELL in $(echo $RENAMED_CELLS); do
&gt;     oc get pod openstack-$CELL-galera-0 -o jsonpath='{.status.phase}{"\n"}'
&gt;     oc get pod rabbitmq-$CELL-server-0 -o jsonpath='{.status.phase}{"\n"}'
&gt; done</pre>
</div>
</div>
<div class="paragraph">
<p>The given cells names are later referred to by using the environment variable <code>RENAMED_CELLS</code>.</p>
</div>
</li>
<li>
<p>Ensure that the statuses of all the Rabbitmq and Galera CRs are <code>Setup complete</code>:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc get Rabbitmqs,Galera
NAME                                                                  STATUS   MESSAGE
rabbitmq.rabbitmq.openstack.org/rabbitmq                              True     Setup complete
rabbitmq.rabbitmq.openstack.org/rabbitmq-cell1                        True     Setup complete

NAME                                                               READY   MESSAGE
galera.mariadb.openstack.org/openstack                             True     Setup complete
galera.mariadb.openstack.org/openstack-cell1                       True     Setup complete</pre>
</div>
</div>
</li>
<li>
<p>Verify that the <code>OpenStackControlPlane</code> CR is waiting for deployment
of the <code>openstackclient</code> pod:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc get OpenStackControlPlane openstack
NAME        STATUS    MESSAGE
openstack   Unknown   OpenStackControlPlane Client not started</pre>
</div>
</div>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="configuring-a-ceph-backend_migrating-databases">Configuring a Red Hat Ceph Storage back end</h3>
<div class="paragraph">
<p>If your Red&#160;Hat OpenStack Platform (RHOSP) 17.1 deployment uses a Red Hat Ceph Storage back end for any service, such as Image Service (glance), Block Storage service (cinder), Compute service (nova), or Shared File Systems service (manila), you must configure the custom resources (CRs) to use the same back end in the Red&#160;Hat OpenStack Services on OpenShift (RHOSO) 18.0 deployment.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
To run <code>ceph</code> commands, you must use SSH to connect to a Red Hat Ceph Storage node and run <code>sudo cephadm shell</code>. This generates a Ceph orchestrator container that enables you to run administrative commands against the Red Hat Ceph Storage cluster. If you deployed the Red Hat Ceph Storage cluster by using director, you can launch the <code>cephadm</code> shell from an RHOSP Controller node.
</td>
</tr>
</table>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>The <code>OpenStackControlPlane</code> CR is created.</p>
</li>
<li>
<p>If your RHOSP 17.1 deployment uses the Shared File Systems service, the openstack keyring is updated. Modify the <code>openstack</code> user so that you can use it across all RHOSP services:</p>
<div class="listingblock">
<div class="content">
<pre>ceph auth caps client.openstack \
  mgr 'allow *' \
  mon 'allow r, profile rbd' \
  osd 'profile rbd pool=vms, profile rbd pool=volumes, profile rbd pool=images, allow rw pool manila_data'</pre>
</div>
</div>
<div class="paragraph">
<p>Using the same user across all services makes it simpler to create a common Red Hat Ceph Storage secret that includes the keyring and <code>ceph.conf</code> file and propagate the secret to all the services that need it.</p>
</div>
</li>
<li>
<p>The following shell variables are defined. Replace the following example values with values that are correct for your environment:</p>
<div class="listingblock">
<div class="content">
<pre>CEPH_SSH="ssh -i <strong>&lt;path to SSH key&gt;</strong> root@<strong>&lt;node IP&gt;</strong>"
CEPH_KEY=$($CEPH_SSH "cat /etc/ceph/ceph.client.openstack.keyring | base64 -w 0")
CEPH_CONF=$($CEPH_SSH "cat /etc/ceph/ceph.conf | base64 -w 0")</pre>
</div>
</div>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Create the <code>ceph-conf-files</code> secret that includes the Red Hat Ceph Storage configuration:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc apply -f - &lt;&lt;EOF
apiVersion: v1
data:
  ceph.client.openstack.keyring: $CEPH_KEY
  ceph.conf: $CEPH_CONF
kind: Secret
metadata:
  name: ceph-conf-files
type: Opaque
EOF</pre>
</div>
</div>
<div class="paragraph">
<p>The content of the file should be similar to the following example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: v1
kind: Secret
metadata:
  name: ceph-conf-files
stringData:
  ceph.client.openstack.keyring: |
    [client.openstack]
        key = &lt;secret key&gt;
        caps mgr = "allow *"
        caps mon = "allow r, profile rbd"
        caps osd = "pool=vms, profile rbd pool=volumes, profile rbd pool=images, allow rw pool manila_data'
  ceph.conf: |
    [global]
    fsid = 7a1719e8-9c59-49e2-ae2b-d7eb08c695d4
    mon_host = 10.1.1.2,10.1.1.3,10.1.1.4 <i class="conum" data-value="1"></i><b>(1)</b></code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>If you use IPv6, use brackets for the <code>mon_host</code>. For example:
<code>mon_host = [v2:[fd00:cccc::100]:3300/0,v1:[fd00:cccc::100]:6789/0]</code></td>
</tr>
</table>
</div>
</li>
<li>
<p>In your <code>OpenStackControlPlane</code> CR, inject <code>ceph.conf</code> and <code>ceph.client.openstack.keyring</code> to the RHOSP services that are defined in the propagation list. For example:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">$ oc patch openstackcontrolplane openstack --type=merge --patch '
spec:
  extraMounts:
    - name: v1
      region: r1
      extraVol:
        - propagation:
          - CinderVolume
          - CinderBackup
          - GlanceAPI
          - ManilaShare
          extraVolType: Ceph
          volumes:
          - name: ceph
            projected:
              sources:
              - secret:
                  name: ceph-conf-files
          mounts:
          - name: ceph
            mountPath: "/etc/ceph"
            readOnly: true
'</code></pre>
</div>
</div>
</li>
</ol>
</div>
</div>
<div class="sect2">
<h3 id="stopping-openstack-services_migrating-databases">Stopping Red&#160;Hat OpenStack Platform services</h3>
<div class="paragraph">
<p>Before you start the Red&#160;Hat OpenStack Services on OpenShift (RHOSO) adoption, you must stop the Red&#160;Hat OpenStack Platform (RHOSP) services to avoid inconsistencies in the data that you migrate for the data plane adoption. Inconsistencies are caused by resource changes after the database is copied to the new deployment.</p>
</div>
<div class="paragraph">
<p>You should not stop the infrastructure management services yet, such as:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Database</p>
</li>
<li>
<p>RabbitMQ</p>
</li>
<li>
<p>HAProxy Load Balancer</p>
</li>
<li>
<p>Ceph-nfs</p>
</li>
<li>
<p>Compute service</p>
</li>
<li>
<p>Containerized modular libvirt daemons</p>
</li>
<li>
<p>Object Storage service (swift) back-end services</p>
</li>
</ul>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>Ensure that there no long-running tasks that require the services that you plan to stop, such as instance live migrations, volume migrations, volume creation, backup and restore, attaching, detaching, and other similar operations:</p>
<div class="listingblock">
<div class="content">
<pre>$ openstack server list --all-projects -c ID -c Status |grep -E '\| .+ing \|'
$ openstack volume list --all-projects -c ID -c Status |grep -E '\| .+ing \|'| grep -vi error
$ openstack volume backup list --all-projects -c ID -c Status |grep -E '\| .+ing \|' | grep -vi error
$ openstack share list --all-projects -c ID -c Status |grep -E '\| .+ing \|'| grep -vi error
$ openstack image list -c ID -c Status |grep -E '\| .+ing \|'</pre>
</div>
</div>
</li>
<li>
<p>Collect the services topology-specific configuration. For more information, see <a href="#proc_retrieving-topology-specific-service-configuration_migrating-databases">Retrieving topology-specific service configuration</a>.</p>
</li>
<li>
<p>Define the following shell variables. The values are examples and refer to a single node standalone director deployment. Replace these example values with values that are correct for your environment:</p>
<div class="listingblock">
<div class="content">
<pre>CONTROLLER1_SSH="ssh -i <strong>&lt;path to SSH key&gt;</strong> root@<strong>&lt;controller-1 IP&gt;</strong>" <i class="conum" data-value="1"></i><b>(1)</b>
CONTROLLER2_SSH="ssh -i <strong>&lt;path to SSH key&gt;</strong> root@<strong>&lt;controller-2 IP&gt;</strong>"
CONTROLLER3_SSH="ssh -i <strong>&lt;path to SSH key&gt;</strong> root@<strong>&lt;controller-3 IP&gt;</strong>"</pre>
</div>
</div>
</li>
<li>
<p>Specify the IP addresses of all Controller nodes, for example:</p>
<div class="listingblock">
<div class="content">
<pre>CONTROLLER1_SSH="ssh -i <strong>&lt;path to SSH key&gt;</strong> root@<strong>&lt;controller-1 IP&gt;</strong>" <i class="conum" data-value="2"></i><b>(2)</b>
CONTROLLER2_SSH="ssh -i <strong>&lt;path to SSH key&gt;</strong> root@<strong>&lt;controller-2 IP&gt;</strong>"
CONTROLLER3_SSH="ssh -i <strong>&lt;path to SSH key&gt;</strong> root@<strong>&lt;controller-3 IP&gt;</strong>"
# ...</pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>Replace <code>&lt;path_to_SSH_key&gt;</code> with the path to your SSH key.</td>
</tr>
<tr>
<td><i class="conum" data-value="2"></i><b>2</b></td>
<td>Replace <code>&lt;controller-&lt;X&gt; IP&gt;</code> with IP addresses of all Controller nodes.</td>
</tr>
</table>
</div>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>If your deployment enables CephFS through NFS as a back end for Shared File Systems service (manila), remove the following Pacemaker ordering and co-location constraints that govern the Virtual IP address of the <code>ceph-nfs</code> service and the <code>manila-share</code> service:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml"># check the co-location and ordering constraints concerning "manila-share"
sudo pcs constraint list --full

# remove these constraints
sudo pcs constraint remove colocation-openstack-manila-share-ceph-nfs-INFINITY
sudo pcs constraint remove order-ceph-nfs-openstack-manila-share-Optional</code></pre>
</div>
</div>
</li>
<li>
<p>Disable RHOSP control plane services:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml"># Update the services list to be stopped
ServicesToStop=("tripleo_aodh_api.service"
                "tripleo_aodh_api_cron.service"
                "tripleo_aodh_evaluator.service"
                "tripleo_aodh_listener.service"
                "tripleo_aodh_notifier.service"
                "tripleo_ceilometer_agent_central.service"
                "tripleo_ceilometer_agent_notification.service"
                "tripleo_octavia_api.service"
                "tripleo_octavia_health_manager.service"
                "tripleo_octavia_rsyslog.service"
                "tripleo_octavia_driver_agent.service"
                "tripleo_octavia_housekeeping.service"
                "tripleo_octavia_worker.service"
                "tripleo_horizon.service"
                "tripleo_keystone.service"
                "tripleo_barbican_api.service"
                "tripleo_barbican_worker.service"
                "tripleo_barbican_keystone_listener.service"
                "tripleo_cinder_api.service"
                "tripleo_cinder_api_cron.service"
                "tripleo_cinder_scheduler.service"
                "tripleo_cinder_volume.service"
                "tripleo_cinder_backup.service"
                "tripleo_collectd.service"
                "tripleo_glance_api.service"
                "tripleo_gnocchi_api.service"
                "tripleo_gnocchi_metricd.service"
                "tripleo_gnocchi_statsd.service"
                "tripleo_manila_api.service"
                "tripleo_manila_api_cron.service"
                "tripleo_manila_scheduler.service"
                "tripleo_neutron_api.service"
                "tripleo_placement_api.service"
                "tripleo_nova_api_cron.service"
                "tripleo_nova_api.service"
                "tripleo_nova_conductor.service"
                "tripleo_nova_metadata.service"
                "tripleo_nova_scheduler.service"
                "tripleo_nova_vnc_proxy.service"
                "tripleo_aodh_api.service"
                "tripleo_aodh_api_cron.service"
                "tripleo_aodh_evaluator.service"
                "tripleo_aodh_listener.service"
                "tripleo_aodh_notifier.service"
                "tripleo_ceilometer_agent_central.service"
                "tripleo_ceilometer_agent_compute.service"
                "tripleo_ceilometer_agent_ipmi.service"
                "tripleo_ceilometer_agent_notification.service"
                "tripleo_ovn_cluster_northd.service"
                "tripleo_ironic_neutron_agent.service"
                "tripleo_ironic_api.service"
                "tripleo_ironic_inspector.service"
                "tripleo_ironic_conductor.service"
                "tripleo_ironic_inspector_dnsmasq.service"
                "tripleo_ironic_pxe_http.service"
                "tripleo_ironic_pxe_tftp.service")

PacemakerResourcesToStop=("openstack-cinder-volume"
                          "openstack-cinder-backup"
                          "openstack-manila-share")

echo "Stopping systemd OpenStack services"
for service in ${ServicesToStop[*]}; do
    for i in {1..3}; do
        SSH_CMD=CONTROLLER${i}_SSH
        if [ ! -z "${!SSH_CMD}" ]; then
            echo "Stopping the $service in controller $i"
            if ${!SSH_CMD} sudo systemctl is-active $service; then
                ${!SSH_CMD} sudo systemctl stop $service
            fi
        fi
    done
done

echo "Checking systemd OpenStack services"
for service in ${ServicesToStop[*]}; do
    for i in {1..3}; do
        SSH_CMD=CONTROLLER${i}_SSH
        if [ ! -z "${!SSH_CMD}" ]; then
            if ! ${!SSH_CMD} systemctl show $service | grep ActiveState=inactive &gt;/dev/null; then
                echo "ERROR: Service $service still running on controller $i"
            else
                echo "OK: Service $service is not running on controller $i"
            fi
        fi
    done
done

echo "Stopping pacemaker OpenStack services"
for i in {1..3}; do
    SSH_CMD=CONTROLLER${i}_SSH
    if [ ! -z "${!SSH_CMD}" ]; then
        echo "Using controller $i to run pacemaker commands"
        for resource in ${PacemakerResourcesToStop[*]}; do
            if ${!SSH_CMD} sudo pcs resource config $resource &amp;&gt;/dev/null; then
                echo "Stopping $resource"
                ${!SSH_CMD} sudo pcs resource disable $resource
            else
                echo "Service $resource not present"
            fi
    done
        break
    fi
done

echo "Checking pacemaker OpenStack services"
for i in {1..3}; do
    SSH_CMD=CONTROLLER${i}_SSH
    if [ ! -z "${!SSH_CMD}" ]; then
        echo "Using controller $i to run pacemaker commands"
        for resource in ${PacemakerResourcesToStop[*]}; do
            if ${!SSH_CMD} sudo pcs resource config $resource &amp;&gt;/dev/null; then
                if ! ${!SSH_CMD} sudo pcs resource status $resource | grep Started; then
                    echo "OK: Service $resource is stopped"
                else
                    echo "ERROR: Service $resource is started"
                fi
            fi
        done
        break
    fi
    done</code></pre>
</div>
</div>
<div class="paragraph">
<p>If the status of each service is <code>OK</code>, then the services stopped successfully.</p>
</div>
</li>
</ol>
</div>
</div>
<div class="sect2">
<h3 id="migrating-databases-to-mariadb-instances_migrating-databases">Migrating databases to MariaDB instances</h3>
<div class="paragraph">
<p>Migrate your databases from the original Red&#160;Hat OpenStack Platform (RHOSP) deployment to the MariaDB instances in the Red Hat OpenShift Container Platform (RHOCP) cluster.</p>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>Ensure that the control plane MariaDB and RabbitMQ are running, and that no other control plane services are running.</p>
</li>
<li>
<p>Retrieve the topology-specific service configuration. For more information, see <a href="#proc_retrieving-topology-specific-service-configuration_migrating-databases">Retrieving topology-specific service configuration</a>.</p>
</li>
<li>
<p>Stop the RHOSP services. For more information, see <a href="#stopping-openstack-services_migrating-databases">Stopping Red&#160;Hat OpenStack Platform services</a>.</p>
</li>
<li>
<p>Ensure that there is network routability between the original MariaDB and the MariaDB for the control plane.</p>
</li>
<li>
<p>Define the following shell variables. Replace the following example values with values that are correct for your environment:</p>
<div class="listingblock">
<div class="content">
<pre>$ STORAGE_CLASS=local-storage
$ MARIADB_IMAGE=registry.redhat.io/rhoso/openstack-mariadb-rhel9:18.0


$ CELLS="default cell1 cell2" <i class="conum" data-value="1"></i><b>(1)</b>
$ DEFAULT_CELL_NAME="cell3"
$ RENAMED_CELLS="cell1 cell2 $DEFAULT_CELL_NAME"

$ CHARACTER_SET=utf8 <i class="conum" data-value="2"></i><b>(2)</b>
$ COLLATION=utf8_general_ci

$ declare -A PODIFIED_DB_ROOT_PASSWORD
$ for CELL in $(echo "super $RENAMED_CELLS"); do
&gt;   PODIFIED_DB_ROOT_PASSWORD[$CELL]=$(oc get -o json secret/osp-secret | jq -r .data.DbRootPassword | base64 -d)
&gt; done

$ declare -A PODIFIED_MARIADB_IP
$ for CELL in $(echo "super $RENAMED_CELLS"); do
&gt;   if [ "$CELL" = "super" ]; then
&gt;     PODIFIED_MARIADB_IP[$CELL]=$(oc get svc --selector "mariadb/name=openstack" -ojsonpath='{.items[0].spec.clusterIP}')
&gt;   else
&gt;     PODIFIED_MARIADB_IP[$CELL]=$(oc get svc --selector "mariadb/name=openstack-$CELL" -ojsonpath='{.items[0].spec.clusterIP}')
&gt;   fi
&gt; done

$ declare -A TRIPLEO_PASSWORDS
$ for CELL in $(echo $CELLS); do
&gt;   if [ "$CELL" = "default" ]; then
&gt;     TRIPLEO_PASSWORDS[default]="$HOME/overcloud-passwords.yaml"
&gt;   else
&gt;     # in a split-stack source cloud, it should take a stack-specific passwords file instead
&gt;     TRIPLEO_PASSWORDS[$CELL]="$HOME/overcloud-passwords.yaml"
&gt;   fi
&gt; done

$ declare -A SOURCE_DB_ROOT_PASSWORD
$ for CELL in $(echo $CELLS); do
&gt;   SOURCE_DB_ROOT_PASSWORD[$CELL]=$(cat ${TRIPLEO_PASSWORDS[$CELL]} | grep ' MysqlRootPassword:' | awk -F ': ' '{ print $2; }')
&gt; done

$ declare -A SOURCE_MARIADB_IP
$ SOURCE_MARIADB_IP[default]=*&lt;galera cluster VIP&gt;* <i class="conum" data-value="3"></i><b>(3)</b>
$ SOURCE_MARIADB_IP[cell1]=*&lt;galera cell1 cluster VIP&gt;* <i class="conum" data-value="4"></i><b>(4)</b>
$ SOURCE_MARIADB_IP[cell2]=*&lt;galera cell2 cluster VIP&gt;* <i class="conum" data-value="5"></i><b>(5)</b>
# ...

$ declare -A SOURCE_GALERA_MEMBERS_DEFAULT
$ SOURCE_GALERA_MEMBERS_DEFAULT=(
&gt;   ["standalone.localdomain"]=172.17.0.100 <i class="conum" data-value="6"></i><b>(6)</b>
&gt;   # [...]=...
&gt; )
$ declare -A SOURCE_GALERA_MEMBERS_CELL1
$ SOURCE_GALERA_MEMBERS_CELL1=(
&gt;   # ...
&gt; )
$ declare -A SOURCE_GALERA_MEMBERS_CELL2
$ SOURCE_GALERA_MEMBERS_CELL2=(
&gt;   # ...
&gt; )</pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td><code>CELLS</code> and <code>RENAMED_CELLS</code> represent changes that are going to be made after you import the databases. The <code>default</code> cell takes a new name from <code>DEFAULT_CELL_NAME</code>.
In a multi-cell adoption scenario, <code>default</code> cell might retain its original <em>default</em> name as well.</td>
</tr>
<tr>
<td><i class="conum" data-value="2"></i><b>2</b></td>
<td>The <code>CHARACTER_SET</code> variable and collation should match the source database. If they do not match, then foreign key relationships break for any tables that are created in the future as part of database sync.</td>
</tr>
<tr>
<td><i class="conum" data-value="3"></i><b>3</b></td>
<td>Add data in  <code>SOURCE_MARIADB_IP[*]= ...</code> for each cell that is defined in <code>CELLS</code>. Provide records for the cell names and VIP addresses of MariaDB Galera clusters.</td>
</tr>
<tr>
<td><i class="conum" data-value="4"></i><b>4</b></td>
<td>Replace <code>&lt;galera_cell1_cluster_VIP&gt;</code> with the VIP of your galera cell1 cluster.</td>
</tr>
<tr>
<td><i class="conum" data-value="5"></i><b>5</b></td>
<td>Replace <code>&lt;galera_cell2_cluster_VIP&gt;</code> with the VIP of your galera cell2 cluster, and so on.</td>
</tr>
<tr>
<td><i class="conum" data-value="6"></i><b>6</b></td>
<td>For each cell defined in <code>CELLS</code>, in <code>SOURCE_GALERA_MEMBERS_CELL&lt;X&gt;</code>, add the names of the MariaDB Galera cluster members and its IP address. Replace <code>["standalone.localdomain"]="172.17.0.100"</code> with the real hosts data.</td>
</tr>
</table>
</div>
</li>
</ul>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
A standalone director environment only creates a <em>default</em> cell, which should be the only <code>CELLS</code> value in this case. The <code>DEFAULT_CELL_NAME</code> value should be <code>cell1</code>.
</td>
</tr>
</table>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
The <code>super</code> is the top-scope Nova API upcall database instance. A super conductor connects to that database. In subsequent examples, the upcall and cells databases use the same password that is defined in <code>osp-secret</code>. Old passwords are only needed to prepare the data exports.
</td>
</tr>
</table>
</div>
<div class="ulist">
<ul>
<li>
<p>To get the values for <code>SOURCE_MARIADB_IP</code>, query the puppet-generated configurations in the Controller and CellController nodes:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo grep -rI 'listen mysql' -A10 /var/lib/config-data/puppet-generated/ | grep bind</pre>
</div>
</div>
</li>
<li>
<p>To get the values for <code>SOURCE_GALERA_MEMBERS_*</code>, query the puppet-generated configurations in the Controller and CellController nodes:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo grep -rI 'listen mysql' -A10 /var/lib/config-data/puppet-generated/ | grep server</pre>
</div>
</div>
<div class="paragraph">
<p>The source cloud always uses the same password for cells databases. For that reason, the same passwords file is used for all cells stacks. However, split-stack topology allows using different passwords files for each stack.</p>
</div>
</li>
<li>
<p>Prepare the MariaDB adoption helper pod:</p>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Create a temporary volume claim and a pod for the database data copy. Edit the volume claim storage request if necessary, to give it enough space for the overcloud databases:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc apply -f - &lt;&lt;EOF
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mariadb-data
spec:
  storageClassName: $STORAGE_CLASS
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
---
apiVersion: v1
kind: Pod
metadata:
  name: mariadb-copy-data
  annotations:
    openshift.io/scc: anyuid
    k8s.v1.cni.cncf.io/networks: internalapi
  labels:
    app: adoption
spec:
  containers:
  - image: $MARIADB_IMAGE
    command: [ "sh", "-c", "sleep infinity"]
    name: adoption
    volumeMounts:
    - mountPath: /backup
      name: mariadb-data
  securityContext:
    allowPrivilegeEscalation: false
    capabilities:
      drop: ALL
    runAsNonRoot: true
    seccompProfile:
      type: RuntimeDefault
  volumes:
  - name: mariadb-data
    persistentVolumeClaim:
      claimName: mariadb-data
EOF</pre>
</div>
</div>
</li>
<li>
<p>Wait for the pod to be ready:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc wait --for condition=Ready pod/mariadb-copy-data --timeout=30s</pre>
</div>
</div>
</li>
</ol>
</div>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Check that the source Galera database clusters in each cell have its members online and synced:</p>
<div class="listingblock">
<div class="content">
<pre>$ for CELL in $(echo $CELLS); do
&gt;   MEMBERS=SOURCE_GALERA_MEMBERS_$(echo ${CELL}|tr '[:lower:]' '[:upper:]')[@]
&gt;   for i in "${!MEMBERS}"; do
&gt;     echo "Checking for the database node $i WSREP status Synced"
&gt;     oc rsh mariadb-copy-data mysql \
&gt;       -h "$i" -uroot -p"${SOURCE_DB_ROOT_PASSWORD[$CELL]}" \
&gt;       -e "show global status like 'wsrep_local_state_comment'" | \
&gt;       grep -qE "\bSynced\b"
&gt;   done
&gt; done</pre>
</div>
</div>
<div class="paragraph">
<p>Each additional Compute service (nova) v2 cell runs a dedicated Galera database cluster, so the command checks each cell.</p>
</div>
</li>
<li>
<p>Get the count of source databases with the <code>NOK</code> (not-OK) status:</p>
<div class="listingblock">
<div class="content">
<pre>$ for CELL in $(echo $CELLS); do
&gt;   oc rsh mariadb-copy-data mysql -h "${SOURCE_MARIADB_IP[$CELL]}" -uroot -p"${SOURCE_DB_ROOT_PASSWORD[$CELL]}" -e "SHOW databases;"
&gt; end</pre>
</div>
</div>
</li>
<li>
<p>Check that <code>mysqlcheck</code> had no errors:</p>
<div class="listingblock">
<div class="content">
<pre>$ for CELL in $(echo $CELLS); do
&gt;   set +u
&gt;   . ~/.source_cloud_exported_variables_$CELL
&gt;   set -u
&gt; done
$ test -z "$PULL_OPENSTACK_CONFIGURATION_MYSQLCHECK_NOK"  || [ "x$PULL_OPENSTACK_CONFIGURATION_MYSQLCHECK_NOK" = "x " ] &amp;&amp; echo "OK" || echo "CHECK FAILED"</pre>
</div>
</div>
</li>
<li>
<p>Test the connection to the control plane upcall and cells databases:</p>
<div class="listingblock">
<div class="content">
<pre>$ for CELL in $(echo "super $RENAMED_CELLS"); do
&gt;   oc run mariadb-client --image $MARIADB_IMAGE -i --rm --restart=Never -- \
&gt;     mysql -rsh "${PODIFIED_MARIADB_IP[$CELL]}" -uroot -p"${PODIFIED_DB_ROOT_PASSWORD[$CELL]}" -e 'SHOW databases;'
&gt; done</pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
You must transition Compute services that you import later into a superconductor architecture by deleting the old service records in the cell databases, starting with <code>cell1</code>. New records are registered with different hostnames that are provided by the Compute service operator. All Compute services, except the Compute agent, have no internal state, and you can safely delete their service records. You also need to rename the former <code>default</code> cell to <code>DEFAULT_CELL_NAME</code>.
</td>
</tr>
</table>
</div>
</li>
<li>
<p>Create a dump of the original databases:</p>
<div class="listingblock">
<div class="content">
<pre>$ for CELL in $(echo $CELLS); do
&gt;   oc rsh mariadb-copy-data &lt;&lt; EOF
&gt;     mysql -h"${SOURCE_MARIADB_IP[$CELL]}" -uroot -p"${SOURCE_DB_ROOT_PASSWORD[$CELL]}" \
&gt;     -N -e "show databases" | grep -E -v "schema|mysql|gnocchi|aodh" | \
&gt;     while read dbname; do
&gt;       echo "Dumping $CELL cell \${dbname}";
&gt;       mysqldump -h"${SOURCE_MARIADB_IP[$CELL]}" -uroot -p"${SOURCE_DB_ROOT_PASSWORD[$CELL]}" \
&gt;         --single-transaction --complete-insert --skip-lock-tables --lock-tables=0 \
&gt;         "\${dbname}" &gt; /backup/"${CELL}.\${dbname}".sql;
&gt;     done
&gt; EOF
&gt; done</pre>
</div>
</div>
<div class="paragraph">
<p>Note filtering the information and performance schema tables.
Gnocchi is no longer used as a metric store as well</p>
</div>
</li>
<li>
<p>Restore the databases from <code>.sql</code> files into the control plane MariaDB:</p>
<div class="listingblock">
<div class="content">
<pre>$ for CELL in $(echo $CELLS); do
&gt;   RCELL=$CELL
&gt;   [ "$CELL" = "default" ] &amp;&amp; RCELL=$DEFAULT_CELL_NAME
&gt;   oc rsh mariadb-copy-data &lt;&lt; EOF
&gt;     declare -A db_name_map  <i class="conum" data-value="1"></i><b>(1)</b>
&gt;     db_name_map['nova']="nova_$RCELL"
&gt;     db_name_map['ovs_neutron']='neutron'
&gt;     db_name_map['ironic-inspector']='ironic_inspector'
&gt;     declare -A db_cell_map  <i class="conum" data-value="2"></i><b>(2)</b>
&gt;     db_cell_map['nova']="nova_$DEFAULT_CELL_NAME"
&gt;     db_cell_map["nova_$RCELL"]="nova_$RCELL"  <i class="conum" data-value="3"></i><b>(3)</b>
&gt;     declare -A db_server_map  <i class="conum" data-value="4"></i><b>(4)</b>
&gt;     db_server_map['default']=${PODIFIED_MARIADB_IP['super']}
&gt;     db_server_map["nova"]=${PODIFIED_MARIADB_IP[$DEFAULT_CELL_NAME]}
&gt;     db_server_map["nova_$RCELL"]=${PODIFIED_MARIADB_IP[$RCELL]}
&gt;     declare -A db_server_password_map  <i class="conum" data-value="5"></i><b>(5)</b>
&gt;     db_server_password_map['default']=${PODIFIED_DB_ROOT_PASSWORD['super']}
&gt;     db_server_password_map["nova"]=${PODIFIED_DB_ROOT_PASSWORD[$DEFAULT_CELL_NAME]}
&gt;     db_server_password_map["nova_$RCELL"]=${PODIFIED_DB_ROOT_PASSWORD[$RCELL]}
&gt;     cd /backup
&gt;     for db_file in \$(ls ${CELL}.*.sql); do
&gt;       db_name=\$(echo \${db_file} | awk -F'.' '{ print \$2; }')
&gt;       [[ "$CELL" != "default" &amp;&amp; ! -v "db_cell_map[\${db_name}]" ]] &amp;&amp; continue
&gt;       if [[ "$CELL" == "default" &amp;&amp; -v "db_cell_map[\${db_name}]" ]] ; then
&gt;         target=$DEFAULT_CELL_NAME
&gt;       elif [[ "$CELL" == "default" &amp;&amp; ! -v "db_cell_map[\${db_name}]" ]] ; then
&gt;         target=super
&gt;       else
&gt;         target=$RCELL
&gt;       fi  <i class="conum" data-value="6"></i><b>(6)</b>
&gt;       renamed_db_file="\${target}_new.\${db_name}.sql"
&gt;       mv -f \${db_file} \${renamed_db_file}
&gt;       if [[ -v "db_name_map[\${db_name}]" ]]; then
&gt;         echo "renaming $CELL cell \${db_name} to \$target \${db_name_map[\${db_name}]}"
&gt;         db_name=\${db_name_map[\${db_name}]}
&gt;       fi
&gt;       db_server=\${db_server_map["default"]}
&gt;       if [[ -v "db_server_map[\${db_name}]" ]]; then
&gt;         db_server=\${db_server_map[\${db_name}]}
&gt;       fi
&gt;       db_password=\${db_server_password_map['default']}
&gt;       if [[ -v "db_server_password_map[\${db_name}]" ]]; then
&gt;         db_password=\${db_server_password_map[\${db_name}]}
&gt;       fi
&gt;       echo "creating $CELL cell \${db_name} in \$target \${db_server}"
&gt;       mysql -h"\${db_server}" -uroot "-p\${db_password}" -e \
&gt;         "CREATE DATABASE IF NOT EXISTS \${db_name} DEFAULT \
&gt;         CHARACTER SET ${CHARACTER_SET} DEFAULT COLLATE ${COLLATION};"
&gt;       echo "importing $CELL cell \${db_name} into \$target \${db_server} from \${renamed_db_file}"
&gt;       mysql -h "\${db_server}" -uroot "-p\${db_password}" "\${db_name}" &lt; "\${renamed_db_file}"
&gt;     done
&gt;     if [ "$CELL" = "default" ] ; then
&gt;       mysql -h "\${db_server_map['default']}" -uroot -p"\${db_server_password_map['default']}" -e \
&gt;         "update nova_api.cell_mappings set name='$DEFAULT_CELL_NAME' where name='default';"
&gt;     fi
&gt;     mysql -h "\${db_server_map["nova_$RCELL"]}" -uroot -p"\${db_server_password_map["nova_$RCELL"]}" -e \
&gt;       "delete from nova_${RCELL}.services where host not like '%nova_${RCELL}-%' and services.binary != 'nova-compute';"
&gt; EOF
&gt; done</pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>Defines which common databases to rename when importing them.</td>
</tr>
<tr>
<td><i class="conum" data-value="2"></i><b>2</b></td>
<td>Defines which cells databases to import, and how to rename them, if needed.</td>
</tr>
<tr>
<td><i class="conum" data-value="3"></i><b>3</b></td>
<td>Omits importing special <code>cell0</code> databases of the cells, as its contents cannot be consolidated during adoption.</td>
</tr>
<tr>
<td><i class="conum" data-value="4"></i><b>4</b></td>
<td>Defines which databases to import into which servers, usually dedicated for cells.</td>
</tr>
<tr>
<td><i class="conum" data-value="5"></i><b>5</b></td>
<td>Defines the root passwords map for database servers. You can only use the same password for now.</td>
</tr>
<tr>
<td><i class="conum" data-value="6"></i><b>6</b></td>
<td>Assigns which databases to import into which hosts when extracting databases from the <code>default</code> cell.</td>
</tr>
</table>
</div>
</li>
</ol>
</div>
<div class="paragraph">
<div class="title">Verification</div>
<p>Compare the following outputs with the topology-specific service configuration.
For more information, see <a href="#proc_retrieving-topology-specific-service-configuration_migrating-databases">Retrieving topology-specific service configuration</a>.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Check that the databases are imported correctly:</p>
<div class="listingblock">
<div class="content">
<pre>$ set +u
$ . ~/.source_cloud_exported_variables_default
$ set -u
$ dbs=$(oc exec openstack-galera-0 -c galera -- mysql -rs -uroot -p"${PODIFIED_DB_ROOT_PASSWORD['super']}" -e 'SHOW databases;')
$ echo $dbs | grep -Eq '\bkeystone\b' &amp;&amp; echo "OK" || echo "CHECK FAILED"
$ echo $dbs | grep -Eq '\bneutron\b' &amp;&amp; echo "OK" || echo "CHECK FAILED"
$ echo "${PULL_OPENSTACK_CONFIGURATION_DATABASES[@]}" | grep -Eq '\bovs_neutron\b' &amp;&amp; echo "OK" || echo "CHECK FAILED" <i class="conum" data-value="1"></i><b>(1)</b>
$ novadb_mapped_cells=$(oc exec openstack-galera-0 -c galera -- mysql -rs -uroot -p"${PODIFIED_DB_ROOT_PASSWORD['super']}" \
&gt;   nova_api -e 'select uuid,name,transport_url,database_connection,disabled from cell_mappings;') <i class="conum" data-value="2"></i><b>(2)</b>
$ uuidf='\S{8,}-\S{4,}-\S{4,}-\S{4,}-\S{12,}'
$ default=$(printf "%s\n" "$PULL_OPENSTACK_CONFIGURATION_NOVADB_MAPPED_CELLS" | sed -rn "s/^($uuidf)\s+default\b.*$/\1/p")
$ difference=$(diff -ZNua \
&gt;   &lt;(printf "%s\n" "$PULL_OPENSTACK_CONFIGURATION_NOVADB_MAPPED_CELLS") \
&gt;   &lt;(printf "%s\n" "$novadb_mapped_cells")) || true
$ if [ "$DEFAULT_CELL_NAME" != "default" ]; then
&gt;   printf "%s\n" "$difference" | grep -qE "^\-$default\s+default\b" &amp;&amp; echo "OK" || echo "CHECK FAILED"
&gt;   printf "%s\n" "$difference" | grep -qE "^\+$default\s+$DEFAULT_CELL_NAME\b" &amp;&amp; echo "OK" || echo "CHECK FAILED"
&gt;   [ $(grep -E "^[-\+]$uuidf" &lt;&lt;&lt;"$difference" | wc -l) -eq 2 ] &amp;&amp; echo "OK" || echo "CHECK FAILED"
&gt; else
&gt;   [ "x$difference" = "x" ] &amp;&amp; echo "OK" || echo "CHECK FAILED"
&gt; fi
$ for CELL in $(echo $RENAMED_CELLS); do <i class="conum" data-value="3"></i><b>(3)</b>
&gt;   RCELL=$CELL
&gt;   [ "$CELL" = "$DEFAULT_CELL_NAME" ] &amp;&amp; RCELL=default
&gt;   set +u
&gt;   . ~/.source_cloud_exported_variables_$RCELL
&gt;   set -u
&gt;   c1dbs=$(oc exec openstack-$CELL-galera-0 -c galera -- mysql -rs -uroot -p${PODIFIED_DB_ROOT_PASSWORD[$CELL]} -e 'SHOW databases;') <i class="conum" data-value="4"></i><b>(4)</b>
&gt;   echo $c1dbs | grep -Eq "\bnova_${CELL}\b" &amp;&amp; echo "OK" || echo "CHECK FAILED"
&gt;   novadb_svc_records=$(oc exec openstack-$CELL-galera-0 -c galera -- mysql -rs -uroot -p${PODIFIED_DB_ROOT_PASSWORD[$CELL]} \
&gt;     nova_$CELL -e "select host from services where services.binary='nova-compute' and deleted=0 order by host asc;")
&gt;   diff -Z &lt;(echo "x$novadb_svc_records") &lt;(echo "x${PULL_OPENSTACK_CONFIGURATION_NOVA_COMPUTE_HOSTNAMES[@]}") &amp;&amp; echo "OK" || echo "CHECK FAILED" <i class="conum" data-value="5"></i><b>(5)</b>
&gt; done</pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>Ensures that the Networking service (neutron) database is renamed from <code>ovs_neutron</code>.</td>
</tr>
<tr>
<td><i class="conum" data-value="2"></i><b>2</b></td>
<td>Ensures that the <code>default</code> cell is renamed to <code>$DEFAULT_CELL_NAME</code>, and the cell UUIDs are retained.</td>
</tr>
<tr>
<td><i class="conum" data-value="3"></i><b>3</b></td>
<td>Ensures that the registered Compute services names have not changed.</td>
</tr>
<tr>
<td><i class="conum" data-value="4"></i><b>4</b></td>
<td>Ensures Compute service cells databases are extracted to separate database servers, and renamed from <code>nova</code> to <code>nova_cell&lt;X&gt;</code>.</td>
</tr>
<tr>
<td><i class="conum" data-value="5"></i><b>5</b></td>
<td>Ensures that the registered Compute service name has not changed.</td>
</tr>
</table>
</div>
</li>
<li>
<p>Delete the <code>mariadb-data</code> pod and the <code>mariadb-copy-data</code> persistent volume claim that contains the database backup:</p>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Consider taking a snapshot of them before deleting.
</td>
</tr>
</table>
</div>
<div class="listingblock">
<div class="content">
<pre>$ oc delete pod mariadb-copy-data
$ oc delete pvc mariadb-data</pre>
</div>
</div>
</li>
</ol>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
During the pre-checks and post-checks, the <code>mariadb-client</code> pod might return a pod security warning related to the <code>restricted:latest</code> security context constraint. This warning is due to default security context constraints and does not prevent the admission controller from creating a pod. You see a warning for the short-lived pod, but it does not interfere with functionality.
For more information, see <a href="https://learn.redhat.com/t5/DO280-Red-Hat-OpenShift/About-pod-security-standards-and-warnings/m-p/32502">About pod security standards and warnings</a>.
</td>
</tr>
</table>
</div>
</div>
<div class="sect2">
<h3 id="migrating-ovn-data_migrating-databases">Migrating OVN data</h3>
<div class="paragraph">
<p>Migrate the data in the OVN databases from the original Red&#160;Hat OpenStack Platform deployment to <code>ovsdb-server</code> instances that are running in the Red Hat OpenShift Container Platform (RHOCP) cluster.</p>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>The <code>OpenStackControlPlane</code> resource is created.</p>
</li>
<li>
<p><code>NetworkAttachmentDefinition</code> custom resources (CRs) for the original cluster are defined. Specifically, the <code>internalapi</code> network is defined.</p>
</li>
<li>
<p>The original Networking service (neutron) and OVN <code>northd</code> are not running.</p>
</li>
<li>
<p>There is network routability between the control plane services and the adopted cluster.</p>
</li>
<li>
<p>The cloud is migrated to the Modular Layer 2 plug-in with Open Virtual Networking (ML2/OVN) mechanism driver.</p>
</li>
<li>
<p>Define the following shell variables. Replace the example values with values that are correct for your environment:</p>
<div class="listingblock">
<div class="content">
<pre>STORAGE_CLASS=local-storage
OVSDB_IMAGE=registry.redhat.io/rhoso/openstack-ovn-base-rhel9:18.0
SOURCE_OVSDB_IP=172.17.0.100 # For IPv4
SOURCE_OVSDB_IP=[fd00:bbbb::100] # For IPv6</pre>
</div>
</div>
<div class="paragraph">
<p>To get the value to set <code>SOURCE_OVSDB_IP</code>, query the puppet-generated configurations in a Controller node:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>$ grep -rI 'ovn_[ns]b_conn' /var/lib/config-data/puppet-generated/</pre>
</div>
</div>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Prepare a temporary <code>PersistentVolume</code> claim and the helper pod for the OVN backup. Adjust the storage requests for a large database, if needed:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">$ oc apply -f - &lt;&lt;EOF
---
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: ovn-data-cert
spec:
  commonName: ovn-data-cert
  secretName: ovn-data-cert
  issuerRef:
    name: rootca-internal
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: ovn-data
spec:
  storageClassName: $STORAGE_CLASS
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
---
apiVersion: v1
kind: Pod
metadata:
  name: ovn-copy-data
  annotations:
    openshift.io/scc: anyuid
    k8s.v1.cni.cncf.io/networks: internalapi
  labels:
    app: adoption
spec:
  containers:
  - image: $OVSDB_IMAGE
    command: [ "sh", "-c", "sleep infinity"]
    name: adoption
    volumeMounts:
    - mountPath: /backup
      name: ovn-data
    - mountPath: /etc/pki/tls/misc
      name: ovn-data-cert
      readOnly: true
  securityContext:
    allowPrivilegeEscalation: false
    capabilities:
      drop: ALL
    runAsNonRoot: true
    seccompProfile:
      type: RuntimeDefault
  volumes:
  - name: ovn-data
    persistentVolumeClaim:
      claimName: ovn-data
  - name: ovn-data-cert
    secret:
      secretName: ovn-data-cert
EOF</code></pre>
</div>
</div>
</li>
<li>
<p>Wait for the pod to be ready:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc wait --for=condition=Ready pod/ovn-copy-data --timeout=30s</pre>
</div>
</div>
</li>
<li>
<p>If the podified internalapi cidr is different than the source internalapi cidr, add an iptables accept rule on the Controller nodes:</p>
<div class="listingblock">
<div class="content">
<pre>$ $CONTROLLER1_SSH sudo iptables -I INPUT -s {PODIFIED_INTERNALAPI_NETWORK} -p tcp -m tcp --dport 6641 -m conntrack --ctstate NEW -j ACCEPT
$ $CONTROLLER1_SSH sudo iptables -I INPUT -s {PODIFIED_INTERNALAPI_NETWORK} -p tcp -m tcp --dport 6642 -m conntrack --ctstate NEW -j ACCEPT</pre>
</div>
</div>
</li>
<li>
<p>Back up your OVN databases:</p>
<div class="ulist">
<ul>
<li>
<p>If you did not enable TLS everywhere, run the following command:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc exec ovn-copy-data -- bash -c "ovsdb-client backup tcp:$SOURCE_OVSDB_IP:6641 &gt; /backup/ovs-nb.db"
$ oc exec ovn-copy-data -- bash -c "ovsdb-client backup tcp:$SOURCE_OVSDB_IP:6642 &gt; /backup/ovs-sb.db"</pre>
</div>
</div>
</li>
<li>
<p>If you enabled TLS everywhere, run the following command:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc exec ovn-copy-data -- bash -c "ovsdb-client backup --ca-cert=/etc/pki/tls/misc/ca.crt --private-key=/etc/pki/tls/misc/tls.key --certificate=/etc/pki/tls/misc/tls.crt ssl:$SOURCE_OVSDB_IP:6641 &gt; /backup/ovs-nb.db"
$ oc exec ovn-copy-data -- bash -c "ovsdb-client backup --ca-cert=/etc/pki/tls/misc/ca.crt --private-key=/etc/pki/tls/misc/tls.key --certificate=/etc/pki/tls/misc/tls.crt ssl:$SOURCE_OVSDB_IP:6642 &gt; /backup/ovs-sb.db"</pre>
</div>
</div>
</li>
</ul>
</div>
</li>
<li>
<p>Start the control plane OVN database services prior to import, with <code>northd</code> disabled:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">$ oc patch openstackcontrolplane openstack --type=merge --patch '
spec:
  ovn:
    enabled: true
    template:
      ovnDBCluster:
        ovndbcluster-nb:
          replicas: 3
          dbType: NB
          storageRequest: 10G
          networkAttachment: internalapi
        ovndbcluster-sb:
          replicas: 3
          dbType: SB
          storageRequest: 10G
          networkAttachment: internalapi
      ovnNorthd:
        replicas: 0
'</code></pre>
</div>
</div>
</li>
<li>
<p>Wait for the OVN database services to reach the <code>Running</code> phase:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc wait --for=jsonpath='{.status.phase}'=Running pod --selector=service=ovsdbserver-nb
$ oc wait --for=jsonpath='{.status.phase}'=Running pod --selector=service=ovsdbserver-sb</pre>
</div>
</div>
</li>
<li>
<p>Fetch the OVN database IP addresses on the <code>clusterIP</code> service network:</p>
<div class="listingblock">
<div class="content">
<pre>PODIFIED_OVSDB_NB_IP=$(oc get svc --selector "statefulset.kubernetes.io/pod-name=ovsdbserver-nb-0" -ojsonpath='{.items[0].spec.clusterIP}')
PODIFIED_OVSDB_SB_IP=$(oc get svc --selector "statefulset.kubernetes.io/pod-name=ovsdbserver-sb-0" -ojsonpath='{.items[0].spec.clusterIP}')</pre>
</div>
</div>
</li>
<li>
<p>If you are  using IPv6, adjust the address to the format expected by <code>ovsdb-*</code> tools:</p>
<div class="listingblock">
<div class="content">
<pre>PODIFIED_OVSDB_NB_IP=[$PODIFIED_OVSDB_NB_IP]
PODIFIED_OVSDB_SB_IP=[$PODIFIED_OVSDB_SB_IP]</pre>
</div>
</div>
</li>
<li>
<p>Upgrade the database schema for the backup files:</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>If you did not enable TLS everywhere, use the following command:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc exec ovn-copy-data -- bash -c "ovsdb-client get-schema tcp:$PODIFIED_OVSDB_NB_IP:6641 &gt; /backup/ovs-nb.ovsschema &amp;&amp; ovsdb-tool convert /backup/ovs-nb.db /backup/ovs-nb.ovsschema"
$ oc exec ovn-copy-data -- bash -c "ovsdb-client get-schema tcp:$PODIFIED_OVSDB_SB_IP:6642 &gt; /backup/ovs-sb.ovsschema &amp;&amp; ovsdb-tool convert /backup/ovs-sb.db /backup/ovs-sb.ovsschema"</pre>
</div>
</div>
</li>
<li>
<p>If you enabled TLS everywhere, use the following command:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc exec ovn-copy-data -- bash -c "ovsdb-client get-schema --ca-cert=/etc/pki/tls/misc/ca.crt --private-key=/etc/pki/tls/misc/tls.key --certificate=/etc/pki/tls/misc/tls.crt ssl:$PODIFIED_OVSDB_NB_IP:6641 &gt; /backup/ovs-nb.ovsschema &amp;&amp; ovsdb-tool convert /backup/ovs-nb.db /backup/ovs-nb.ovsschema"
$ oc exec ovn-copy-data -- bash -c "ovsdb-client get-schema --ca-cert=/etc/pki/tls/misc/ca.crt --private-key=/etc/pki/tls/misc/tls.key --certificate=/etc/pki/tls/misc/tls.crt ssl:$PODIFIED_OVSDB_SB_IP:6642 &gt; /backup/ovs-sb.ovsschema &amp;&amp; ovsdb-tool convert /backup/ovs-sb.db /backup/ovs-sb.ovsschema"</pre>
</div>
</div>
</li>
</ol>
</div>
</li>
<li>
<p>Restore the database backup to the new OVN database servers:</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>If you did not enable TLS everywhere, use the following command:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc exec ovn-copy-data -- bash -c "ovsdb-client restore tcp:$PODIFIED_OVSDB_NB_IP:6641 &lt; /backup/ovs-nb.db"
$ oc exec ovn-copy-data -- bash -c "ovsdb-client restore tcp:$PODIFIED_OVSDB_SB_IP:6642 &lt; /backup/ovs-sb.db"</pre>
</div>
</div>
</li>
<li>
<p>If you enabled TLS everywhere, use the following command:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc exec ovn-copy-data -- bash -c "ovsdb-client restore --ca-cert=/etc/pki/tls/misc/ca.crt --private-key=/etc/pki/tls/misc/tls.key --certificate=/etc/pki/tls/misc/tls.crt ssl:$PODIFIED_OVSDB_NB_IP:6641 &lt; /backup/ovs-nb.db"
$ oc exec ovn-copy-data -- bash -c "ovsdb-client restore --ca-cert=/etc/pki/tls/misc/ca.crt --private-key=/etc/pki/tls/misc/tls.key --certificate=/etc/pki/tls/misc/tls.crt ssl:$PODIFIED_OVSDB_SB_IP:6642 &lt; /backup/ovs-sb.db"</pre>
</div>
</div>
</li>
</ol>
</div>
</li>
<li>
<p>Check that the data was successfully migrated by running the following commands against the new database servers, for example:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc exec -it ovsdbserver-nb-0 -- ovn-nbctl show
$ oc exec -it ovsdbserver-sb-0 -- ovn-sbctl list Chassis</pre>
</div>
</div>
</li>
<li>
<p>Start the control plane <code>ovn-northd</code> service to keep both OVN databases in sync:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">$ oc patch openstackcontrolplane openstack --type=merge --patch '
spec:
  ovn:
    enabled: true
    template:
      ovnNorthd:
        replicas: 1
'</code></pre>
</div>
</div>
</li>
<li>
<p>If you are running OVN gateway services on RHOCP nodes, enable the control plane <code>ovn-controller</code> service:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">$ oc patch openstackcontrolplane openstack --type=merge --patch '
spec:
  ovn:
    enabled: true
    template:
      ovnController:
        nicMappings:
          physNet: NIC <i class="conum" data-value="1"></i><b>(1)</b></code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td><code>physNet</code> is the name of your physical network. <code>NIC</code> is the name of the physical interface that is connected to your physical network.
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Running OVN gateways on RHOCP nodes might be prone to data plane downtime during Open vSwitch upgrades. Consider running OVN gateways on dedicated <code>Networker</code> data plane nodes for production deployments instead.
</td>
</tr>
</table>
</div></td>
</tr>
</table>
</div>
</li>
<li>
<p>Delete the <code>ovn-data</code> helper pod and the temporary <code>PersistentVolumeClaim</code> that is used to store OVN database backup files:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc delete --ignore-not-found=true pod ovn-copy-data
$ oc delete --ignore-not-found=true pvc ovn-data</pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Consider taking a snapshot of the <code>ovn-data</code> helper pod and the temporary <code>PersistentVolumeClaim</code> before deleting them. For more information, see <a href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/html/storage/index#lvms-about-volume-snapsot_logical-volume-manager-storage">About volume snapshots</a> in <em>OpenShift Container Platform storage overview</em>.
</td>
</tr>
</table>
</div>
</li>
<li>
<p>Stop the adopted OVN database servers:</p>
<div class="listingblock">
<div class="content">
<pre>ServicesToStop=("tripleo_ovn_cluster_north_db_server.service"
                "tripleo_ovn_cluster_south_db_server.service")

echo "Stopping systemd OpenStack services"
for service in ${ServicesToStop[*]}; do
    for i in {1..3}; do
        SSH_CMD=CONTROLLER${i}_SSH
        if [ ! -z "${!SSH_CMD}" ]; then
            echo "Stopping the $service in controller $i"
            if ${!SSH_CMD} sudo systemctl is-active $service; then
                ${!SSH_CMD} sudo systemctl stop $service
            fi
        fi
    done
done

echo "Checking systemd OpenStack services"
for service in ${ServicesToStop[*]}; do
    for i in {1..3}; do
        SSH_CMD=CONTROLLER${i}_SSH
        if [ ! -z "${!SSH_CMD}" ]; then
            if ! ${!SSH_CMD} systemctl show $service | grep ActiveState=inactive &gt;/dev/null; then
                echo "ERROR: Service $service still running on controller $i"
            else
                echo "OK: Service $service is not running on controller $i"
            fi
        fi
    done
done</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="adopting-openstack-control-plane-services_configuring-network">Adopting Red&#160;Hat OpenStack Platform control plane services</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Adopt your Red&#160;Hat OpenStack Platform 17.1 control plane services to deploy them in the Red&#160;Hat OpenStack Services on OpenShift (RHOSO) 18.0 control plane.</p>
</div>
<div class="sect2">
<h3 id="adopting-the-identity-service_adopt-control-plane">Adopting the Identity service</h3>
<div class="paragraph">
<p>To adopt the Identity service (keystone), you patch an existing <code>OpenStackControlPlane</code> custom resource (CR) where the Identity service is disabled. The patch starts the service with the configuration parameters that are provided by the Red&#160;Hat OpenStack Platform (RHOSP) environment.</p>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>Create the keystone secret that includes the Fernet keys that were copied from the RHOSP environment:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc apply -f - &lt;&lt;EOF
apiVersion: v1
data:
  CredentialKeys0: $($CONTROLLER1_SSH sudo cat /var/lib/config-data/puppet-generated/keystone/etc/keystone/credential-keys/0 | base64 -w 0)
  CredentialKeys1: $($CONTROLLER1_SSH sudo cat /var/lib/config-data/puppet-generated/keystone/etc/keystone/credential-keys/1 | base64 -w 0)
  FernetKeys0: $($CONTROLLER1_SSH sudo cat /var/lib/config-data/puppet-generated/keystone/etc/keystone/fernet-keys/0 | base64 -w 0)
  FernetKeys1: $($CONTROLLER1_SSH sudo cat /var/lib/config-data/puppet-generated/keystone/etc/keystone/fernet-keys/1 | base64 -w 0)
kind: Secret
metadata:
  name: keystone
type: Opaque
EOF</pre>
</div>
</div>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Patch the <code>OpenStackControlPlane</code> CR to deploy the Identity service:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc patch openstackcontrolplane openstack --type=merge --patch '
spec:
  keystone:
    enabled: true
    apiOverride:
      route: {}
    template:
      override:
        service:
          internal:
            metadata:
              annotations:
                metallb.universe.tf/address-pool: internalapi
                metallb.universe.tf/allow-shared-ip: internalapi
                metallb.universe.tf/loadBalancerIPs: 172.17.0.80 <i class="conum" data-value="1"></i><b>(1)</b>
            spec:
              type: LoadBalancer
      databaseInstance: openstack
      secret: osp-secret
'</pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>If you use IPv6, change the load balancer IP to the load balancer IP in your environment, for example, <code>metallb.universe.tf/loadBalancerIPs: fd00:bbbb::80</code>.</td>
</tr>
</table>
</div>
</li>
<li>
<p>Create an alias to use the <code>openstack</code> command in the Red&#160;Hat OpenStack Services on OpenShift (RHOSO) deployment:</p>
<div class="listingblock">
<div class="content">
<pre>$ alias openstack="oc exec -t openstackclient -- openstack"</pre>
</div>
</div>
</li>
<li>
<p>Remove services and endpoints that still point to the RHOSP
control plane, excluding the Identity service and its endpoints:</p>
<div class="listingblock">
<div class="content">
<pre>$ openstack endpoint list | grep keystone | awk '/admin/{ print $2; }' | xargs ${BASH_ALIASES[openstack]} endpoint delete || true

for service in aodh heat heat-cfn barbican cinderv3 glance gnocchi manila manilav2 neutron nova placement swift ironic-inspector ironic octavia; do
  openstack service list | awk "/ $service /{ print \$2; }" | xargs -r ${BASH_ALIASES[openstack]} service delete || true
done</pre>
</div>
</div>
</li>
</ol>
</div>
<div class="olist arabic">
<div class="title">Verification</div>
<ol class="arabic">
<li>
<p>Verify that you can access the <code>OpenStackClient</code> pod. For more information, see <a href="https://docs.redhat.com/en/documentation/red_hat_openstack_services_on_openshift/18.0/html/maintaining_the_red_hat_openstack_services_on_openshift_deployment/assembly_accessing-the-rhoso-cloud#proc_accessing-the-OpenStackClient-pod_cloud-access-admin">Accessing the OpenStackClient pod</a> in <em>Maintaining the Red&#160;Hat OpenStack Services on OpenShift deployment</em>.</p>
</li>
<li>
<p>Confirm that the Identity service endpoints are defined and are pointing to the control plane FQDNs:</p>
<div class="listingblock">
<div class="content">
<pre>$ openstack endpoint list | grep keystone</pre>
</div>
</div>
</li>
<li>
<p>Wait for the <code>OpenStackControlPlane</code> resource to become <code>Ready</code>:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc wait --for=condition=Ready --timeout=1m OpenStackControlPlane openstack</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
<div class="sect2">
<h3 id="adopting-the-key-manager-service_adopt-control-plane">Adopting the Key Manager service</h3>
<div class="paragraph">
<p>To adopt the Key Manager service (barbican), you patch an existing <code>OpenStackControlPlane</code> custom resource (CR) where Key Manager service is disabled. The patch starts the service with the configuration parameters that are provided by the Red&#160;Hat OpenStack Platform (RHOSP) environment.</p>
</div>
<div class="paragraph">
<p>The Key Manager service adoption is complete if you see the following results:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The <code>BarbicanAPI</code>, <code>BarbicanWorker</code>, and <code>BarbicanKeystoneListener</code> services are up and running.</p>
</li>
<li>
<p>Keystone endpoints are updated, and the same crypto plugin of the source cloud is available.</p>
</li>
</ul>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
This procedure configures the Key Manager service to use the <code>simple_crypto</code> back end. Additional back ends, such as PKCS11 and DogTag, are currently not supported in Red&#160;Hat OpenStack Services on OpenShift (RHOSO).
</td>
</tr>
</table>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Add the kek secret:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc set data secret/osp-secret "BarbicanSimpleCryptoKEK=$($CONTROLLER1_SSH "python3 -c \"import configparser; c = configparser.ConfigParser(); c.read('/var/lib/config-data/puppet-generated/barbican/etc/barbican/barbican.conf'); print(c['simple_crypto_plugin']['kek'])\"")"</pre>
</div>
</div>
</li>
<li>
<p>Patch the <code>OpenStackControlPlane</code> CR to deploy the Key Manager service:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc patch openstackcontrolplane openstack --type=merge --patch '
spec:
  barbican:
    enabled: true
    apiOverride:
      route: {}
    template:
      databaseInstance: openstack
      databaseAccount: barbican
      rabbitMqClusterName: rabbitmq
      secret: osp-secret
      simpleCryptoBackendSecret: osp-secret
      serviceAccount: barbican
      serviceUser: barbican
      passwordSelectors:
        service: BarbicanPassword
        simplecryptokek: BarbicanSimpleCryptoKEK
      barbicanAPI:
        replicas: 1
        override:
          service:
            internal:
              metadata:
                annotations:
                  metallb.universe.tf/address-pool: internalapi
                  metallb.universe.tf/allow-shared-ip: internalapi
                  metallb.universe.tf/loadBalancerIPs: 172.17.0.80 <i class="conum" data-value="1"></i><b>(1)</b>
              spec:
                type: LoadBalancer
      barbicanWorker:
        replicas: 1
      barbicanKeystoneListener:
        replicas: 1
'</pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>If you use IPv6, change the load balancer IP to the load balancer IP in your environment, for example, <code>metallb.universe.tf/loadBalancerIPs: fd00:bbbb::80</code>.</td>
</tr>
</table>
</div>
</li>
</ol>
</div>
<div class="ulist">
<div class="title">Verification</div>
<ul>
<li>
<p>Ensure that the Identity service (keystone) endpoints are defined and are pointing to the control plane FQDNs:</p>
<div class="listingblock">
<div class="content">
<pre>$ openstack endpoint list | grep key-manager</pre>
</div>
</div>
</li>
<li>
<p>Ensure that Barbican API service is registered in the Identity service:</p>
<div class="listingblock">
<div class="content">
<pre>$ openstack service list | grep key-manager</pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre>$ openstack endpoint list | grep key-manager</pre>
</div>
</div>
</li>
<li>
<p>List the secrets:</p>
<div class="listingblock">
<div class="content">
<pre>$ openstack secret list</pre>
</div>
</div>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="adopting-the-networking-service_adopt-control-plane">Adopting the Networking service</h3>
<div class="paragraph">
<p>To adopt the Networking service (neutron), you patch an existing <code>OpenStackControlPlane</code> custom resource (CR) that has the Networking service disabled. The patch starts the service with the
configuration parameters that are provided by the Red&#160;Hat OpenStack Platform (RHOSP) environment.</p>
</div>
<div class="paragraph">
<p>The Networking service adoption is complete if you see the following results:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The <code>NeutronAPI</code> service is running.</p>
</li>
<li>
<p>The Identity service (keystone) endpoints are updated, and the same back end of the source cloud is available.</p>
</li>
</ul>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>Ensure that Single Node OpenShift or OpenShift Local is running in the Red Hat OpenShift Container Platform (RHOCP) cluster.</p>
</li>
<li>
<p>Adopt the Identity service. For more information, see <a href="#adopting-the-identity-service_adopt-control-plane">Adopting the Identity service</a>.</p>
</li>
<li>
<p>Migrate your OVN databases to <code>ovsdb-server</code> instances that run in the Red Hat OpenShift Container Platform (RHOCP) cluster. For more information, see <a href="#migrating-ovn-data_migrating-databases">Migrating OVN data</a>.</p>
</li>
</ul>
</div>
<div class="ulist">
<div class="title">Procedure</div>
<ul>
<li>
<p>Patch the <code>OpenStackControlPlane</code> CR to deploy the Networking service:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">$ oc patch openstackcontrolplane openstack --type=merge --patch '
spec:
  neutron:
    enabled: true
    apiOverride:
      route: {}
    template:
      override:
        service:
          internal:
            metadata:
              annotations:
                metallb.universe.tf/address-pool: internalapi
                metallb.universe.tf/allow-shared-ip: internalapi
                metallb.universe.tf/loadBalancerIPs: 172.17.0.80 <i class="conum" data-value="1"></i><b>(1)</b>
            spec:
              type: LoadBalancer
      databaseInstance: openstack
      databaseAccount: neutron
      secret: osp-secret
      networkAttachments:
      - internalapi
'</code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>If you use IPv6, change the load balancer IP to the load balancer IP in your environment, for example, <code>metallb.universe.tf/loadBalancerIPs: fd00:bbbb::80</code>.
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>If you used the <code>neutron-dhcp-agent</code> in your RHOSP 17.1 deployment and you still need to use it after adoption, you must enable the <code>dhcp_agent_notification</code> for the <code>neutron-api</code> service:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">$ oc patch openstackcontrolplane openstack --type=merge --patch '
 spec:
  neutron:
    template:
      customServiceConfig: |
        [DEFAULT]
        dhcp_agent_notification = True
'</code></pre>
</div>
</div>
</td>
</tr>
</table>
</div></td>
</tr>
</table>
</div>
</li>
</ul>
</div>
<div class="ulist">
<div class="title">Verification</div>
<ul>
<li>
<p>Inspect the resulting Networking service pods:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc get pods -l service=neutron</pre>
</div>
</div>
</li>
<li>
<p>Ensure that the <code>Neutron API</code> service is registered in the Identity service:</p>
<div class="listingblock">
<div class="content">
<pre>$ openstack service list | grep network</pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">$ openstack endpoint list | grep network

| 6a805bd6c9f54658ad2f24e5a0ae0ab6 | regionOne | neutron      | network      | True    | public    | http://neutron-public-openstack.apps-crc.testing  |
| b943243e596847a9a317c8ce1800fa98 | regionOne | neutron      | network      | True    | internal  | http://neutron-internal.openstack.svc:9696        |</code></pre>
</div>
</div>
</li>
<li>
<p>Create sample resources so that you can test whether the user can create networks, subnets, ports, or routers:</p>
<div class="listingblock">
<div class="content">
<pre>$ openstack network create net
$ openstack subnet create --network net --subnet-range 10.0.0.0/24 subnet
$ openstack router create router</pre>
</div>
</div>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="adopting-the-object-storage-service_adopt-control-plane">Adopting the Object Storage service</h3>
<div class="paragraph">
<p>If you are using Object Storage as a service, adopt the Object Storage service (swift) to the Red&#160;Hat OpenStack Services on OpenShift (RHOSO) environment. If you are using the Object Storage API of the Ceph Object Gateway (RGW), skip the following procedure.</p>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>The Object Storage service storage back-end services are running in the Red&#160;Hat OpenStack Platform (RHOSP) deployment.</p>
</li>
<li>
<p>The storage network is properly configured on the Red Hat OpenShift Container Platform (RHOCP) cluster. For more information, see <a href="https://docs.redhat.com/en/documentation/red_hat_openstack_services_on_openshift/18.0/html/deploying_red_hat_openstack_services_on_openshift/assembly_preparing-rhocp-for-rhoso#proc_configuring-the-data-plane-network_preparing">Preparing Red Hat OpenShift Container Platform for Red Hat OpenStack Services on OpenShift</a> in <em>Deploying Red Hat OpenStack Services on OpenShift</em>.</p>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Create the <code>swift-conf</code> secret that includes the Object Storage service hash path suffix and prefix:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc apply -f - &lt;&lt;EOF
apiVersion: v1
kind: Secret
metadata:
  name: swift-conf
type: Opaque
data:
  swift.conf: $($CONTROLLER1_SSH sudo cat /var/lib/config-data/puppet-generated/swift/etc/swift/swift.conf | base64 -w0)
EOF</pre>
</div>
</div>
</li>
<li>
<p>Create the <code>swift-ring-files</code> <code>ConfigMap</code> that includes the Object Storage service ring files:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc apply -f - &lt;&lt;EOF
apiVersion: v1
kind: ConfigMap
metadata:
  name: swift-ring-files
binaryData:
  swiftrings.tar.gz: $($CONTROLLER1_SSH "cd /var/lib/config-data/puppet-generated/swift/etc/swift &amp;&amp; tar cz *.builder *.ring.gz backups/ | base64 -w0")
  account.ring.gz: $($CONTROLLER1_SSH "base64 -w0 /var/lib/config-data/puppet-generated/swift/etc/swift/account.ring.gz")
  container.ring.gz: $($CONTROLLER1_SSH "base64 -w0 /var/lib/config-data/puppet-generated/swift/etc/swift/container.ring.gz")
  object.ring.gz: $($CONTROLLER1_SSH "base64 -w0 /var/lib/config-data/puppet-generated/swift/etc/swift/object.ring.gz")
EOF</pre>
</div>
</div>
</li>
<li>
<p>Patch the <code>OpenStackControlPlane</code> custom resource to deploy the Object Storage service:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc patch openstackcontrolplane openstack --type=merge --patch '
spec:
  swift:
    enabled: true
    template:
      memcachedInstance: memcached
      swiftRing:
        ringReplicas: 1
      swiftStorage:
        replicas: 0
        networkAttachments:
        - storage
        storageClass: local-storage <i class="conum" data-value="1"></i><b>(1)</b>
        storageRequest: 10Gi
      swiftProxy:
        secret: osp-secret
        replicas: 1
        passwordSelectors:
          service: SwiftPassword
        serviceUser: swift
        override:
          service:
            internal:
              metadata:
                annotations:
                  metallb.universe.tf/address-pool: internalapi
                  metallb.universe.tf/allow-shared-ip: internalapi
                  metallb.universe.tf/loadBalancerIPs: 172.17.0.80 <i class="conum" data-value="2"></i><b>(2)</b>
              spec:
                type: LoadBalancer
        networkAttachments: <i class="conum" data-value="3"></i><b>(3)</b>
        - storage
'</pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>Must match the RHOSO deployment storage class.</td>
</tr>
<tr>
<td><i class="conum" data-value="2"></i><b>2</b></td>
<td>If you use IPv6, change the load balancer IP to the load balancer IP in your environment, for example, <code>metallb.universe.tf/loadBalancerIPs: fd00:bbbb::80</code>.</td>
</tr>
<tr>
<td><i class="conum" data-value="3"></i><b>3</b></td>
<td>Must match the network attachment for the previous Object Storage service configuration from the RHOSP deployment.</td>
</tr>
</table>
</div>
</li>
</ol>
</div>
<div class="ulist">
<div class="title">Verification</div>
<ul>
<li>
<p>Inspect the resulting Object Storage service pods:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc get pods -l component=swift-proxy</pre>
</div>
</div>
</li>
<li>
<p>Verify that the Object Storage proxy service is registered in the Identity service (keystone):</p>
<div class="listingblock">
<div class="content">
<pre>$ openstack service list | grep swift
| b5b9b1d3c79241aa867fa2d05f2bbd52 | swift    | object-store |</pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre>$ openstack endpoint list | grep swift
| 32ee4bd555414ab48f2dc90a19e1bcd5 | regionOne | swift        | object-store | True    | public    | https://swift-public-openstack.apps-crc.testing/v1/AUTH_%(tenant_id)s |
| db4b8547d3ae4e7999154b203c6a5bed | regionOne | swift        | object-store | True    | internal  | http://swift-internal.openstack.svc:8080/v1/AUTH_%(tenant_id)s        |</pre>
</div>
</div>
</li>
<li>
<p>Verify that you are able to upload and download objects:</p>
<div class="listingblock">
<div class="content">
<pre>$ openstack container create test
+---------------------------------------+-----------+------------------------------------+
| account                               | container | x-trans-id                         |
+---------------------------------------+-----------+------------------------------------+
| AUTH_4d9be0a9193e4577820d187acdd2714a | test      | txe5f9a10ce21e4cddad473-0065ce41b9 |
+---------------------------------------+-----------+------------------------------------+

$ openstack object create test --name obj &lt;(echo "Hello World!")
+--------+-----------+----------------------------------+
| object | container | etag                             |
+--------+-----------+----------------------------------+
| obj    | test      | d41d8cd98f00b204e9800998ecf8427e |
+--------+-----------+----------------------------------+

$ openstack object save test obj --file -
Hello World!</pre>
</div>
</div>
</li>
</ul>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
The Object Storage data is still stored on the existing RHOSP nodes. For more information about migrating the actual data from the RHOSP deployment to the RHOSO deployment, see <a href="#migrating-object-storage-data-to-rhoso-nodes_migrate-object-storage-service">Migrating the Object Storage service (swift) data from RHOSP to Red&#160;Hat OpenStack Services on OpenShift (RHOSO) nodes</a>.
</td>
</tr>
</table>
</div>
</div>
<div class="sect2">
<h3 id="adopting-the-image-service_adopt-control-plane">Adopting the Image service</h3>
<div class="paragraph">
<p>To adopt the Image Service (glance) you patch an existing <code>OpenStackControlPlane</code> custom resource (CR) that has the Image service disabled. The patch starts the service with the configuration parameters that are provided by the Red&#160;Hat OpenStack Platform (RHOSP) environment.</p>
</div>
<div class="paragraph">
<p>The Image service adoption is complete if you see the following results:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The <code>GlanceAPI</code> service up and running.</p>
</li>
<li>
<p>The Identity service endpoints are updated, and the same back end of the source cloud is available.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>To complete the Image service adoption, ensure that your environment meets the following criteria:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>You have a running director environment (the source cloud).</p>
</li>
<li>
<p>You have a Single Node OpenShift or OpenShift Local that is running in the Red Hat OpenShift Container Platform (RHOCP) cluster.</p>
</li>
<li>
<p>Optional: You can reach an internal/external <code>Ceph</code> cluster by both <code>crc</code> and director.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>If you have image quotas in RHOSP 17.1, these quotas are transferred to Red&#160;Hat OpenStack Services on OpenShift (RHOSO) 18.0 because the image quota system in 18.0 is disabled by default. For more information about enabling image quotas in 18.0, see <a href="https://docs.redhat.com/en/documentation/red_hat_openstack_services_on_openshift/18.0/html/customizing_persistent_storage/assembly_glance-customizing-the-image-service_customizing-cinder#assembly_glance-configuring-quotas_configuring-glance">Configuring image quotas</a> in <em>Customizing persistent storage</em>. If you enable image quotas in RHOSO 18.0, the new quotas replace the legacy quotas from RHOSP 17.1.</p>
</div>
<div class="sect3">
<h4 id="adopting-image-service-with-object-storage-backend_image-service">Adopting the Image service that is deployed with a Object Storage service back end</h4>
<div class="paragraph">
<p>Adopt the Image Service (glance) that you deployed with an Object Storage service (swift) back end in the Red&#160;Hat OpenStack Platform (RHOSP) environment. The control plane <code>glanceAPI</code> instance is deployed with the following configuration. You use this configuration in the patch manifest that deploys the Image service with the object storage back end:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>..
spec
  glance:
   ...
      customServiceConfig: |
          [DEFAULT]
          enabled_backends = default_backend:swift
          [glance_store]
          default_backend = default_backend
          [default_backend]
          swift_store_create_container_on_put = True
          swift_store_auth_version = 3
          swift_store_auth_address = {{ .KeystoneInternalURL }}
          swift_store_endpoint_type = internalURL
          swift_store_user = service:glance
          swift_store_key = {{ .ServicePassword }}</pre>
</div>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>You have completed the previous adoption steps.</p>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Create a new file, for example, <code>glance_swift.patch</code>, and include the following content:</p>
<div class="listingblock">
<div class="content">
<pre>spec:
  glance:
    enabled: true
    apiOverride:
      route: {}
    template:
      secret: osp-secret
      databaseInstance: openstack
      storage:
        storageRequest: 10G
      customServiceConfig: |
        [DEFAULT]
        enabled_backends = default_backend:swift
        [glance_store]
        default_backend = default_backend
        [default_backend]
        swift_store_create_container_on_put = True
        swift_store_auth_version = 3
        swift_store_auth_address = {{ .KeystoneInternalURL }}
        swift_store_endpoint_type = internalURL
        swift_store_user = service:glance
        swift_store_key = {{ .ServicePassword }}
      glanceAPIs:
        default:
          replicas: 1
          override:
            service:
              internal:
                metadata:
                  annotations:
                    metallb.universe.tf/address-pool: internalapi
                    metallb.universe.tf/allow-shared-ip: internalapi
                    metallb.universe.tf/loadBalancerIPs: 172.17.0.80 <i class="conum" data-value="1"></i><b>(1)</b>
                spec:
                  type: LoadBalancer
          networkAttachments:
            - storage</pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>If you use IPv6, change the load balancer IP to the load balancer IP in your environment, for example, <code>metallb.universe.tf/loadBalancerIPs: fd00:bbbb::80</code>.
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
The Object Storage service as a back end establishes a dependency with the Image service. Any deployed <code>GlanceAPI</code> instances do not work if the Image service is configured with the Object Storage service that is not available in the <code>OpenStackControlPlane</code> custom resource.
After the Object Storage service, and in particular <code>SwiftProxy</code>, is adopted, you can proceed with the <code>GlanceAPI</code> adoption. For more information, see <a href="#adopting-the-object-storage-service_adopt-control-plane">Adopting the Object Storage service</a>.
</td>
</tr>
</table>
</div></td>
</tr>
</table>
</div>
</li>
<li>
<p>Verify that <code>SwiftProxy</code> is available:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc get pod -l component=swift-proxy | grep Running
swift-proxy-75cb47f65-92rxq   3/3     Running   0</pre>
</div>
</div>
</li>
<li>
<p>Patch the <code>GlanceAPI</code> service that is deployed in the control plane:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc patch openstackcontrolplane openstack --type=merge --patch-file=glance_swift.patch</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="adopting-image-service-with-block-storage-backend_image-service">Adopting the Image service that is deployed with a Block Storage service back end</h4>
<div class="paragraph">
<p>Adopt the Image Service (glance) that you deployed with a Block Storage service (cinder) back end in the Red&#160;Hat OpenStack Platform (RHOSP) environment. The control plane <code>glanceAPI</code> instance is deployed with the following configuration. You use this configuration in the patch manifest that deploys the Image service with the block storage back end:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>..
spec
  glance:
   ...
      customServiceConfig: |
          [DEFAULT]
          enabled_backends = default_backend:cinder
          [glance_store]
          default_backend = default_backend
          [default_backend]
          rootwrap_config = /etc/glance/rootwrap.conf
          description = Default cinder backend
          cinder_store_auth_address = {{ .KeystoneInternalURL }}
          cinder_store_user_name = {{ .ServiceUser }}
          cinder_store_password = {{ .ServicePassword }}
          cinder_store_project_name = service
          cinder_catalog_info = volumev3::internalURL
          cinder_use_multipath = true</pre>
</div>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>You have completed the previous adoption steps.</p>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Create a new file, for example <code>glance_cinder.patch</code>, and include the following content:</p>
<div class="listingblock">
<div class="content">
<pre>spec:
  glance:
    enabled: true
    apiOverride:
      route: {}
    template:
      secret: osp-secret
      databaseInstance: openstack
      storage:
        storageRequest: 10G
      customServiceConfig: |
        [DEFAULT]
        enabled_backends = default_backend:cinder
        [glance_store]
        default_backend = default_backend
        [default_backend]
        rootwrap_config = /etc/glance/rootwrap.conf
        description = Default cinder backend
        cinder_store_auth_address = {{ .KeystoneInternalURL }}
        cinder_store_user_name = {{ .ServiceUser }}
        cinder_store_password = {{ .ServicePassword }}
        cinder_store_project_name = service
        cinder_catalog_info = volumev3::internalURL
        cinder_use_multipath = true
      glanceAPIs:
        default:
          replicas: 1
          override:
            service:
              internal:
                metadata:
                  annotations:
                    metallb.universe.tf/address-pool: internalapi
                    metallb.universe.tf/allow-shared-ip: internalapi
                    metallb.universe.tf/loadBalancerIPs: 172.17.0.80 <i class="conum" data-value="1"></i><b>(1)</b>
                spec:
                  type: LoadBalancer
          networkAttachments:
            - storage</pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>If you use IPv6, change the load balancer IP to the load balancer IP in your environment, for example, <code>metallb.universe.tf/loadBalancerIPs: fd00:bbbb::80</code>.
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
The Block Storage service as a back end establishes a dependency with the Image service. Any deployed <code>GlanceAPI</code> instances do not work if the Image service is configured with the Block Storage service that is not available in the <code>OpenStackControlPlane</code> custom resource.
After the Block Storage service, and in particular <code>CinderVolume</code>, is adopted, you can proceed with the <code>GlanceAPI</code> adoption. For more information, see <a href="#adopting-the-block-storage-service_adopt-control-plane">Adopting the Block Storage service</a>.
</td>
</tr>
</table>
</div></td>
</tr>
</table>
</div>
</li>
<li>
<p>Verify that <code>CinderVolume</code> is available:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc get pod -l component=cinder-volume | grep Running
cinder-volume-75cb47f65-92rxq   3/3     Running   0</pre>
</div>
</div>
</li>
<li>
<p>Patch the <code>GlanceAPI</code> service that is deployed in the control plane:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc patch openstackcontrolplane openstack --type=merge --patch-file=glance_cinder.patch</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="adopting-image-service-with-nfs-backend_image-service">Adopting the Image service that is deployed with an NFS back end</h4>
<div class="paragraph">
<p>Adopt the Image Service (glance) that you deployed with an NFS back end. To complete the following procedure, ensure that your environment meets the following criteria:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The Storage network is propagated to the Red&#160;Hat OpenStack Platform (RHOSP) control plane.</p>
</li>
<li>
<p>The Image service can reach the Storage network and connect to the nfs-server through the port <code>2049</code>.</p>
</li>
</ul>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>You have completed the previous adoption steps.</p>
</li>
<li>
<p>In the source cloud, verify the NFS parameters that the overcloud uses to configure the Image service back end. Specifically, in yourdirector heat templates, find the following variables that override the default content that is provided by the <code>glance-nfs.yaml</code> file in the
<code>/usr/share/openstack-tripleo-heat-templates/environments/storage</code> directory:</p>
<div class="listingblock">
<div class="content">
<pre>GlanceBackend: file
GlanceNfsEnabled: true
GlanceNfsShare: 192.168.24.1:/var/nfs</pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>In this example, the <code>GlanceBackend</code> variable shows that the Image service has no notion of an NFS back end. The variable is using the <code>File</code> driver and, in the background, the <code>filesystem_store_datadir</code>. The <code>filesystem_store_datadir</code> is mapped to the export value provided by the <code>GlanceNfsShare</code> variable instead of <code>/var/lib/glance/images/</code>.
If you do not export the <code>GlanceNfsShare</code> through a network that is propagated to the adopted Red&#160;Hat OpenStack Services on OpenShift (RHOSO) control plane, you must stop the <code>nfs-server</code> and remap the export to the <code>storage</code> network. Before doing so, ensure that the Image service is stopped in the source Controller nodes.</p>
</div>
<div class="paragraph">
<p>In the control plane, the Image service is attached to the Storage network, then propagated through the associated <code>NetworkAttachmentsDefinition</code> custom resource (CR), and the resulting pods already have the right permissions to handle the Image service traffic through this network.
In a deployed RHOSP control plane, you can verify that the network mapping matches with what has been deployed in the director-based environment by checking both the <code>NodeNetworkConfigPolicy</code> (<code>nncp</code>) and the <code>NetworkAttachmentDefinition</code> (<code>net-attach-def</code>). The following is an example of the output that you should check in the Red Hat OpenShift Container Platform (RHOCP) environment to make sure that there are no issues with the propagated networks:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>$ oc get nncp
NAME                        STATUS      REASON
enp6s0-crc-8cf2w-master-0   Available   SuccessfullyConfigured

$ oc get net-attach-def
NAME
ctlplane
internalapi
storage
tenant

$ oc get ipaddresspool -n metallb-system
NAME          AUTO ASSIGN   AVOID BUGGY IPS   ADDRESSES
ctlplane      true          false             ["192.168.122.80-192.168.122.90"]
internalapi   true          false             ["172.17.0.80-172.17.0.90"]
storage       true          false             ["172.18.0.80-172.18.0.90"]
tenant        true          false             ["172.19.0.80-172.19.0.90"]</pre>
</div>
</div>
</td>
</tr>
</table>
</div>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Adopt the Image service and create a new <code>default</code> <code>GlanceAPI</code> instance that is connected with the existing NFS share:</p>
<div class="listingblock">
<div class="content">
<pre>$ cat &lt;&lt; EOF &gt; glance_nfs_patch.yaml

spec:
  extraMounts:
  - extraVol:
    - extraVolType: Nfs
      mounts:
      - mountPath: /var/lib/glance/images
        name: nfs
      propagation:
      - Glance
      volumes:
      - name: nfs
        nfs:
          path: &lt;exported_path&gt; <i class="conum" data-value="1"></i><b>(1)</b>
          server: &lt;ip_address&gt; <i class="conum" data-value="2"></i><b>(2)</b>
    name: r1
    region: r1
  glance:
    enabled: true
    template:
      databaseInstance: openstack
      customServiceConfig: |
        [DEFAULT]
        enabled_backends = default_backend:file
        [glance_store]
        default_backend = default_backend
        [default_backend]
        filesystem_store_datadir = /var/lib/glance/images/
      storage:
        storageRequest: 10G
      glanceAPIs:
        default:
          replicas: 3
          type: single
          override:
            service:
              internal:
                metadata:
                  annotations:
                    metallb.universe.tf/address-pool: internalapi
                    metallb.universe.tf/allow-shared-ip: internalapi
                    metallb.universe.tf/loadBalancerIPs: 172.17.0.80 <i class="conum" data-value="3"></i><b>(3)</b>
                spec:
                  type: LoadBalancer
          networkAttachments:
          - storage
EOF</pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>Replace <code>&lt;exported_path&gt;</code> with the exported path in the <code>nfs-server</code>.</td>
</tr>
<tr>
<td><i class="conum" data-value="2"></i><b>2</b></td>
<td>Replace <code>&lt;ip_address&gt;</code> with the IP address that you use to communicate with the <code>nfs-server</code>.</td>
</tr>
<tr>
<td><i class="conum" data-value="3"></i><b>3</b></td>
<td>If you use IPv6, change the load balancer IP to the load balancer IP in your environment, for example, <code>metallb.universe.tf/loadBalancerIPs: fd00:bbbb::80</code>.</td>
</tr>
</table>
</div>
</li>
<li>
<p>Patch the <code>OpenStackControlPlane</code> CR to deploy the Image service with an NFS back end:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc patch openstackcontrolplane openstack --type=merge --patch-file glance_nfs_patch.yaml</pre>
</div>
</div>
</li>
</ol>
</div>
<div class="ulist">
<div class="title">Verification</div>
<ul>
<li>
<p>When <code>GlanceAPI</code> is active, confirm that you can see a single API instance:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc get pods -l service=glance
NAME                      READY   STATUS    RESTARTS
glance-default-single-0   3/3     Running   0</pre>
</div>
</div>
</li>
<li>
<p>Ensure that the description of the pod reports the following output:</p>
<div class="listingblock">
<div class="content">
<pre>Mounts:
...
  nfs:
    Type:      NFS (an NFS mount that lasts the lifetime of a pod)
    Server:    {{ server ip address }}
    Path:      {{ nfs export path }}
    ReadOnly:  false
...</pre>
</div>
</div>
</li>
<li>
<p>Check that the mountpoint that points to <code>/var/lib/glance/images</code> is mapped to the expected <code>nfs server ip</code> and <code>nfs path</code> that you defined in the new default <code>GlanceAPI</code> instance:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc rsh -c glance-api glance-default-single-0

sh-5.1# mount
...
...
{{ ip address }}:/var/nfs on /var/lib/glance/images type nfs4 (rw,relatime,vers=4.2,rsize=1048576,wsize=1048576,namlen=255,hard,proto=tcp,timeo=600,retrans=2,sec=sys,clientaddr=172.18.0.5,local_lock=none,addr=172.18.0.5)
...
...</pre>
</div>
</div>
</li>
<li>
<p>Confirm that the UUID is created in the exported directory on the NFS node. For example:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc rsh openstackclient
$ openstack image list

sh-5.1$  curl -L -o /tmp/cirros-0.6.3-x86_64-disk.img http://download.cirros-cloud.net/0.6.3/cirros-0.6.3-x86_64-disk.img
...
...

sh-5.1$ openstack image create --container-format bare --disk-format raw --file /tmp/cirros-0.6.3-x86_64-disk.img cirros
...
...

sh-5.1$ openstack image list
+--------------------------------------+--------+--------+
| ID                                   | Name   | Status |
+--------------------------------------+--------+--------+
| 634482ca-4002-4a6d-b1d5-64502ad02630 | cirros | active |
+--------------------------------------+--------+--------+</pre>
</div>
</div>
</li>
<li>
<p>On the <code>nfs-server</code> node, the same <code>uuid</code> is in the exported <code>/var/nfs</code>:</p>
<div class="listingblock">
<div class="content">
<pre>$ ls /var/nfs/
634482ca-4002-4a6d-b1d5-64502ad02630</pre>
</div>
</div>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="adopting-image-service-with-ceph-backend_image-service">Adopting the Image service that is deployed with a Red Hat Ceph Storage back end</h4>
<div class="paragraph">
<p>Adopt the Image Service (glance) that you deployed with a Red Hat Ceph Storage back end. Use the <code>customServiceConfig</code> parameter to inject the right configuration to the <code>GlanceAPI</code> instance.</p>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>You have completed the previous adoption steps.</p>
</li>
<li>
<p>Ensure that the Ceph-related secret (<code>ceph-conf-files</code>) is created in
the <code>openstack</code> namespace and that the <code>extraMounts</code> property of the
<code>OpenStackControlPlane</code> custom resource (CR) is configured properly. For more information, see <a href="#configuring-a-ceph-backend_migrating-databases">Configuring a Ceph back end</a>.</p>
<div class="listingblock">
<div class="content">
<pre>$ cat &lt;&lt; EOF &gt; glance_patch.yaml
spec:
  glance:
    enabled: true
    template:
      databaseInstance: openstack
      customServiceConfig: |
        [DEFAULT]
        enabled_backends=default_backend:rbd
        [glance_store]
        default_backend=default_backend
        [default_backend]
        rbd_store_ceph_conf=/etc/ceph/ceph.conf
        rbd_store_user=openstack
        rbd_store_pool=images
        store_description=Ceph glance store backend.
      storage:
        storageRequest: 10G
      glanceAPIs:
        default:
          replicas: 0
          override:
            service:
              internal:
                metadata:
                  annotations:
                    metallb.universe.tf/address-pool: internalapi
                    metallb.universe.tf/allow-shared-ip: internalapi
                    metallb.universe.tf/loadBalancerIPs: 172.17.0.80 <i class="conum" data-value="1"></i><b>(1)</b>
                spec:
                  type: LoadBalancer
          networkAttachments:
          - storage
EOF</pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>If you use IPv6, change the load balancer IP to the load balancer IP in your environment, for example, <code>metallb.universe.tf/loadBalancerIPs: fd00:bbbb::80</code>.</td>
</tr>
</table>
</div>
</li>
</ul>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>If you backed up your Red&#160;Hat OpenStack Platform (RHOSP) services configuration file from the original environment, you can compare it with the confgiuration file that you adopted and ensure that the configuration is correct.
For more information, see <a href="#pulling-configuration-from-tripleo-deployment_adopt-control-plane">Pulling the configuration from a director deployment</a>.</p>
</div>
<div class="listingblock">
<div class="content">
<pre>os-diff diff /tmp/collect_tripleo_configs/glance/etc/glance/glance-api.conf glance_patch.yaml --crd</pre>
</div>
</div>
<div class="paragraph">
<p>This command produces the difference between both ini configuration files.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="ulist">
<div class="title">Procedure</div>
<ul>
<li>
<p>Patch the <code>OpenStackControlPlane</code> CR to deploy the Image service with a Red Hat Ceph Storage back end:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc patch openstackcontrolplane openstack --type=merge --patch-file glance_patch.yaml</pre>
</div>
</div>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="verifying-the-image-service-adoption_image-service">Verifying the Image service adoption</h4>
<div class="paragraph">
<p>Verify that you adopted the Image Service (glance) to the Red&#160;Hat OpenStack Services on OpenShift (RHOSO) 18.0 deployment.</p>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Test the Image service from the Red&#160;Hat OpenStack Platform CLI. You can compare and ensure that the configuration is applied to the Image service pods:</p>
<div class="listingblock">
<div class="content">
<pre>$ os-diff diff /etc/glance/glance.conf.d/02-config.conf glance_patch.yaml --frompod -p glance-api</pre>
</div>
</div>
<div class="paragraph">
<p>If no line appears, then the configuration is correct.</p>
</div>
</li>
<li>
<p>Inspect the resulting Image service pods:</p>
<div class="listingblock">
<div class="content">
<pre>GLANCE_POD=`oc get pod |grep glance-default | cut -f 1 -d' ' | head -n 1`
oc exec -t $GLANCE_POD -c glance-api -- cat /etc/glance/glance.conf.d/02-config.conf

[DEFAULT]
enabled_backends=default_backend:rbd
[glance_store]
default_backend=default_backend
[default_backend]
rbd_store_ceph_conf=/etc/ceph/ceph.conf
rbd_store_user=openstack
rbd_store_pool=images
store_description=Ceph glance store backend.</pre>
</div>
</div>
</li>
<li>
<p>If you use a Red Hat Ceph Storage back end, ensure that the Red Hat Ceph Storage secrets are mounted:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc exec -t $GLANCE_POD -c glance-api -- ls /etc/ceph
ceph.client.openstack.keyring
ceph.conf</pre>
</div>
</div>
</li>
<li>
<p>Check that the service is active, and that the endpoints are updated in the RHOSP CLI:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc rsh openstackclient
$ openstack service list | grep image

| fc52dbffef36434d906eeb99adfc6186 | glance    | image        |

$ openstack endpoint list | grep image

| 569ed81064f84d4a91e0d2d807e4c1f1 | regionOne | glance       | image        | True    | internal  | http://glance-internal-openstack.apps-crc.testing   |
| 5843fae70cba4e73b29d4aff3e8b616c | regionOne | glance       | image        | True    | public    | http://glance-public-openstack.apps-crc.testing     |</pre>
</div>
</div>
</li>
<li>
<p>Check that the images that you previously listed in the source cloud are available in the adopted service:</p>
<div class="listingblock">
<div class="content">
<pre>$ openstack image list
+--------------------------------------+--------+--------+
| ID                                   | Name   | Status |
+--------------------------------------+--------+--------+
| c3158cad-d50b-452f-bec1-f250562f5c1f | cirros | active |
+--------------------------------------+--------+--------+</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
</div>
<div class="sect2">
<h3 id="adopting-the-placement-service_adopt-control-plane">Adopting the Placement service</h3>
<div class="paragraph">
<p>To adopt the Placement service, you patch an existing <code>OpenStackControlPlane</code> custom resource (CR) that has the Placement service disabled. The patch starts the service with the configuration parameters that are provided by the Red&#160;Hat OpenStack Platform (RHOSP) environment.</p>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>You import your databases to MariaDB instances on the control plane. For more information, see <a href="#migrating-databases-to-mariadb-instances_migrating-databases">Migrating databases to MariaDB instances</a>.</p>
</li>
<li>
<p>You adopt the Identity service (keystone). For more information, see <a href="#adopting-the-identity-service_adopt-control-plane">Adopting the Identity service</a>.</p>
</li>
</ul>
</div>
<div class="ulist">
<div class="title">Procedure</div>
<ul>
<li>
<p>Patch the <code>OpenStackControlPlane</code> CR to deploy the Placement service:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc patch openstackcontrolplane openstack --type=merge --patch '
spec:
  placement:
    enabled: true
    apiOverride:
      route: {}
    template:
      databaseInstance: openstack
      databaseAccount: placement
      secret: osp-secret
      override:
        service:
          internal:
            metadata:
              annotations:
                metallb.universe.tf/address-pool: internalapi
                metallb.universe.tf/allow-shared-ip: internalapi
                metallb.universe.tf/loadBalancerIPs: 172.17.0.80 <i class="conum" data-value="1"></i><b>(1)</b>
            spec:
              type: LoadBalancer
'</pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>If you use IPv6, change the load balancer IP to the load balancer IP in your environment, for example, <code>metallb.universe.tf/loadBalancerIPs: fd00:bbbb::80</code>.</td>
</tr>
</table>
</div>
</li>
</ul>
</div>
<div class="ulist">
<div class="title">Verification</div>
<ul>
<li>
<p>Check that the Placement service endpoints are defined and pointing to the
control plane FQDNs, and that the Placement API responds:</p>
<div class="listingblock">
<div class="content">
<pre>$ alias openstack="oc exec -t openstackclient -- openstack"

$ openstack endpoint list | grep placement


# Without OpenStack CLI placement plugin installed:
PLACEMENT_PUBLIC_URL=$(openstack endpoint list -c 'Service Name' -c 'Service Type' -c URL | grep placement | grep public | awk '{ print $6; }')
oc exec -t openstackclient -- curl "$PLACEMENT_PUBLIC_URL"

# With OpenStack CLI placement plugin installed:
openstack resource class list</pre>
</div>
</div>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="adopting-the-bare-metal-provisioning-service_adopt-control-plane">Adopting the Bare Metal Provisioning service</h3>
<div class="paragraph">
<p>Review information about your Bare Metal Provisioning service (ironic) configuration and then adopt the Bare Metal Provisioning service to the Red&#160;Hat OpenStack Services on OpenShift control plane.</p>
</div>
<div class="sect3">
<h4 id="con_bare-metal-provisioning-service-configurations_adopting-bare-metal-provisioning">Bare Metal Provisioning service configurations</h4>
<div class="paragraph">
<p>You configure the Bare Metal Provisioning service (ironic) by using configuration snippets. For more information about configuring the control plane with the Bare Metal Provisioning service, see <a href="https://docs.redhat.com/en/documentation/red_hat_openstack_services_on_openshift/18.0/html/customizing_the_red_hat_openstack_services_on_openshift_deployment/index">Customizing the Red Hat OpenStack Services on OpenShift deployment</a>.</p>
</div>
<div class="paragraph">
<p>Some Bare Metal Provisioning service configuration is overridden in director, for example, PXE Loader file names are often overridden at intermediate layers. You must pay attention to the settings you apply in your Red&#160;Hat OpenStack Services on OpenShift (RHOSO) deployment. The <code>ironic-operator</code> applies a reasonable working default configuration, but if you override them with your prior configuration, your experience might not be ideal or your new Bare Metal Provisioning service fails to operate. Similarly, additional configuration might be necessary, for example, if you enable and use additional hardware types in your <code>ironic.conf</code> file.</p>
</div>
<div class="paragraph">
<p>The model of reasonable defaults includes commonly used hardware-types and driver interfaces. For example, the <code>redfish-virtual-media</code> boot interface and the <code>ramdisk</code> deploy interface are enabled by default. If you add new bare metal nodes after the adoption is complete, the driver interface selection occurs based on the order of precedence in the configuration if you do not explicitly set it on the node creation request or as an established default in the <code>ironic.conf</code> file.</p>
</div>
<div class="paragraph">
<p>Some configuration parameters do not need to be set on an individual node level, for example, network UUID values, or they are centrally configured in the <code>ironic.conf</code> file, as the setting controls security behavior.</p>
</div>
<div class="paragraph">
<p>It is critical that you maintain the following parameters that you configured and formatted as <code>[section]</code> and parameter name from the prior deployment to the new deployment. These parameters that govern the underlying behavior and values in the previous configuration would have used specific values if set.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>[neutron]cleaning_network</p>
</li>
<li>
<p>[neutron]provisioning_network</p>
</li>
<li>
<p>[neutron]rescuing_network</p>
</li>
<li>
<p>[neutron]inspection_network</p>
</li>
<li>
<p>[conductor]automated_clean</p>
</li>
<li>
<p>[deploy]erase_devices_priority</p>
</li>
<li>
<p>[deploy]erase_devices_metadata_priority</p>
</li>
<li>
<p>[conductor]force_power_state_during_sync</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>You can set the following parameters individually on a node. However, you might choose to use embedded configuration options to avoid the need to set the parameters individually when creating or managing bare metal nodes. Check your prior <code>ironic.conf</code> file for these parameters, and if set, apply a specific override configuration.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>[conductor]bootloader</p>
</li>
<li>
<p>[conductor]rescue_ramdisk</p>
</li>
<li>
<p>[conductor]rescue_kernel</p>
</li>
<li>
<p>[conductor]deploy_kernel</p>
</li>
<li>
<p>[conductor]deploy_ramdisk</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The instances of <code>kernel_append_params</code>, formerly <code>pxe_append_params</code> in the <code>[pxe]</code> and <code>[redfish]</code> configuration sections, are used to apply boot time options like "console" for the deployment ramdisk and as such often must be changed.</p>
</div>
<div class="admonitionblock warning">
<table>
<tr>
<td class="icon">
<i class="fa icon-warning" title="Warning"></i>
</td>
<td class="content">
You cannot migrate hardware types that are set with the <code>ironic.conf</code> file <code>enabled_hardware_types</code> parameter, and hardware type driver interfaces starting with <code>staging-</code> into the adopted configuration.
</td>
</tr>
</table>
</div>
</div>
<div class="sect3">
<h4 id="deploying-the-bare-metal-provisioning-service_adopting-bare-metal-provisioning">Deploying the Bare Metal Provisioning service</h4>
<div class="paragraph">
<p>To deploy the Bare Metal Provisioning service (ironic), you patch an existing <code>OpenStackControlPlane</code> custom resource (CR) that has the Bare Metal Provisioning service disabled. The <code>ironic-operator</code> applies the configuration and starts the Bare Metal Provisioning services. After the services are running, the Bare Metal Provisioning service automatically begins polling the power state of the bare-metal nodes that it manages.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
By default, RHOSO versions 18.0 and later of the Bare Metal Provisioning service include a new multi-tenant aware role-based access control (RBAC) model. As a result, bare-metal nodes might be missing when you run the <code>openstack baremetal node list</code> command after you adopt the Bare Metal Provisioning service. Your nodes are not deleted. Due to the increased access restrictions in the RBAC model, you must identify which project owns the missing bare-metal nodes and set the <code>owner</code> field on each missing bare-metal node.
</td>
</tr>
</table>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>You have imported the service databases into the control plane database.</p>
</li>
<li>
<p>The Bare Metal Provisioning service is disabled in the RHOSO 18.0. The following command should return a string of <code>false</code>:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc get openstackcontrolplanes.core.openstack.org &lt;name&gt; -o jsonpath='{.spec.ironic.enabled}'</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Replace <code>&lt;name&gt;</code> with the name of your existing <code>OpenStackControlPlane</code> CR, for example, <code>openstack-control-plane</code>.</p>
</li>
</ul>
</div>
</li>
<li>
<p>The Identity service (keystone), Networking service (neutron), and Image Service (glance) are operational.</p>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
If you use the Bare Metal Provisioning service in a Bare Metal as a Service configuration, do not adopt the Compute service (nova) before you adopt the Bare Metal Provisioning service.
</td>
</tr>
</table>
</div>
</li>
<li>
<p>For the Bare Metal Provisioning service conductor services, the services must be able to reach Baseboard Management Controllers of hardware that is configured to be managed by the Bare Metal Provisioning service. If this hardware is unreachable, the nodes might enter "maintenance" state and be unavailable until connectivity is restored later.</p>
</li>
<li>
<p>You have downloaded the <code>ironic.conf</code> file locally:</p>
<div class="listingblock">
<div class="content">
<pre>$CONTROLLER1_SSH cat /var/lib/config-data/puppet-generated/ironic/etc/ironic/ironic.conf &gt; ironic.conf</pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
This configuration file must come from one of the Controller nodes and not a director undercloud node. The director undercloud node operates with different configuration that does not apply when you adopt the Overcloud Ironic deployment.
</td>
</tr>
</table>
</div>
</li>
<li>
<p>If you are adopting the Ironic Inspector service, you need the value of the <code>IronicInspectorSubnets</code> director parameter. Use the same values to populate the <code>dhcpRanges</code> parameter in the RHOSO environment.</p>
</li>
<li>
<p>You have defined the following shell variables. Replace the following example values with values that apply to your environment:</p>
<div class="listingblock">
<div class="content">
<pre>$ alias openstack="oc exec -t openstackclient -- openstack"</pre>
</div>
</div>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Patch the <code>OpenStackControlPlane</code> custom resource (CR) to deploy the Bare Metal Provisioning service:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc patch openstackcontrolplane openstack --type=merge --patch '
spec:
  ironic:
    enabled: true
    template:
      rpcTransport: oslo
      databaseInstance: openstack
      ironicAPI:
        replicas: 1
        override:
          service:
            internal:
              metadata:
                annotations:
                  metallb.universe.tf/address-pool: internalapi
                  metallb.universe.tf/allow-shared-ip: internalapi
                  metallb.universe.tf/loadBalancerIPs: 172.17.0.80 <i class="conum" data-value="1"></i><b>(1)</b>
              spec:
                type: LoadBalancer
      ironicConductors:
      - replicas: 1
        networkAttachments:
          - baremetal
        provisionNetwork: baremetal
        storageRequest: 10G
        customServiceConfig: |
          [neutron]
          cleaning_network=&lt;cleaning network uuid&gt;
          provisioning_network=&lt;provisioning network uuid&gt;
          rescuing_network=&lt;rescuing network uuid&gt;
          inspection_network=&lt;introspection network uuid&gt;
          [conductor]
          automated_clean=true
      ironicInspector:
        replicas: 1
        inspectionNetwork: baremetal
        networkAttachments:
          - baremetal
        dhcpRanges:
          - name: inspector-0
            cidr: 172.20.1.0/24
            start: 172.20.1.190
            end: 172.20.1.199
            gateway: 172.20.1.1
        serviceUser: ironic-inspector
        databaseAccount: ironic-inspector
        passwordSelectors:
          database: IronicInspectorDatabasePassword
          service: IronicInspectorPassword
      ironicNeutronAgent:
        replicas: 1
        rabbitMqClusterName: rabbitmq
      secret: osp-secret
'</pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>If you use IPv6, change the load balancer IP to the load balancer IP in your environment, for example, <code>metallb.universe.tf/loadBalancerIPs: fd00:bbbb::80</code>.</td>
</tr>
</table>
</div>
</li>
<li>
<p>Wait for the Bare Metal Provisioning service control plane services CRs to become ready:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc wait --for condition=Ready --timeout=300s ironics.ironic.openstack.org ironic</pre>
</div>
</div>
</li>
<li>
<p>Verify that the individual services are ready:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc wait --for condition=Ready --timeout=300s ironicapis.ironic.openstack.org ironic-api
$ oc wait --for condition=Ready --timeout=300s ironicconductors.ironic.openstack.org ironic-conductor
$ oc wait --for condition=Ready --timeout=300s ironicinspectors.ironic.openstack.org ironic-inspector
$ oc wait --for condition=Ready --timeout=300s ironicneutronagents.ironic.openstack.org ironic-ironic-neutron-agent</pre>
</div>
</div>
</li>
<li>
<p>Update the DNS Nameservers on the provisioning, cleaning, and rescue networks:</p>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
For name resolution to work for Bare Metal Provisioning service operations, you must set the DNS nameserver to use the internal DNS servers in the RHOSO control plane:
</td>
</tr>
</table>
</div>
<div class="listingblock">
<div class="content">
<pre>$ openstack subnet set --dns-nameserver 192.168.122.80 provisioning-subnet</pre>
</div>
</div>
</li>
<li>
<p>Verify that no Bare Metal Provisioning service nodes are missing from the node list:</p>
<div class="listingblock">
<div class="content">
<pre>$ openstack baremetal node list</pre>
</div>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
If the <code>openstack baremetal node list</code> command output reports an incorrect power status, wait a few minutes and re-run the command to see if the output syncs with the actual state of the hardware being managed. The time required for the Bare Metal Provisioning service to review and reconcile the power state of bare-metal nodes depends on the number of operating conductors through the <code>replicas</code> parameter and which are present in the Bare Metal Provisioning service deployment being adopted.
</td>
</tr>
</table>
</div>
</li>
<li>
<p>If any Bare Metal Provisioning service nodes are missing from the <code>openstack baremetal node list</code> command, temporarily disable the new RBAC policy to see the nodes again:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc patch openstackcontrolplane openstack --type=merge --patch '
spec:
  ironic:
    enabled: true
    template:
      databaseInstance: openstack
      ironicAPI:
        replicas: 1
        customServiceConfig: |
          [oslo_policy]
          enforce_scope=false
          enforce_new_defaults=false
'</pre>
</div>
</div>
<div class="paragraph">
<p>After this configuration is applied, the operator restarts the Ironic API service and disables the new RBAC policy that is enabled by default.</p>
</div>
</li>
<li>
<p>View the bare-metal nodes that do not have an owner assigned:</p>
<div class="listingblock">
<div class="content">
<pre>$ openstack baremetal node list --long -c UUID -c Owner -c 'Provisioning State'</pre>
</div>
</div>
</li>
<li>
<p>Assign all bare-metal nodes with no owner to a new project, for example, the admin project:</p>
<div class="listingblock">
<div class="content">
<pre>ADMIN_PROJECT_ID=$(openstack project show -c id -f value --domain default admin)
for node in $(openstack baremetal node list -f json -c UUID -c Owner | jq -r '.[] | select(.Owner == null) | .UUID'); do openstack baremetal node set --owner $ADMIN_PROJECT_ID $node; done</pre>
</div>
</div>
</li>
<li>
<p>Re-apply the default RBAC by removing the <code>customServiceConfig</code> section or by setting the following values in the <code>customServiceConfig</code> section to <code>true</code>. For example:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc patch openstackcontrolplane openstack --type=merge --patch '
spec:
  ironic:
    enabled: true
    template:
      databaseInstance: openstack
      ironicAPI:
        replicas: 1
        customServiceConfig: |
          [oslo_policy]
          enforce_scope=true
          enforce_new_defaults=true
'</pre>
</div>
</div>
</li>
</ol>
</div>
<div class="olist arabic">
<div class="title">Verification</div>
<ol class="arabic">
<li>
<p>Verify the list of endpoints:</p>
<div class="listingblock">
<div class="content">
<pre>$ openstack endpoint list |grep ironic</pre>
</div>
</div>
</li>
<li>
<p>Verify the list of bare-metal nodes:</p>
<div class="listingblock">
<div class="content">
<pre>$ openstack baremetal node list</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
</div>
<div class="sect2">
<h3 id="adopting-the-compute-service_adopt-control-plane">Adopting the Compute service</h3>
<div class="paragraph">
<p>To adopt the Compute service (nova), you patch an existing <code>OpenStackControlPlane</code> custom resource (CR) where the Compute service is disabled. The patch starts the service with the configuration parameters that are provided by the Red&#160;Hat OpenStack Platform (RHOSP) environment. The following procedure describes a single-cell setup.</p>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>You have completed the previous adoption steps.</p>
</li>
<li>
<p>You have defined the following shell variables. Replace the following example values with the values that are correct for your environment:</p>
<div class="listingblock">
<div class="content">
<pre>alias openstack="oc exec -t openstackclient -- openstack"

DEFAULT_CELL_NAME="cell3"
RENAMED_CELLS="cell1 cell2 $DEFAULT_CELL_NAME"</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>The source cloud <code>default</code> cell takes a new <code>$DEFAULT_CELL_NAME</code>. In a multi-cell adoption scenario, the <em>default</em> cell might retain its original name,<code>DEFAULT_CELL_NAME=default</code>, or become renamed as a cell that is free for use. Do not use other existing cell names for <code>DEFAULT_CELL_NAME</code>, except for <code>default</code>.</p>
</li>
<li>
<p>If you deployed the source cloud with a <code>default</code> cell, and want to rename it during adoption, define the new name that you want to use, as shown in the following example:</p>
<div class="listingblock">
<div class="content">
<pre>DEFAULT_CELL_NAME="cell1"
RENAMED_CELLS="cell1"</pre>
</div>
</div>
</li>
</ul>
</div>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Patch the <code>OpenStackControlPlane</code> CR to deploy the Compute service:</p>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
This procedure assumes that Compute service metadata is deployed on the top level and not on each cell level. If the RHOSP deployment has a per-cell metadata deployment, adjust the following patch as needed. You cannot run the metadata service in <code>cell0</code>.
To enable the metadata services of a local cell, set the <code>enabled</code> property in the <code>metadataServiceTemplate</code> field of the local cell to <code>true</code> in the <code>OpenStackControlPlane</code> CR.
</td>
</tr>
</table>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">$ rm -f celltemplates
for CELL in $(echo $RENAMED_CELLS); do
 $ cat &gt;&gt; celltemplates &lt;&lt; EOF
        ${CELL}:
          hasAPIAccess: true <i class="conum" data-value="1"></i><b>(1)</b>
          cellDatabaseAccount: nova-$CELL
          cellDatabaseInstance: openstack-$CELL <i class="conum" data-value="2"></i><b>(2)</b>
          cellMessageBusInstance: rabbitmq-$CELL <i class="conum" data-value="3"></i><b>(3)</b>
          metadataServiceTemplate:
            enabled: false
            override:
                service:
                  metadata:
                    annotations:
                      metallb.universe.tf/address-pool: internalapi
                      metallb.universe.tf/allow-shared-ip: internalapi
                      metallb.universe.tf/loadBalancerIPs: 172.17.0.$(( 79 + ${CELL##*cell} ))
                  spec:
                    type: LoadBalancer
            customServiceConfig: |
              [workarounds]
              disable_compute_service_check_for_ffu=true
          conductorServiceTemplate:
            customServiceConfig: |
              [workarounds]
              disable_compute_service_check_for_ffu=true
EOF
done

$ cat &gt; oscp-patch.yaml &lt;&lt; EOF
spec:
  nova:
    enabled: true
    apiOverride:
      route: {}
    template:
      secret: osp-secret
      apiDatabaseAccount: nova-api
      apiServiceTemplate:
        override:
          service:
            internal:
              metadata:
                annotations:
                  metallb.universe.tf/address-pool: internalapi
                  metallb.universe.tf/allow-shared-ip: internalapi
                  metallb.universe.tf/loadBalancerIPs: 172.17.0.80 <i class="conum" data-value="4"></i><b>(4)</b>
              spec:
                type: LoadBalancer
        customServiceConfig: |
          [workarounds]
          disable_compute_service_check_for_ffu=true
      metadataServiceTemplate:
        enabled: true
        override:
          service:
            metadata:
              annotations:
                metallb.universe.tf/address-pool: internalapi
                metallb.universe.tf/allow-shared-ip: internalapi
                metallb.universe.tf/loadBalancerIPs: 172.17.0.80
            spec:
              type: LoadBalancer
        customServiceConfig: |
          [workarounds]
          disable_compute_service_check_for_ffu=true
      schedulerServiceTemplate:
        customServiceConfig: |
          [workarounds]
          disable_compute_service_check_for_ffu=true
      cellTemplates:
        cell0:
          hasAPIAccess: true
          cellDatabaseAccount: nova-cell0
          cellDatabaseInstance: openstack
          cellMessageBusInstance: rabbitmq
          conductorServiceTemplate:
            customServiceConfig: |
              [workarounds]
              disable_compute_service_check_for_ffu=true
EOF
$ cat celltemplates &gt;&gt; oscp-patch.yaml
$ oc patch openstackcontrolplane openstack  --type=merge --patch-file=oscp-patch.yaml</code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>In the source cloud, cells are always configured with the main Nova API database upcall access. You can disable upcall access to the API by setting <code>hasAPIAccess</code> to <code>false</code>. However, do not make changes to the API during adoption.</td>
</tr>
<tr>
<td><i class="conum" data-value="2"></i><b>2</b></td>
<td>The database instance that is used by the cell. The database instance names must match the names that are defined in the <code>OpenStackControlPlane</code> CR that you created in when you deployed the back-end services as described in <a href="#deploying-backend-services__migrating-databases">Deploying back-end services</a>.</td>
</tr>
<tr>
<td><i class="conum" data-value="3"></i><b>3</b></td>
<td>The message bus instance that is used by the cell. The message bus instance names must match the names that are defined in the <code>OpenStackControlPlane</code> CR.</td>
</tr>
<tr>
<td><i class="conum" data-value="4"></i><b>4</b></td>
<td>If you use IPv6, change the load balancer IP to the load balancer IP in your environment, for example, <code>metallb.universe.tf/loadBalancerIPs: fd00:bbbb::80</code>.</td>
</tr>
</table>
</div>
</li>
<li>
<p>If you are adopting the Compute service with the Bare Metal Provisioning service (ironic), append the <code>novaComputeTemplates</code> field with the following content in each cell in the Compute service CR patch. For example:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">        cell1:
          novaComputeTemplates:
            standalone:
              customServiceConfig: |
                [DEFAULT]
                host = &lt;hostname&gt;
                [workarounds]
                disable_compute_service_check_for_ffu=true
              computeDriver: ironic.IronicDriver
        ...</code></pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Replace <code>&lt;hostname&gt;</code> with the hostname of the node that is running the <code>ironic</code> Compute driver in the source cloud.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Wait for the CRs for the Compute control plane services to be ready:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc wait --for condition=Ready --timeout=300s Nova/nova</pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
The local Conductor services are started for each cell, while the superconductor runs in <code>cell0</code>.
Note that <code>disable_compute_service_check_for_ffu</code> is mandatory for all imported Compute services until the external data plane is imported, and until the Compute services are fast-forward upgraded. For more information, see <a href="#adopting-compute-services-to-the-data-plane_data-plane">Adopting Compute services to the RHOSO data plane</a> and <a href="#performing-a-fast-forward-upgrade-on-compute-services_data-plane">Upgrading Compute services</a>.
</td>
</tr>
</table>
</div>
</li>
</ol>
</div>
<div class="ulist">
<div class="title">Verification</div>
<ul>
<li>
<p>Check that Compute service endpoints are defined and pointing to the
control plane FQDNs, and that the Nova API responds:</p>
<div class="listingblock">
<div class="content">
<pre>$ openstack endpoint list | grep nova
$ openstack server list</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Compare the outputs with the topology-specific configuration in <a href="#proc_retrieving-topology-specific-service-configuration_migrating-databases">Retrieving topology-specific service configuration</a>.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Query the superconductor to check that the expected cells exist, and compare it to its pre-adoption values:</p>
<div class="listingblock">
<div class="content">
<pre>$ for CELL in $(echo $CELLS); do
  set +u
  . ~/.source_cloud_exported_variables_$CELL
  set -u
  RCELL=$CELL
  [ "$CELL" = "default" ] &amp;&amp; RCELL=$DEFAULT_CELL_NAME

  echo "comparing $CELL to $RCELL"
  echo $PULL_OPENSTACK_CONFIGURATION_NOVAMANAGE_CELL_MAPPINGS | grep -F "| $CELL |"
$ oc rsh nova-cell0-conductor-0 nova-manage cell_v2 list_cells | grep -F "| $RCELL |"
done</pre>
</div>
</div>
<div class="paragraph">
<p>The following changes are expected for each cell:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The <code>cellX</code> <code>nova</code> database and username become <code>nova_cellX</code>.</p>
</li>
<li>
<p>The <code>default</code> cell is renamed to <code>DEFAULT_CELL_NAME</code>. The <code>default</code> cell might retain the original name if there are multiple cells.</p>
</li>
<li>
<p>The RabbitMQ transport URL no longer uses <code>guest</code>.</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>At this point, the Compute service control plane services do not control the existing Compute service workloads. The control plane manages the data plane only after the data adoption process is completed. For more information, see <a href="#adopting-compute-services-to-the-data-plane_data-plane">Adopting Compute services to the RHOSO data plane</a>.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
To import external Compute services to the RHOSO data plane, you must upgrade them first.
For more information, see <a href="#adopting-compute-services-to-the-data-plane_data-plane">Adopting Compute services to the RHOSO data plane</a>, and <a href="#performing-a-fast-forward-upgrade-on-compute-services_data-plane">Performing a fast-forward upgrade on Compute services</a>.
</td>
</tr>
</table>
</div>
</div>
<div class="sect2">
<h3 id="adopting-the-block-storage-service_adopt-control-plane">Adopting the Block Storage service</h3>
<div class="paragraph">
<p>To adopt a director-deployed Block Storage service (cinder), create the manifest based on the existing <code>cinder.conf</code> file, deploy the Block Storage service, and validate the new deployment.</p>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>You have reviewed the Block Storage service limitations. For more information, see <a href="#block-storage-limitations_storage-requirements">Limitations for adopting the Block Storage service</a>.</p>
</li>
<li>
<p>You have planned the placement of the Block Storage services.</p>
</li>
<li>
<p>You have prepared the Red Hat OpenShift Container Platform (RHOCP) nodes where the volume and backup services run. For more information, see <a href="#openshift-preparation-for-block-storage-adoption_storage-requirements">RHOCP preparation for Block Storage service adoption</a>.</p>
</li>
<li>
<p>The Block Storage service (cinder) is stopped.</p>
</li>
<li>
<p>The service databases are imported into the control plane MariaDB.</p>
</li>
<li>
<p>The Identity service (keystone) and Key Manager service (barbican) are adopted.</p>
</li>
<li>
<p>The Storage network is correctly configured on the RHOCP cluster.</p>
</li>
<li>
<p>The contents of <code>cinder.conf</code> file. Download the file so that you can access it locally:</p>
<div class="listingblock">
<div class="content">
<pre>$CONTROLLER1_SSH cat /var/lib/config-data/puppet-generated/cinder/etc/cinder/cinder.conf &gt; cinder.conf</pre>
</div>
</div>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Create a new file, for example, <code>cinder_api.patch</code>, and apply the configuration:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc patch openstackcontrolplane openstack --type=merge --patch-file=&lt;patch_name&gt;</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Replace <code>&lt;patch_name&gt;</code> with the name of your patch file.</p>
<div class="paragraph">
<p>The following example shows a <code>cinder_api.patch</code> file:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">spec:
  extraMounts:
  - extraVol:
    - extraVolType: Ceph
      mounts:
      - mountPath: /etc/ceph
        name: ceph
        readOnly: true
      propagation:
      - CinderVolume
      - CinderBackup
      - Glance
      volumes:
      - name: ceph
        projected:
          sources:
          - secret:
              name: ceph-conf-files
  cinder:
    enabled: true
    apiOverride:
      route: {}
    template:
      databaseInstance: openstack
      databaseAccount: cinder
      secret: osp-secret
      cinderAPI:
        override:
          service:
            internal:
              metadata:
                annotations:
                  metallb.universe.tf/address-pool: internalapi
                  metallb.universe.tf/allow-shared-ip: internalapi
                  metallb.universe.tf/loadBalancerIPs: 172.17.0.80 <i class="conum" data-value="1"></i><b>(1)</b>
              spec:
                type: LoadBalancer
        replicas: 1
        customServiceConfig: |
          [DEFAULT]
          default_volume_type=tripleo
      cinderScheduler:
        replicas: 0
      cinderBackup:
        networkAttachments:
        - storage
        replicas: 0
      cinderVolumes:
        ceph:
          networkAttachments:
          - storage
          replicas: 0</code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>If you use IPv6, change the load balancer IP to the load balancer IP in your environment, for example, <code>metallb.universe.tf/loadBalancerIPs: fd00:bbbb::80</code>.</td>
</tr>
</table>
</div>
</li>
</ul>
</div>
</li>
<li>
<p>Retrieve the list of the previous scheduler and backup services:</p>
<div class="listingblock">
<div class="content">
<pre>$ openstack volume service list

+------------------+------------------------+------+---------+-------+----------------------------+
| Binary           | Host                   | Zone | Status  | State | Updated At                 |
+------------------+------------------------+------+---------+-------+----------------------------+
| cinder-scheduler | standalone.localdomain | nova | enabled | down  | 2024-11-04T17:47:14.000000 |
| cinder-backup    | standalone.localdomain | nova | enabled | down  | 2024-11-04T17:47:14.000000 |
| cinder-volume    | hostgroup@tripleo_ceph | nova | enabled | down  | 2024-11-04T17:47:14.000000 |
+------------------+------------------------+------+---------+-------+----------------------------+</pre>
</div>
</div>
</li>
<li>
<p>Remove services for hosts that are in the <code>down</code> state:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc exec -t cinder-api-0 -c cinder-api -- cinder-manage service remove &lt;service_binary&gt; &lt;service_host&gt;</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Replace <code>&lt;service_binary&gt;</code> with the name of the binary, for example, <code>cinder-backup</code>.</p>
</li>
<li>
<p>Replace <code>&lt;service_host&gt;</code> with the host name, for example, <code>cinder-backup-0</code>.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Deploy the scheduler, backup, and volume services:</p>
<div class="ulist">
<ul>
<li>
<p>Create another file, for example, <code>cinder_services.patch</code>, and apply the configuration:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc patch openstackcontrolplane openstack --type=merge --patch-file=&lt;patch_name&gt;</pre>
</div>
</div>
</li>
<li>
<p>Replace <code>&lt;patch_name&gt;</code> with the name of your patch file.</p>
</li>
<li>
<p>The following example shows a <code>cinder_services.patch</code> file for a Ceph RBD deployment:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">spec:
  cinder:
    enabled: true
    template:
      cinderScheduler:
        replicas: 1
      cinderBackup:
        networkAttachments:
        - storage
        replicas: 1
        customServiceConfig: |
          [DEFAULT]
          backup_driver=cinder.backup.drivers.ceph.CephBackupDriver
          backup_ceph_conf=/etc/ceph/ceph.conf
          backup_ceph_user=openstack
          backup_ceph_pool=backups
      cinderVolumes:
        ceph:
          networkAttachments:
          - storage
          replicas: 1
          customServiceConfig: |
            [tripleo_ceph]
            backend_host=hostgroup
            volume_backend_name=tripleo_ceph
            volume_driver=cinder.volume.drivers.rbd.RBDDriver
            rbd_ceph_conf=/etc/ceph/ceph.conf
            rbd_user=openstack
            rbd_pool=volumes
            rbd_flatten_volume_from_snapshot=False
            report_discard_supported=True</code></pre>
</div>
</div>
</li>
</ul>
</div>
</li>
<li>
<p>Configure the NetApp NFS Block Storage volume service:</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>Create secrets that include sensitive information such as hostnames, passwords, and usernames to access the third-party NetApp NFS storage. You can find the credentials in the <code>cinder.conf</code> file that was generated from the director deployment.</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">$ oc apply -f - &lt;&lt;EOF
apiVersion: v1
kind: Secret
metadata:
  labels:
    service: cinder
    component: cinder-volume
  name: cinder-volume-ontap-secrets
type: Opaque
stringData:
  ontap-cinder-secrets: |
    [ontap-nfs]
    netapp_login= netapp_username
    netapp_password= netapp_password
    netapp_vserver= netapp_vserver
    nas_host= netapp_nfsip
    nas_share_path=/netapp_nfspath
    netapp_pool_name_search_pattern=(netapp_poolpattern)
EOF</code></pre>
</div>
</div>
</li>
<li>
<p>Patch the <code>OpenStackControlPlane</code> CR to deploy NetApp NFS Block Storage volume back end:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc patch openstackcontrolplane openstack --type=merge --patch-file=&lt;cinder_netappNFS.patch&gt;</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>The following example shows a <code>cinder_netappNFS.patch</code> file that configures a NetApp NFS Block Storage volume service:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">spec:
  cinder:
    enabled: true
    template:
      cinderVolumes:
        ontap-nfs:
          networkAttachments:
            - storage
          customServiceConfig: |
            [ontap-nfs]
            volume_backend_name=ontap-nfs
            volume_driver=cinder.volume.drivers.netapp.common.NetAppDriver
            nfs_snapshot_support=true
            nas_secure_file_operations=false
            nas_secure_file_permissions=false
            netapp_server_hostname= netapp_backendip
            netapp_server_port=80
            netapp_storage_protocol=nfs
            netapp_storage_family=ontap_cluster
          customServiceConfigSecrets:
          - cinder-volume-ontap-secrets</code></pre>
</div>
</div>
</li>
</ul>
</div>
</li>
</ol>
</div>
</li>
<li>
<p>Check if all the services are up and running:</p>
<div class="listingblock">
<div class="content">
<pre>$ openstack volume service list

+------------------+------------------------+------+---------+-------+----------------------------+
| Binary           | Host                   | Zone | Status  | State | Updated At                 |
+------------------+------------------------+------+---------+-------+----------------------------+
| cinder-volume    | hostgroup@tripleo_ceph | nova | enabled | up    | 2023-06-28T17:00:03.000000 |
| cinder-scheduler | cinder-scheduler-0     | nova | enabled | up    | 2023-06-28T17:00:02.000000 |
| cinder-backup    | cinder-backup-0        | nova | enabled | up    | 2023-06-28T17:00:01.000000 |
+------------------+------------------------+------+---------+-------+----------------------------+</pre>
</div>
</div>
</li>
<li>
<p>Apply the DB data migrations:</p>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>You are not required to run the data migrations at this step, but you must run them before the next upgrade. However, for adoption, you can run the migrations now to ensure that there are no issues before you run production workloads on the deployment.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="listingblock">
<div class="content">
<pre>$ oc exec -it cinder-scheduler-0 -- cinder-manage db online_data_migrations</pre>
</div>
</div>
</li>
</ol>
</div>
<div class="olist arabic">
<div class="title">Verification</div>
<ol class="arabic">
<li>
<p>Ensure that the <code>openstack</code> alias is defined:</p>
<div class="listingblock">
<div class="content">
<pre>$ alias openstack="oc exec -t openstackclient -- openstack"</pre>
</div>
</div>
</li>
<li>
<p>Confirm that Block Storage service endpoints are defined and pointing to the control plane FQDNs:</p>
<div class="listingblock">
<div class="content">
<pre>$ openstack endpoint list --service &lt;endpoint&gt;</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Replace <code>&lt;endpoint&gt;</code> with the name of the endpoint that you want to confirm.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Confirm that the Block Storage services are running:</p>
<div class="listingblock">
<div class="content">
<pre>$ openstack volume service list</pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Cinder API services do not appear in the list. However, if you get a response from the <code>openstack volume service list</code> command, that means at least one of the cinder API services is running.
</td>
</tr>
</table>
</div>
</li>
<li>
<p>Confirm that you have your previous volume types, volumes, snapshots, and backups:</p>
<div class="listingblock">
<div class="content">
<pre>$ openstack volume type list
$ openstack volume list
$ openstack volume snapshot list
$ openstack volume backup list</pre>
</div>
</div>
</li>
<li>
<p>To confirm that the configuration is working, perform the following steps:</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>Create a volume from an image to check that the connection to Image Service (glance) is working:</p>
<div class="listingblock">
<div class="content">
<pre>$ openstack volume create --image cirros --bootable --size 1 disk_new</pre>
</div>
</div>
</li>
<li>
<p>Back up the previous attached volume:</p>
<div class="listingblock">
<div class="content">
<pre>$ openstack --os-volume-api-version 3.47 volume create --backup &lt;backup_name&gt;</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Replace <code>&lt;backup_name&gt;</code> with the name of your new backup location.</p>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
You do not boot a Compute service (nova) instance by using the new <code>volume from</code> image or try to detach the previous volume because the Compute service and the Block Storage service are still not connected.
</td>
</tr>
</table>
</div>
</li>
</ul>
</div>
</li>
</ol>
</div>
</li>
</ol>
</div>
</div>
<div class="sect2">
<h3 id="adopting-the-openstack-dashboard_adopt-control-plane">Adopting the Dashboard service</h3>
<div class="paragraph">
<p>To adopt the Dashboard service (horizon), you patch an existing <code>OpenStackControlPlane</code> custom resource (CR) that has the Dashboard service disabled. The patch starts the service with the configuration parameters that are provided by the Red&#160;Hat OpenStack Platform environment.</p>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>You adopted Memcached. For more information, see <a href="#deploying-backend-services_migrating-databases">Deploying back-end services</a>.</p>
</li>
<li>
<p>You adopted the Identity service (keystone). For more information, see <a href="#adopting-the-identity-service_adopt-control-plane">Adopting the Identity service</a>.</p>
</li>
</ul>
</div>
<div class="ulist">
<div class="title">Procedure</div>
<ul>
<li>
<p>Patch the <code>OpenStackControlPlane</code> CR to deploy the Dashboard service:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc patch openstackcontrolplane openstack --type=merge --patch '
spec:
  horizon:
    enabled: true
    apiOverride:
      route: {}
    template:
      memcachedInstance: memcached
      secret: osp-secret
'</pre>
</div>
</div>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Verification</div>
<ol class="arabic">
<li>
<p>Verify that the Dashboard service instance is successfully deployed and ready:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc get horizon</pre>
</div>
</div>
</li>
<li>
<p>Confirm that the Dashboard service is reachable and returns a <code>200</code> status code:</p>
<div class="listingblock">
<div class="content">
<pre>PUBLIC_URL=$(oc get horizon horizon -o jsonpath='{.status.endpoint}')
curl --silent --output /dev/stderr --head --write-out "%{http_code}" "$PUBLIC_URL/dashboard/auth/login/?next=/dashboard/" -k | grep 200</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
<div class="sect2">
<h3 id="adopting-the-shared-file-systems-service_adopt-control-plane">Adopting the Shared File Systems service</h3>
<div class="paragraph">
<p>The Shared File Systems service (manila) in Red&#160;Hat OpenStack Services on OpenShift (RHOSO) provides a self-service API to create and manage file shares. File shares (or "shares"), are built for concurrent read/write access from multiple clients. This makes the Shared File Systems service essential in cloud environments that require a ReadWriteMany persistent storage.</p>
</div>
<div class="paragraph">
<p>File shares in RHOSO require network access. Ensure that the networking in the Red&#160;Hat OpenStack Platform (RHOSP) 17.1 environment matches the network plans for your new cloud after adoption. This ensures that tenant workloads remain connected to storage during the adoption process. The Shared File Systems service control plane services are not in the data path. Shutting down the API, scheduler, and share manager services do not impact access to existing shared file systems.</p>
</div>
<div class="paragraph">
<p>Typically, storage and storage device management are separate networks. Shared File Systems services only need access to the storage device management network.
For example, if you used a Red Hat Ceph Storage cluster in the deployment, the "storage"
network refers to the Red Hat Ceph Storage cluster&#8217;s public network, and the Shared File Systems service&#8217;s share manager service needs to be able to reach it.</p>
</div>
<div class="paragraph">
<p>The Shared File Systems service supports the following storage networking scenarios:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>You can directly control the networking for your respective file shares.</p>
</li>
<li>
<p>The RHOSO administrator configures the storage networking.</p>
</li>
</ul>
</div>
<div class="sect3">
<h4 id="preparing-the-shared-file-systems-service-configuration_adopting-shared-file-systems">Guidelines for preparing the Shared File Systems service configuration</h4>
<div class="paragraph">
<p>To deploy Shared File Systems service (manila) on the control plane, you must copy the original configuration file from the Red&#160;Hat OpenStack Platform 17.1 deployment. You must review the content in the file to make sure you are adopting the correct configuration for Red&#160;Hat OpenStack Services on OpenShift (RHOSO) 18.0. Not all of the content needs to be brought into the new cloud environment.</p>
</div>
<div class="paragraph">
<p>Review the following guidelines for preparing your Shared File Systems service configuration file for adoption:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The Shared File Systems service operator sets up the following configurations and can be ignored:</p>
<div class="ulist">
<ul>
<li>
<p>Database-related configuration (<code>[database]</code>)</p>
</li>
<li>
<p>Service authentication (<code>auth_strategy</code>, <code>[keystone_authtoken]</code>)</p>
</li>
<li>
<p>Message bus configuration (<code>transport_url</code>, <code>control_exchange</code>)</p>
</li>
<li>
<p>The default paste config (<code>api_paste_config</code>)</p>
</li>
<li>
<p>Inter-service communication configuration (<code>[neutron]</code>, <code>[nova]</code>, <code>[cinder]</code>, <code>[glance]</code> <code>[oslo_messaging_*]</code>)</p>
</li>
</ul>
</div>
</li>
<li>
<p>Ignore the <code>osapi_share_listen</code> configuration. In Red&#160;Hat OpenStack Services on OpenShift (RHOSO) 18.0, you rely on Red Hat OpenShift Container Platform (RHOCP) routes and ingress.</p>
</li>
<li>
<p>Check for policy overrides. In RHOSO 18.0, the Shared File Systems service ships with a secure default Role-based access control (RBAC), and overrides might not be necessary.</p>
</li>
<li>
<p>If a custom policy is necessary, you must provide it as a <code>ConfigMap</code>. The following example spec illustrates how you can set up a <code>ConfigMap</code> called <code>manila-policy</code> with the contents of a file called <code>policy.yaml</code>:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">  spec:
    manila:
      enabled: true
      template:
        manilaAPI:
          customServiceConfig: |
             [oslo_policy]
             policy_file=/etc/manila/policy.yaml
        extraMounts:
        - extraVol:
          - extraVolType: Undefined
            mounts:
            - mountPath: /etc/manila/
              name: policy
              readOnly: true
            propagation:
            - ManilaAPI
            volumes:
            - name: policy
              projected:
                sources:
                - configMap:
                    name: manila-policy
                    items:
                      - key: policy
                        path: policy.yaml</code></pre>
</div>
</div>
</li>
<li>
<p>The value of the <code>host</code> option under the <code>[DEFAULT]</code> section must be <code>hostgroup</code>.</p>
</li>
<li>
<p>To run the Shared File Systems service API service, you must add the <code>enabled_share_protocols</code> option to the <code>customServiceConfig</code> section in <code>manila: template: manilaAPI</code>.</p>
</li>
<li>
<p>If you have scheduler overrides, add them to the <code>customServiceConfig</code>
section in <code>manila: template: manilaScheduler</code>.</p>
</li>
<li>
<p>If you have multiple storage back-end drivers configured with RHOSP 17.1, you need to split them up when deploying RHOSO 18.0. Each storage back-end driver needs to use its own instance of the <code>manila-share</code> service.</p>
</li>
<li>
<p>If a storage back-end driver needs a custom container image, find it in the
<a href="https://catalog.redhat.com/software/containers/search?gs&amp;q=manila">Red Hat Ecosystem Catalog</a>, and create or modify an <code>OpenStackVersion</code> custom resource (CR) to specify the custom image using the same <code>custom name</code>.</p>
<div class="paragraph">
<p>The following example shows a manila spec from the <code>OpenStackControlPlane</code> CR that includes multiple storage back-end drivers, where only one is using a custom container image:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">  spec:
    manila:
      enabled: true
      template:
        manilaAPI:
          customServiceConfig: |
            [DEFAULT]
            enabled_share_protocols = nfs
          replicas: 3
        manilaScheduler:
          replicas: 3
        manilaShares:
         netapp:
           customServiceConfig: |
             [DEFAULT]
             debug = true
             enabled_share_backends = netapp
             host = hostgroup
             [netapp]
             driver_handles_share_servers = False
             share_backend_name = netapp
             share_driver = manila.share.drivers.netapp.common.NetAppDriver
             netapp_storage_family = ontap_cluster
             netapp_transport_type = http
           replicas: 1
         pure:
            customServiceConfig: |
             [DEFAULT]
             debug = true
             enabled_share_backends=pure-1
             host = hostgroup
             [pure-1]
             driver_handles_share_servers = False
             share_backend_name = pure-1
             share_driver = manila.share.drivers.purestorage.flashblade.FlashBladeShareDriver
             flashblade_mgmt_vip = 203.0.113.15
             flashblade_data_vip = 203.0.10.14
            replicas: 1</code></pre>
</div>
</div>
<div class="paragraph">
<p>The following example shows the <code>OpenStackVersion</code> CR that defines the custom container image:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: core.openstack.org/v1beta1
kind: OpenStackVersion
metadata:
  name: openstack
spec:
  customContainerImages:
    cinderVolumeImages:
      pure: registry.connect.redhat.com/purestorage/openstack-manila-share-pure-rhosp-18-0</code></pre>
</div>
</div>
<div class="paragraph">
<p>The name of the <code>OpenStackVersion</code> CR must match the name of your <code>OpenStackControlPlane</code> CR.</p>
</div>
</li>
<li>
<p>If you are providing sensitive information, such as passwords, hostnames, and usernames, use RHOCP secrets, and the <code>customServiceConfigSecrets</code> key. You can use <code>customConfigSecrets</code> in any service. If you use third party storage that requires credentials, create a secret that is referenced in the manila CR/patch file by using the <code>customServiceConfigSecrets</code> key. For example:</p>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Create a file that includes the secrets, for example, <code>netapp_secrets.conf</code>:</p>
<div class="listingblock">
<div class="content">
<pre>$ cat &lt;&lt; __EOF__ &gt; ~/netapp_secrets.conf

[netapp]
netapp_server_hostname = 203.0.113.10
netapp_login = fancy_netapp_user
netapp_password = secret_netapp_password
netapp_vserver = mydatavserver
__EOF__</pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre>$ oc create secret generic osp-secret-manila-netapp --from-file=~/&lt;secret&gt;</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Replace <code>&lt;secret&gt;</code> with the name of the file that includes your secrets, for example, <code>netapp_secrets.conf</code>.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Add the secret to any Shared File Systems service file in the <code>customServiceConfigSecrets</code> section. The following example adds the <code>osp-secret-manila-netapp</code> secret to the <code>manilaShares</code> service:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">  spec:
    manila:
      enabled: true
      template:
        &lt; . . . &gt;
        manilaShares:
         netapp:
           customServiceConfig: |
             [DEFAULT]
             debug = true
             enabled_share_backends = netapp
             host = hostgroup
             [netapp]
             driver_handles_share_servers = False
             share_backend_name = netapp
             share_driver = manila.share.drivers.netapp.common.NetAppDriver
             netapp_storage_family = ontap_cluster
             netapp_transport_type = http
           customServiceConfigSecrets:
             - osp-secret-manila-netapp
           replicas: 1
    &lt; . . . &gt;</code></pre>
</div>
</div>
</li>
</ol>
</div>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="deploying-file-systems-service-control-plane_adopting-shared-file-systems">Deploying the Shared File Systems service on the control plane</h4>
<div class="paragraph">
<p>Copy the Shared File Systems service (manila) configuration from the Red&#160;Hat OpenStack Platform (RHOSP) 17.1 deployment, and then deploy the Shared File Systems service on the control plane.</p>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>The Shared File Systems service systemd services such as <code>api</code>, <code>cron</code>, and <code>scheduler</code> are stopped. For more information, see <a href="#stopping-openstack-services_migrating-databases">Stopping Red&#160;Hat OpenStack Platform services</a>.</p>
</li>
<li>
<p>If the deployment uses CephFS through NFS as a storage back end, the Pacemaker ordering and collocation constraints are adjusted. For more information, see <a href="#stopping-openstack-services_migrating-databases">Stopping Red&#160;Hat OpenStack Platform services</a>.</p>
</li>
<li>
<p>The Shared File Systems service Pacemaker service (<code>openstack-manila-share</code>) is stopped. For more information, see <a href="#stopping-openstack-services_migrating-databases">Stopping Red&#160;Hat OpenStack Platform services</a>.</p>
</li>
<li>
<p>The database migration is complete. For more information, see <a href="#migrating-databases-to-mariadb-instances_migrating-databases">Migrating databases to MariaDB instances</a>.</p>
</li>
<li>
<p>The Red Hat OpenShift Container Platform (RHOCP) nodes where the <code>manila-share</code> service is to be deployed can reach the management network that the storage system is in.</p>
</li>
<li>
<p>If the deployment uses CephFS through NFS as a storage back end, a new clustered Ceph NFS service is deployed on the Red Hat Ceph Storage cluster with the help
of Ceph orchestrator. For more information, see <a href="#creating-a-ceph-nfs-cluster_ceph-prerequisites">Creating a Ceph NFS cluster</a>.</p>
</li>
<li>
<p>Services such as the Identity service (keystone) and memcached are available prior to adopting the Shared File Systems services.</p>
</li>
<li>
<p>If you enabled tenant-driven networking by setting <code>driver_handles_share_servers=True</code>, the Networking service (neutron) is deployed.</p>
</li>
<li>
<p>The <code>CONTROLLER1_SSH</code> environment variable is defined and points to the RHOSP Controller node. Replace the following example values with values that are correct for your environment:</p>
<div class="listingblock">
<div class="content">
<pre>$ CONTROLLER1_SSH="ssh -i &lt;path to SSH key&gt; root@&lt;node IP&gt;"</pre>
</div>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Copy the configuration file from RHOSP 17.1 for reference:</p>
<div class="listingblock">
<div class="content">
<pre>$ CONTROLLER1_SSH cat /var/lib/config-data/puppet-generated/manila/etc/manila/manila.conf | awk '!/^ *#/ &amp;&amp; NF' &gt; ~/manila.conf</pre>
</div>
</div>
</li>
<li>
<p>Review the configuration file for configuration changes that were made since RHOSP 17.1. For more information on preparing this file for Red&#160;Hat OpenStack Services on OpenShift (RHOSO), see <a href="#preparing-the-shared-file-systems-service-configuration_adopting-shared-file-systems">Guidelines for preparing the Shared File Systems service configuration</a>.</p>
</li>
<li>
<p>Create a patch file for the <code>OpenStackControlPlane</code> CR to deploy the Shared File Systems service. The following example <code>manila.patch</code> file uses native CephFS:</p>
<div class="listingblock">
<div class="content">
<pre>$ cat &lt;&lt; __EOF__ &gt; ~/manila.patch
spec:
  manila:
    enabled: true
    apiOverride:
      route: {}
    template:
      databaseInstance: openstack
      databaseAccount: manila
      secret: osp-secret
      manilaAPI:
        replicas: 3
        customServiceConfig: |
          [DEFAULT]
          enabled_share_protocols = cephfs
        override:
          service:
            internal:
              metadata:
                annotations:
                  metallb.universe.tf/address-pool: internalapi
                  metallb.universe.tf/allow-shared-ip: internalapi
                  metallb.universe.tf/loadBalancerIPs: 172.17.0.80 <i class="conum" data-value="1"></i><b>(1)</b>
              spec:
                type: LoadBalancer
      manilaScheduler:
        replicas: 3
      manilaShares:
        cephfs:
          replicas: 1
          customServiceConfig: |
            [DEFAULT]
            enabled_share_backends = tripleo_ceph
            host = hostgroup
            [cephfs]
            driver_handles_share_servers=False
            share_backend_name=cephfs <i class="conum" data-value="2"></i><b>(2)</b>
            share_driver=manila.share.drivers.cephfs.driver.CephFSDriver
            cephfs_conf_path=/etc/ceph/ceph.conf
            cephfs_auth_id=openstack
            cephfs_cluster_name=ceph
            cephfs_volume_mode=0755
            cephfs_protocol_helper_type=CEPHFS
          networkAttachments: <i class="conum" data-value="3"></i><b>(3)</b>
              - storage
      extraMounts: <i class="conum" data-value="4"></i><b>(4)</b>
      - name: v1
        region: r1
        extraVol:
          - propagation:
            - ManilaShare
          extraVolType: Ceph
          volumes:
          - name: ceph
            secret:
              secretName: ceph-conf-files
          mounts:
          - name: ceph
            mountPath: "/etc/ceph"
            readOnly: true
__EOF__</pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>If you use IPv6, change the load balancer IP to the load balancer IP in your environment, for example, <code>metallb.universe.tf/loadBalancerIPs: fd00:bbbb::80</code>.</td>
</tr>
<tr>
<td><i class="conum" data-value="2"></i><b>2</b></td>
<td>Ensure that the names of the back ends (<code>share_backend_name</code>) are the same as they were in RHOSP 17.1.</td>
</tr>
<tr>
<td><i class="conum" data-value="3"></i><b>3</b></td>
<td>Ensure that you specify the appropriate storage management network in the <code>networkAttachments</code> section. For example, the <code>manilaShares</code> instance with the CephFS back-end driver is connected to the <code>storage</code> network.</td>
</tr>
<tr>
<td><i class="conum" data-value="4"></i><b>4</b></td>
<td>If you need to add extra files to any of the services, you can use <code>extraMounts</code>. For example, when using Red Hat Ceph Storage, you can add the Shared File Systems service Ceph user&#8217;s keyring file as well as the <code>ceph.conf</code> configuration file.
<div class="paragraph">
<p>The following example patch file uses CephFS through NFS:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>$ cat &lt;&lt; __EOF__ &gt; ~/manila.patch
spec:
  manila:
    enabled: true
    apiOverride:
      route: {}
    template:
      databaseInstance: openstack
      secret: osp-secret
      manilaAPI:
        replicas: 3
        customServiceConfig: |
          [DEFAULT]
          enabled_share_protocols = cephfs
        override:
          service:
            internal:
              metadata:
                annotations:
                  metallb.universe.tf/address-pool: internalapi
                  metallb.universe.tf/allow-shared-ip: internalapi
                  metallb.universe.tf/loadBalancerIPs: 172.17.0.80
              spec:
                type: LoadBalancer
      manilaScheduler:
        replicas: 3
      manilaShares:
        cephfs:
          replicas: 1
          customServiceConfig: |
            [DEFAULT]
            enabled_share_backends = cephfs
            host = hostgroup
            [cephfs]
            driver_handles_share_servers=False
            share_backend_name=tripleo_ceph
            share_driver=manila.share.drivers.cephfs.driver.CephFSDriver
            cephfs_conf_path=/etc/ceph/ceph.conf
            cephfs_auth_id=openstack
            cephfs_cluster_name=ceph
            cephfs_protocol_helper_type=NFS
            cephfs_nfs_cluster_id=cephfs
            cephfs_ganesha_server_ip=172.17.5.47
          networkAttachments:
              - storage
__EOF__</pre>
</div>
</div></td>
</tr>
</table>
</div>
</li>
</ol>
</div>
</li>
<li>
<p>Prior to adopting the <code>manilaShares</code> service for CephFS through NFS, ensure that you create a clustered Ceph NFS service. The name of the service must be <code>cephfs_nfs_cluster_id</code>. The <code>cephfs_nfs_cluster_id</code> option is set with the name of the NFS cluster created on Red Hat Ceph Storage.</p>
</li>
<li>
<p>The <code>cephfs_ganesha_server_ip</code> option is preserved from the configuration on the RHOSP 17.1 environment.</p>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Patch the <code>OpenStackControlPlane</code> CR:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc patch openstackcontrolplane openstack --type=merge --patch-file=~/&lt;manila.patch&gt;</pre>
</div>
</div>
</li>
</ol>
</div>
</li>
<li>
<p>Replace <code>&lt;manila.patch&gt;</code> with the name of your patch file.</p>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Verification</div>
<ol class="arabic">
<li>
<p>Inspect the resulting Shared File Systems service pods:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc get pods -l service=manila</pre>
</div>
</div>
</li>
<li>
<p>Check that the Shared File Systems API service is registered in the Identity service (keystone):</p>
<div class="listingblock">
<div class="content">
<pre>$ openstack service list | grep manila</pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre>$ openstack endpoint list | grep manila

| 1164c70045d34b959e889846f9959c0e | regionOne | manila       | share        | True    | internal  | http://manila-internal.openstack.svc:8786/v1/%(project_id)s        |
| 63e89296522d4b28a9af56586641590c | regionOne | manilav2     | sharev2      | True    | public    | https://manila-public-openstack.apps-crc.testing/v2                |
| af36c57adcdf4d50b10f484b616764cc | regionOne | manila       | share        | True    | public    | https://manila-public-openstack.apps-crc.testing/v1/%(project_id)s |
| d655b4390d7544a29ce4ea356cc2b547 | regionOne | manilav2     | sharev2      | True    | internal  | http://manila-internal.openstack.svc:8786/v2                       |</pre>
</div>
</div>
</li>
<li>
<p>Test the health of the service:</p>
<div class="listingblock">
<div class="content">
<pre>$ openstack share service list
$ openstack share pool list --detail</pre>
</div>
</div>
</li>
<li>
<p>Check existing workloads:</p>
<div class="listingblock">
<div class="content">
<pre>$ openstack share list
$ openstack share snapshot list</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="decommissioning-RHOSP-standalone-Ceph-NFS-service_adopting-shared-file-systems">Decommissioning the Red&#160;Hat OpenStack Platform standalone Ceph NFS service</h4>
<div class="paragraph">
<p>If your deployment uses CephFS through NFS, you must decommission the Red&#160;Hat OpenStack Platform(RHOSP) standalone NFS service. Since future software upgrades do not support the previous NFS service, ensure that the decommissioning period is short.</p>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>You identified the new export locations for your existing shares by querying the Shared File Systems API.</p>
</li>
<li>
<p>You unmounted and remounted the shared file systems on each client to stop using the previous NFS server.</p>
</li>
<li>
<p>If you are consuming the Shared File Systems service shares with the Shared File Systems service CSI plugin for Red Hat OpenShift Container Platform (RHOCP), you migrated the shares by scaling down the application pods and scaling them back up.</p>
</li>
</ul>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Clients that are creating new workloads cannot use share exports through the previous NFS service. The Shared File Systems service no longer communicates with the previous NFS service, and cannot apply or alter export rules on the previous NFS service.
</td>
</tr>
</table>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Remove the <code>cephfs_ganesha_server_ip</code> option from the <code>manila-share</code> service configuration:</p>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
This restarts the <code>manila-share</code> process and removes the export locations that applied to the previous NFS service from all the shares.
</td>
</tr>
</table>
</div>
<div class="listingblock">
<div class="content">
<pre>$ cat &lt;&lt; __EOF__ &gt; ~/manila.patch
spec:
  manila:
    enabled: true
    apiOverride:
      route: {}
    template:
      manilaShares:
        cephfs:
          replicas: 1
          customServiceConfig: |
            [DEFAULT]
            enabled_share_backends = cephfs
            host = hostgroup
            [cephfs]
            driver_handles_share_servers=False
            share_backend_name=cephfs
            share_driver=manila.share.drivers.cephfs.driver.CephFSDriver
            cephfs_conf_path=/etc/ceph/ceph.conf
            cephfs_auth_id=openstack
            cephfs_cluster_name=ceph
            cephfs_protocol_helper_type=NFS
            cephfs_nfs_cluster_id=cephfs
          networkAttachments:
              - storage
__EOF__</pre>
</div>
</div>
</li>
<li>
<p>Patch the <code>OpenStackControlPlane</code> custom resource:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc patch openstackcontrolplane openstack --type=merge --patch-file=~/&lt;manila.patch&gt;</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Replace <code>&lt;manila.patch&gt;</code> with the name of your patch file.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Clean up the standalone <code>ceph-nfs</code> service from the RHOSP control plane nodes by disabling and deleting the Pacemaker resources associated with the service:</p>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
You can defer this step until after RHOSO 18.0 is operational. During this time, you cannot decommission the Controller nodes.
</td>
</tr>
</table>
</div>
<div class="listingblock">
<div class="content">
<pre>$ sudo pcs resource disable ceph-nfs
$ sudo pcs resource disable ip-&lt;VIP&gt;
$ sudo pcs resource unmanage ceph-nfs
$ sudo pcs resource unmanage ip-&lt;VIP&gt;</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Replace <code>&lt;VIP&gt;</code> with the IP address assigned to the <code>ceph-nfs</code> service in your environment.</p>
</li>
</ul>
</div>
</li>
</ol>
</div>
</div>
</div>
<div class="sect2">
<h3 id="adopting-the-orchestration-service_adopt-control-plane">Adopting the Orchestration service</h3>
<div class="paragraph">
<p>To adopt the Orchestration service (heat), you patch an existing <code>OpenStackControlPlane</code> custom resource (CR), where the Orchestration service
is disabled. The patch starts the service with the configuration parameters that are provided by the Red&#160;Hat OpenStack Platform (RHOSP) environment.</p>
</div>
<div class="paragraph">
<p>After you complete the adoption process, you have CRs for <code>Heat</code>, <code>HeatAPI</code>, <code>HeatEngine</code>, and <code>HeatCFNAPI</code>, and endpoints within the Identity service (keystone) to facilitate these services.</p>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>The source director environment is running.</p>
</li>
<li>
<p>The target Red Hat OpenShift Container Platform (RHOCP) environment is running.</p>
</li>
<li>
<p>You adopted MariaDB and the Identity service.</p>
</li>
<li>
<p>If your existing Orchestration service stacks contain resources from other services such as Networking service (neutron), Compute service (nova), Object Storage service (swift), and so on, adopt those sevices before adopting the Orchestration service.</p>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Retrieve the existing <code>auth_encryption_key</code> and <code>service</code> passwords. You use these passwords to patch the <code>osp-secret</code>. In the following example, the <code>auth_encryption_key</code> is used as <code>HeatAuthEncryptionKey</code> and the <code>service</code> password is used as <code>HeatPassword</code>:</p>
<div class="listingblock">
<div class="content">
<pre>[stack@rhosp17 ~]$ grep -E 'HeatPassword|HeatAuth|HeatStackDomainAdmin' ~/overcloud-deploy/overcloud/overcloud-passwords.yaml
  HeatAuthEncryptionKey: Q60Hj8PqbrDNu2dDCbyIQE2dibpQUPg2
  HeatPassword: dU2N0Vr2bdelYH7eQonAwPfI3
  HeatStackDomainAdminPassword: dU2N0Vr2bdelYH7eQonAwPfI3</pre>
</div>
</div>
</li>
<li>
<p>Log in to a Controller node and verify the <code>auth_encryption_key</code> value in use:</p>
<div class="listingblock">
<div class="content">
<pre>[stack@rhosp17 ~]$ ansible -i overcloud-deploy/overcloud/config-download/overcloud/tripleo-ansible-inventory.yaml overcloud-controller-0 -m shell -a "grep auth_encryption_key /var/lib/config-data/puppet-generated/heat/etc/heat/heat.conf | grep -Ev '^#|^$'" -b
overcloud-controller-0 | CHANGED | rc=0 &gt;&gt;
auth_encryption_key=Q60Hj8PqbrDNu2dDCbyIQE2dibpQUPg2</pre>
</div>
</div>
</li>
<li>
<p>Encode the password to Base64 format:</p>
<div class="listingblock">
<div class="content">
<pre>$ echo Q60Hj8PqbrDNu2dDCbyIQE2dibpQUPg2 | base64
UTYwSGo4UHFickROdTJkRENieUlRRTJkaWJwUVVQZzIK</pre>
</div>
</div>
</li>
<li>
<p>Patch the <code>osp-secret</code> to update the <code>HeatAuthEncryptionKey</code> and <code>HeatPassword</code> parameters. These values must match the values in the director Orchestration service configuration:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc patch secret osp-secret --type='json' -p='[{"op" : "replace" ,"path" : "/data/HeatAuthEncryptionKey" ,"value" : "UTYwSGo4UHFickROdTJkRENieUlRRTJkaWJwUVVQZzIK"}]'
secret/osp-secret patched</pre>
</div>
</div>
</li>
<li>
<p>Patch the <code>OpenStackControlPlane</code> CR to deploy the Orchestration service:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc patch openstackcontrolplane openstack --type=merge --patch '
spec:
  heat:
    enabled: true
    apiOverride:
      route: {}
    template:
      databaseInstance: openstack
      databaseAccount: heat
      secret: osp-secret
      memcachedInstance: memcached
      passwordSelectors:
        authEncryptionKey: HeatAuthEncryptionKey
        service: HeatPassword
        stackDomainAdminPassword: HeatStackDomainAdminPassword
'</pre>
</div>
</div>
</li>
</ol>
</div>
<div class="olist arabic">
<div class="title">Verification</div>
<ol class="arabic">
<li>
<p>Ensure that the statuses of all the CRs are <code>Setup complete</code>:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc get Heat,HeatAPI,HeatEngine,HeatCFNAPI
NAME                           STATUS   MESSAGE
heat.heat.openstack.org/heat   True     Setup complete

NAME                                  STATUS   MESSAGE
heatapi.heat.openstack.org/heat-api   True     Setup complete

NAME                                        STATUS   MESSAGE
heatengine.heat.openstack.org/heat-engine   True     Setup complete

NAME                                        STATUS   MESSAGE
heatcfnapi.heat.openstack.org/heat-cfnapi   True     Setup complete</pre>
</div>
</div>
</li>
<li>
<p>Check that the Orchestration service is registered in the Identity service:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc exec -it openstackclient -- openstack service list -c Name -c Type
+------------+----------------+
| Name       | Type           |
+------------+----------------+
| heat       | orchestration  |
| glance     | image          |
| heat-cfn   | cloudformation |
| ceilometer | Ceilometer     |
| keystone   | identity       |
| placement  | placement      |
| cinderv3   | volumev3       |
| nova       | compute        |
| neutron    | network        |
+------------+----------------+</pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre>$ oc exec -it openstackclient -- openstack endpoint list --service=heat -f yaml
- Enabled: true
  ID: 1da7df5b25b94d1cae85e3ad736b25a5
  Interface: public
  Region: regionOne
  Service Name: heat
  Service Type: orchestration
  URL: http://heat-api-public-openstack-operators.apps.okd.bne-shift.net/v1/%(tenant_id)s
- Enabled: true
  ID: 414dd03d8e9d462988113ea0e3a330b0
  Interface: internal
  Region: regionOne
  Service Name: heat
  Service Type: orchestration
  URL: http://heat-api-internal.openstack-operators.svc:8004/v1/%(tenant_id)s</pre>
</div>
</div>
</li>
<li>
<p>Check that the Orchestration service engine services are running:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc exec -it openstackclient -- openstack orchestration service list -f yaml
- Binary: heat-engine
  Engine ID: b16ad899-815a-4b0c-9f2e-e6d9c74aa200
  Host: heat-engine-6d47856868-p7pzz
  Hostname: heat-engine-6d47856868-p7pzz
  Status: up
  Topic: engine
  Updated At: '2023-10-11T21:48:01.000000'
- Binary: heat-engine
  Engine ID: 887ed392-0799-4310-b95c-ac2d3e6f965f
  Host: heat-engine-6d47856868-p7pzz
  Hostname: heat-engine-6d47856868-p7pzz
  Status: up
  Topic: engine
  Updated At: '2023-10-11T21:48:00.000000'
- Binary: heat-engine
  Engine ID: 26ed9668-b3f2-48aa-92e8-2862252485ea
  Host: heat-engine-6d47856868-p7pzz
  Hostname: heat-engine-6d47856868-p7pzz
  Status: up
  Topic: engine
  Updated At: '2023-10-11T21:48:00.000000'
- Binary: heat-engine
  Engine ID: 1011943b-9fea-4f53-b543-d841297245fd
  Host: heat-engine-6d47856868-p7pzz
  Hostname: heat-engine-6d47856868-p7pzz
  Status: up
  Topic: engine
  Updated At: '2023-10-11T21:48:01.000000'</pre>
</div>
</div>
</li>
<li>
<p>Verify that you can see your Orchestration service stacks:</p>
<div class="listingblock">
<div class="content">
<pre>$ openstack stack list -f yaml
- Creation Time: '2023-10-11T22:03:20Z'
  ID: 20f95925-7443-49cb-9561-a1ab736749ba
  Project: 4eacd0d1cab04427bc315805c28e66c9
  Stack Name: test-networks
  Stack Status: CREATE_COMPLETE
  Updated Time: null</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
<div class="sect2">
<h3 id="adopting-the-loadbalancer-service_adopt-control-plane">Adopting the Load-balancing service</h3>
<div class="paragraph">
<p>To adopt the Load-balancing service (octavia), you patch an existing <code>OpenStackControlPlane</code> custom resource (CR) where the Load-balancing service is disabled. The patch starts the service with the configuration parameters that are provided by the Red&#160;Hat OpenStack Platform (RHOSP) environment. Existing load balancers fail over after the adoption process to upgrade their image, and set up a network connection with the new control plane.</p>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Create an alias for the <code>openstack</code> command:</p>
<div class="listingblock">
<div class="content">
<pre>$ alias openstack="oc exec -t openstackclient -- openstack"</pre>
</div>
</div>
</li>
<li>
<p>Use the <code>CONTROLLER1_SCP</code> shell variable to set the value of the existing
<code>CONTROLLER1_SSH</code> variable:</p>
<div class="listingblock">
<div class="content">
<pre>$ CONTROLLER1_SCP=$(echo "$CONTROLLER1_SSH" | sed 's/^ssh/scp/g')</pre>
</div>
</div>
</li>
<li>
<p>Run the following set of commands to regenerate the keys and certificates and install the data in Red Hat OpenShift Container Platform (RHOCP). Convert the existing single certificate authority (CA) configuration into a dual CA configuration. You can use these commands to regenerate the keys and certificates and insert them into RHOCP:</p>
<div class="listingblock">
<div class="content">
<pre>SERVER_CA_PASSPHRASE=$($CONTROLLER1_SSH grep ^ca_private_key_passphrase /var/lib/config-data/puppet-generated/octavia/etc/octavia/octavia.conf)
export SERVER_CA_PASSPHRASE=$(echo "${SERVER_CA_PASSPHRASE}"  | cut -d '=' -f 2 | xargs)
export CLIENT_PASSPHRASE="ThisIsOnlyAppliedTemporarily"
CERT_SUBJECT="/C=US/ST=Denial/L=Springfield/O=Dis/CN=www.example.com"
CERT_MIGRATE_PATH="$HOME/octavia_cert_migration"

mkdir -p ${CERT_MIGRATE_PATH}
cd ${CERT_MIGRATE_PATH}
# Set up the server CA
mkdir -p server_ca
cd server_ca
mkdir -p certs crl newcerts private csr
chmod 700 private
${CONTROLLER1_SCP}:/var/lib/config-data/puppet-generated/octavia/etc/octavia/certs/private/cakey.pem private/server_ca.key.pem
chmod 400 private/server_ca.key.pem
${CONTROLLER1_SCP}:{{ octavia_ssh_path }}/client-.pem certs/old_client_cert.pem
${CONTROLLER1_SCP}:{{ octavia_ssh_path }}/index.txt* ./
${CONTROLLER1_SCP}:{{ octavia_ssh_path }}/serial* ./
${CONTROLLER1_SCP}:{{ octavia_ssh_path }}/openssl.cnf ../
openssl req -config ../openssl.cnf -key private/server_ca.key.pem -new -passin env:SERVER_CA_PASSPHRASE -x509 -days 18250 -sha256 -extensions v3_ca -out certs/server_ca.cert.pem -subj "/C=US/ST=Denial/L=Springfield/O=Dis/CN=www.example.com"

# Set up the new client CA
sed -i "s|^dir\s\+=\s\+\"{{ octavia_ssh_path }}\"|dir = \"$CERT_MIGRATE_PATH/client_ca\"|" ../openssl.cnf
cd ${CERT_MIGRATE_PATH}
mkdir -p client_ca
cd client_ca
mkdir -p certs crl csr newcerts private
chmod 700 private
touch index.txt
echo 1000 &gt; serial
openssl genrsa -aes256 -out private/ca.key.pem -passout env:SERVER_CA_PASSPHRASE 4096
chmod 400 private/ca.key.pem
openssl req -config ../openssl.cnf -key private/ca.key.pem -new -passin env:SERVER_CA_PASSPHRASE -x509 -days 18250 -sha256 -extensions v3_ca -out certs/client_ca.cert.pem -subj "${CERT_SUBJECT}"

# Create client certificates
cd ${CERT_MIGRATE_PATH}/client_ca
openssl genrsa -aes256 -out private/client.key.pem -passout env:CLIENT_PASSPHRASE 4096
openssl req -config ../openssl.cnf -new -passin env:CLIENT_PASSPHRASE -sha256 -key private/client.key.pem -out csr/client.csr.pem -subj "${CERT_SUBJECT}"
mkdir -p ${CERT_MIGRATE_PATH}/client_ca/private ${CERT_MIGRATE_PATH}/client_ca/newcerts ${CERT_MIGRATE_PATH}/private
chmod 700 ${CERT_MIGRATE_PATH}/client_ca/private ${CERT_MIGRATE_PATH}/private

cp ${CERT_MIGRATE_PATH}/client_ca/private/ca.key.pem ${CERT_MIGRATE_PATH}/client_ca/private/cakey.pem
cp ${CERT_MIGRATE_PATH}/client_ca/certs/client_ca.cert.pem $CERT_MIGRATE_PATH/client_ca/ca_01.pem
openssl ca -config ../openssl.cnf -extensions usr_cert -passin env:SERVER_CA_PASSPHRASE -days 1825 -notext -batch -md sha256 -in csr/client.csr.pem -out certs/client.cert.pem
openssl rsa -passin env:CLIENT_PASSPHRASE -in private/client.key.pem -out private/client.cert-and-key.pem
cat certs/client.cert.pem &gt;&gt; private/client.cert-and-key.pem

# Install new data in k8s
oc apply -f - &lt;&lt;EOF
apiVersion: v1
kind: Secret
metadata:
  name: octavia-certs-secret
type: Opaque
data:
  server_ca.key.pem:  $(cat ${CERT_MIGRATE_PATH}/server_ca/private/server_ca.key.pem | base64 -w0)
  server_ca.cert.pem: $(cat ${CERT_MIGRATE_PATH}/server_ca/certs/server_ca.cert.pem | base64 -w0)
  client_ca.cert.pem: $(cat ${CERT_MIGRATE_PATH}/client_ca/certs/client_ca.cert.pem | base64 -w0)
  client.cert-and-key.pem: $(cat ${CERT_MIGRATE_PATH}/client_ca/private/client.cert-and-key.pem | base64 -w0)
EOF

oc apply -f - &lt;&lt;EOF
apiVersion: v1
kind: Secret
metadata:
  name: octavia-ca-passphrase
type: Opaque
data:
  server-ca-passphrase: $(echo $SERVER_CA_PASSPHRASE | base64 -w0)
EOF

rm -rf ${CERT_MIGRATE_PATH}</pre>
</div>
</div>
</li>
<li>
<p>Optional: Copy the existing public SSH key that you can use for connecting to the amphorae and install it into RHOCP:</p>
<div class="listingblock">
<div class="content">
<pre>    ${CONTROLLER1_SCP}:&lt;octavia_ssh_pubkey_path&gt; $HOME/octavia_sshkey.pub

    # Install new data in k8s
    oc apply -f - &lt;&lt;EOF
    apiVersion: v1
    kind: ConfigMap
    metadata:
      name: octavia-ssh-pubkey
    data:
      key: $(cat $HOME/octavia_sshkey.pub)
    EOF

    rm -f $HOME/octavia_sshkey.pub</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Replace <code>&lt;octavia_ssh_pubkey_path&gt;</code> with the path that you configured by using the <code>OctaviaAmphoraSshKeyFile</code> parameter in the previous deployment. The default path is <code>/etc/octavia/ssh/octavia_id_rsa</code>. For example:</p>
<div class="listingblock">
<div class="content">
<pre>parameter_defaults:
    OctaviaAmphoraSshKeyFile: /etc/octavia/ssh/octavia_id_rsa</pre>
</div>
</div>
</li>
</ul>
</div>
</li>
<li>
<p>To isolate the management network, add the network interface for the VLAN base interface:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc get --no-headers nncp | cut -f 1 -d ' ' | grep -v nncp-dns | while read; do

interfaces=$(oc get nncp $REPLY -o jsonpath="{.spec.desiredState.interfaces[*].name}")

(echo $interfaces | grep -w -q "octbr\|enp6s0.24") || \
        oc patch nncp $REPLY --type json --patch '
[{
    "op": "add",
    "path": "/spec/desiredState/interfaces/-",
    "value": {
      "description": "Octavia VLAN host interface",
      "name": "enp6s0.24",
      "state": "up",
      "type": "vlan",
      "vlan": {
        "base-iface": "&lt;enp6s0&gt;", <i class="conum" data-value="1"></i><b>(1)</b>
        "id": 24
        }
    }
},
{
    "op": "add",
    "path": "/spec/desiredState/interfaces/-",
    "value": {
      "description": "Octavia Bridge",
      "mtu": &lt;mtu&gt;, <i class="conum" data-value="2"></i><b>(2)</b>
      "state": "up",
      "type": "linux-bridge",
      "name": "octbr",
      "bridge": {
        "options": { "stp": { "enabled": "false" } },
        "port": [ { "name": "enp6s0.24" } ]
        }
    }
}]'

done</pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>Replace <code>&lt;enp6s0&gt;</code> with the name of the network interface in your RHOCP setup.</td>
</tr>
<tr>
<td><i class="conum" data-value="2"></i><b>2</b></td>
<td>Replace <code>&lt;mtu&gt;</code> with the <code>mtu</code> value in your environment.</td>
</tr>
</table>
</div>
</li>
<li>
<p>To connect pods that manage load balancer virtual machines (amphorae) and the OpenvSwitch pods that are managed by the OVN operator, configure the Load-balancing service network attachment definition:</p>
<div class="listingblock">
<div class="content">
<pre>$ cat &gt;&gt; octavia-nad.yaml &lt;&lt; EOF_CAT
apiVersion: k8s.cni.cncf.io/v1
kind: NetworkAttachmentDefinition
metadata:
  labels:
    osp/net: octavia
  name: octavia
spec:
  config: |
    {
      "cniVersion": "0.3.1",
      "name": "octavia",
      "type": "bridge",
      "bridge": "octbr",
      "ipam": {
        "type": "whereabouts",
        "range": "172.23.0.0/24",
        "range_start": "172.23.0.30",
        "range_end": "172.23.0.70",
        "routes": [
           {
             "dst": "172.24.0.0/16",
             "gw" : "172.23.0.150"
           }
         ]
      }
    }
EOF_CAT
$ oc apply -f octavia-nad.yaml</pre>
</div>
</div>
</li>
<li>
<p>Enable the Load-balancing service in RHOCP:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc patch openstackcontrolplane openstack --type=merge --patch '
spec:
  ovn:
    template:
      ovnController:
        networkAttachment: tenant
        nicMappings:
          octavia: octbr
  octavia:
    enabled: true
    template:
      apacheContainerImage: registry.redhat.io/rhel8/httpd-24:latest
      amphoraImageContainerImage: quay.io/gthiemonge/octavia-amphora-image
      databaseInstance: openstack
      octaviaHousekeeping:
        networkAttachments:
          - octavia
      octaviaHealthManager:
        networkAttachments:
          - octavia
      octaviaWorker:
        networkAttachments:
          - octavia
'</pre>
</div>
</div>
</li>
<li>
<p>Wait for the Load-balancing service control plane services CRs to be ready:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc wait --for condition=Ready --timeout=600s octavia.octavia.openstack.org/octavia</pre>
</div>
</div>
</li>
<li>
<p>Ensure that the Load-balancing service is registered in the Identity service:</p>
<div class="listingblock">
<div class="content">
<pre>$ openstack service list | grep load-balancer
| bd078ca6f90c4b86a48801f45eb6f0d7 | octavia   | load-balancer |
$ openstack endpoint list | grep load-balancer
| f1ae7756b6164baf9cb82a1a670067a2 | regionOne | octavia      | load-balancer | True    | public    | https://octavia-public-openstack.apps-crc.testing                     |
| ff3222b4621843669e89843395213049 | regionOne | octavia      | load-balancer | True    | internal  | http://octavia-internal.openstack.svc:9876                            |</pre>
</div>
</div>
</li>
<li>
<p>Change the <code>ONBOOT</code> option in the network script for the management interface to <code>no</code> to ensure that the interface is disabled on reboot:</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>Connect to each of the Controller nodes on the old control plane, for example, <code>overcloud-controller-0</code>.</p>
</li>
<li>
<p>Open the management interface configuration file in a text editor such as <code>vi</code>:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo vi /etc/sysconfig/network-scripts/ifcfg-o-hm0</pre>
</div>
</div>
<div class="paragraph">
<p>The script name is based on the previous <code>OctaviaMgmtPortDevName</code> setting and might differ in your environment.</p>
</div>
</li>
</ol>
</div>
</li>
<li>
<p>Delete old flavors that are migrated to the new control plane:</p>
<div class="listingblock">
<div class="content">
<pre>$ openstack flavor list --all | grep octavia
| 484c351a-57ca-4a4b-8e6e-93d31596fec5 | octavia-amphora-4vcpus   | 4096 |    3 |         0 |     4 | False     |
| 65                                   | octavia_65               | 1024 |    3 |         0 |     1 | False     |
| amphora-mvcpu-ha                     | octavia_amphora-mvcpu-ha | 4096 |    3 |         0 |     4 | False     |
| cf9d1d80-5680-4ed8-a051-e8ec4c5871e0 | octavia-amphora          | 1024 |    3 |         0 |     1 | False     |
$ openstack flavor delete octavia_65
$ openstack flavor delete octavia_amphora-mvcpu-ha
$ openstack flavor list --all | grep octavia
| 484c351a-57ca-4a4b-8e6e-93d31596fec5 | octavia-amphora-4vcpus | 4096 |    3 |         0 |     4 | False     |
| cf9d1d80-5680-4ed8-a051-e8ec4c5871e0 | octavia-amphora        | 1024 |    3 |         0 |     1 | False     |</pre>
</div>
</div>
</li>
<li>
<p>Delete old Load-balancing service flavors that are migrated to the new control plane:</p>
<div class="listingblock">
<div class="content">
<pre>$ openstack loadbalancer flavor list
+--------------------------------------+--------------------------+--------------------------------------+---------+
| id                                   | name                     | flavor_profile_id                    | enabled |
+--------------------------------------+--------------------------+--------------------------------------+---------+
| 5db54d9b-ba08-4b51-a859-0a81533604aa | octavia_amphora-mvcpu-ha | 4fa6a793-4c20-4480-be4f-806912840511 | True    |
| 6d649fd5-6322-4265-b5f3-c3277fc29ec8 | amphora-4vcpus           | d9764a80-99f5-4f22-bbe0-3ddbdc5c485c | True    |
| 93f34308-24a7-42de-9065-959a3b36e7f6 | amphora                  | e75e50c8-7786-4623-abcf-bccbea59d213 | True    |
+--------------------------------------+--------------------------+--------------------------------------+---------+
$ openstack loadbalancer flavor delete octavia_amphora-mvcpu-ha
$ openstack loadbalancer flavor list
+--------------------------------------+----------------+--------------------------------------+---------+
| id                                   | name           | flavor_profile_id                    | enabled |
+--------------------------------------+----------------+--------------------------------------+---------+
| 6d649fd5-6322-4265-b5f3-c3277fc29ec8 | amphora-4vcpus | d9764a80-99f5-4f22-bbe0-3ddbdc5c485c | True    |
| 93f34308-24a7-42de-9065-959a3b36e7f6 | amphora        | e75e50c8-7786-4623-abcf-bccbea59d213 | True    |
+--------------------------------------+----------------+--------------------------------------+---------+</pre>
</div>
</div>
</li>
<li>
<p>Delete old flavor profiles that are migrated to the new control plane:</p>
<div class="listingblock">
<div class="content">
<pre>$ openstack loadbalancer flavorprofile list
+--------------------------------------+----------------------------------+---------------+
| id                                   | name                             | provider_name |
+--------------------------------------+----------------------------------+---------------+
| 4fa6a793-4c20-4480-be4f-806912840511 | octavia_amphora-mvcpu-ha_profile | amphora       |
| d9764a80-99f5-4f22-bbe0-3ddbdc5c485c | amphora-4vcpus                   | amphora       |
| e75e50c8-7786-4623-abcf-bccbea59d213 | amphora                          | amphora       |
+--------------------------------------+----------------------------------+---------------+
$ openstack loadbalancer flavorprofile delete octavia_amphora-mvcpu-ha_profile
$ openstack loadbalancer flavorprofile list
+--------------------------------------+----------------+---------------+
| id                                   | name           | provider_name |
+--------------------------------------+----------------+---------------+
| d9764a80-99f5-4f22-bbe0-3ddbdc5c485c | amphora-4vcpus | amphora       |
| e75e50c8-7786-4623-abcf-bccbea59d213 | amphora        | amphora       |
+--------------------------------------+----------------+---------------+</pre>
</div>
</div>
</li>
<li>
<p>Delete the old management network ports. Store the network ID of the old management network in the variable <code>WALLABY_LB_MGMT_NET_ID</code> to use later:</p>
<div class="listingblock">
<div class="content">
<pre>$ for net_id in $(openstack network list -f value -c ID --name lb-mgmt-net); do desc=$(openstack network show "$net_id" -f value -c description); [ -z "$desc" ] &amp;&amp; WALLABY_LB_MGMT_NET_ID="$net_id" ; done
$ echo $WALLABY_LB_MGMT_NET_ID
1e21f9c1-7485-4104-a2f3-eed098ab9cad</pre>
</div>
</div>
</li>
<li>
<p>Delete all ports that are used in this network:</p>
<div class="listingblock">
<div class="content">
<pre>$ for id in $(openstack port list --network "$WALLABY_LB_MGMT_NET_ID" -f value -c ID) ; do openstack port delete "$id" ; done</pre>
</div>
</div>
</li>
<li>
<p>Delete the old management network:</p>
<div class="listingblock">
<div class="content">
<pre>$ openstack network delete "$WALLABY_LB_MGMT_NET_ID"</pre>
</div>
</div>
</li>
<li>
<p>Verify that only one <code>lb-mgmt-net</code> and one <code>lb-mgmt-subnet</code> exists:</p>
<div class="listingblock">
<div class="content">
<pre>$ openstack network list | grep lb-mgmt-net
| fe470c29-0482-4809-9996-6d636e3feea3 | lb-mgmt-net          | 6a881091-097d-441c-937b-5a23f4f243b7 |
$ openstack subnet list | grep lb-mgmt-subnet
| 6a881091-097d-441c-937b-5a23f4f243b7 | lb-mgmt-subnet          | fe470c29-0482-4809-9996-6d636e3feea3 | 172.24.0.0/16   |</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
<div class="sect2">
<h3 id="adopting-telemetry-services_adopt-control-plane">Adopting Telemetry services</h3>
<div class="paragraph">
<p>To adopt Telemetry services, you patch an existing <code>OpenStackControlPlane</code> custom resource (CR) that has Telemetry services disabled to start the service with the configuration parameters that are provided by the Red&#160;Hat OpenStack Platform (RHOSP) 17.1 environment.</p>
</div>
<div class="paragraph">
<p>If you adopt Telemetry services, the observability solution that is used in the RHOSP 17.1 environment, Service Telemetry Framework, is removed from the cluster. The new solution is deployed in the Red&#160;Hat OpenStack Services on OpenShift (RHOSO) environment, allowing for metrics, and optionally logs, to be retrieved and stored in the new back ends.</p>
</div>
<div class="paragraph">
<p>You cannot automatically migrate old data because different back ends are used. Metrics and logs are considered short-lived data and are not intended to be migrated to the RHOSO environment. For information about adopting legacy autoscaling stack templates to the RHOSO environment, see <a href="#adopting-autoscaling_adopt-control-plane">Adopting Autoscaling services</a>.</p>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>The director environment is running (the source cloud).</p>
</li>
<li>
<p>The Single Node OpenShift or OpenShift Local is running in the Red Hat OpenShift Container Platform (RHOCP) cluster.</p>
</li>
<li>
<p>Previous adoption steps are completed.</p>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Patch the <code>OpenStackControlPlane</code> CR to deploy <code>cluster-observability-operator</code>:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc create -f - &lt;&lt;EOF
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: cluster-observability-operator
  namespace: openshift-operators
spec:
  channel: stable
  installPlanApproval: Automatic
  name: cluster-observability-operator
  source: redhat-operators
  sourceNamespace: openshift-marketplace
EOF</pre>
</div>
</div>
</li>
<li>
<p>Wait for the installation to succeed:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc wait --for jsonpath="{.status.phase}"=Succeeded csv --namespace=openshift-operators -l operators.coreos.com/cluster-observability-operator.openshift-operators</pre>
</div>
</div>
</li>
<li>
<p>Patch the <code>OpenStackControlPlane</code> CR to deploy Ceilometer services:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc patch openstackcontrolplane openstack --type=merge --patch '
spec:
  telemetry:
    enabled: true
    template:
      ceilometer:
        passwordSelector:
          ceilometerService: CeilometerPassword
        enabled: true
        secret: osp-secret
        serviceUser: ceilometer
'</pre>
</div>
</div>
</li>
<li>
<p>Enable the metrics storage back end:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc patch openstackcontrolplane openstack --type=merge --patch '
spec:
  telemetry:
    template:
      metricStorage:
        enabled: true
        monitoringStack:
          alertingEnabled: true
          scrapeInterval: 30s
          storage:
            strategy: persistent
            retention: 24h
            persistent:
              pvcStorageRequest: 20G
'</pre>
</div>
</div>
</li>
</ol>
</div>
<div class="olist arabic">
<div class="title">Verification</div>
<ol class="arabic">
<li>
<p>Verify that the <code>alertmanager</code> and <code>prometheus</code> pods are available:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc get pods -l alertmanager=metric-storage
NAME                            READY   STATUS    RESTARTS   AGE
alertmanager-metric-storage-0   2/2     Running   0          46s
alertmanager-metric-storage-1   2/2     Running   0          46s

$ oc get pods -l prometheus=metric-storage
NAME                          READY   STATUS    RESTARTS   AGE
prometheus-metric-storage-0   3/3     Running   0          46s</pre>
</div>
</div>
</li>
<li>
<p>Inspect the resulting Ceilometer pods:</p>
<div class="listingblock">
<div class="content">
<pre>CEILOMETETR_POD=`oc get pods -l service=ceilometer | tail -n 1 | cut -f 1 -d' '`
oc exec -t $CEILOMETETR_POD -c ceilometer-central-agent -- cat /etc/ceilometer/ceilometer.conf</pre>
</div>
</div>
</li>
<li>
<p>Inspect enabled pollsters:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc get secret ceilometer-config-data -o jsonpath="{.data['polling\.yaml\.j2']}"  | base64 -d</pre>
</div>
</div>
</li>
<li>
<p>Optional: Override default pollsters according to the requirements of your environment:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc patch openstackcontrolplane controlplane --type=merge --patch '
spec:
  telemetry:
    template:
      ceilometer:
          defaultConfigOverwrite:
            polling.yaml.j2: |
              ---
              sources:
                - name: pollsters
                  interval: 100
                  meters:
                    - volume.*
                    - image.size
          enabled: true
          secret: osp-secret
'</pre>
</div>
</div>
</li>
</ol>
</div>
<div class="olist arabic">
<div class="title">Next steps</div>
<ol class="arabic">
<li>
<p>Optional: Patch the <code>OpenStackControlPlane</code> CR to include <code>logging</code>:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc patch openstackcontrolplane openstack --type=merge --patch '
spec:
  telemetry:
    template:
      logging:
      enabled: false
      ipaddr: 172.17.0.80
      port: 10514
      cloNamespace: openshift-logging
'</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
<div class="sect2">
<h3 id="adopting-autoscaling_adopt-control-plane">Adopting autoscaling services</h3>
<div class="paragraph">
<p>To adopt services that enable autoscaling, you patch an existing <code>OpenStackControlPlane</code> custom resource (CR) where the Alarming services (aodh) are disabled. The patch starts the service with the configuration parameters that are provided by the Red&#160;Hat OpenStack Platform environment.</p>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>The source director environment is running.</p>
</li>
<li>
<p>A Single Node OpenShift or OpenShift Local is running in the Red Hat OpenShift Container Platform (RHOCP) cluster.</p>
</li>
<li>
<p>You have adopted the following services:</p>
<div class="ulist">
<ul>
<li>
<p>MariaDB</p>
</li>
<li>
<p>Identity service (keystone)</p>
</li>
<li>
<p>Orchestration service (heat)</p>
</li>
<li>
<p>Telemetry service</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Patch the <code>OpenStackControlPlane</code> CR to deploy the autoscaling services:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc patch openstackcontrolplane openstack --type=merge --patch '
spec:
  telemetry:
    enabled: true
    template:
      autoscaling:
        enabled: true
        aodh:
          passwordSelector:
            aodhService: AodhPassword
          databaseAccount: aodh
          databaseInstance: openstack
          secret: osp-secret
          serviceUser: aodh
        heatInstance: heat
'</pre>
</div>
</div>
</li>
<li>
<p>Inspect the aodh pods:</p>
<div class="listingblock">
<div class="content">
<pre>$ AODH_POD=`oc get pods -l service=aodh | tail -n 1 | cut -f 1 -d' '`
$ oc exec -t $AODH_POD -c aodh-api -- cat /etc/aodh/aodh.conf</pre>
</div>
</div>
</li>
<li>
<p>Check whether the aodh API service is registered in the Identity service:</p>
<div class="listingblock">
<div class="content">
<pre>$ openstack endpoint list | grep aodh
| d05d120153cd4f9b8310ac396b572926 | regionOne | aodh  | alarming  | True    | internal  | http://aodh-internal.openstack.svc:8042  |
| d6daee0183494d7a9a5faee681c79046 | regionOne | aodh  | alarming  | True    | public    | http://aodh-public.openstack.svc:8042    |</pre>
</div>
</div>
</li>
<li>
<p>Optional: Create aodh alarms with the <code>PrometheusAlarm</code> alarm type:</p>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
You must use the <code>PrometheusAlarm</code> alarm type instead of <code>GnocchiAggregationByResourcesAlarm</code>.
</td>
</tr>
</table>
</div>
<div class="listingblock">
<div class="content">
<pre>$ openstack alarm create --name high_cpu_alarm \
--type prometheus \
--query "(rate(ceilometer_cpu{resource_name=~'cirros'})) * 100" \
--alarm-action 'log://' \
--granularity 15 \
--evaluation-periods 3 \
--comparison-operator gt \
--threshold 7000000000</pre>
</div>
</div>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>Verify that the alarm is enabled:</p>
<div class="listingblock">
<div class="content">
<pre>$ openstack alarm list
+--------------------------------------+------------+------------------+-------------------+----------+
| alarm_id                             | type       | name             | state  | severity | enabled  |
+--------------------------------------+------------+------------------+-------------------+----------+
| 209dc2e9-f9d6-40e5-aecc-e767ce50e9c0 | prometheus | prometheus_alarm |   ok   |    low   |   True   |
+--------------------------------------+------------+------------------+-------------------+----------+</pre>
</div>
</div>
</li>
</ol>
</div>
</li>
</ol>
</div>
</div>
<div class="sect2">
<h3 id="pulling-configuration-from-tripleo-deployment_adopt-control-plane">Pulling the configuration from a director deployment</h3>
<div class="paragraph">
<p>Before you start the data plane adoption workflow, back up the configuration from the Red&#160;Hat OpenStack Platform (RHOSP) services and director. You can then use the files during the configuration of the adopted services to ensure that nothing is missed or misconfigured.</p>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>The os-diff tool is installed and configured. For more information, see
<a href="#comparing-configuration-files-between-deployments_configuring-network">Comparing configuration files between deployments</a>.</p>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Update your ssh parameters according to your environment in the <code>os-diff.cfg</code>. Os-diff uses the ssh parameters to connect to your director node, and then query and download the configuration files:</p>
<div class="listingblock">
<div class="content">
<pre>ssh_cmd=ssh -F ssh.config standalone
container_engine=podman
connection=ssh
remote_config_path=/tmp/tripleo</pre>
</div>
</div>
<div class="paragraph">
<p>Ensure that the ssh command you provide in <code>ssh_cmd</code> parameter is correct and includes key authentication.</p>
</div>
</li>
<li>
<p>Enable the services that you want to include in the <code>/etc/os-diff/config.yaml</code> file, and disable the services that you want to exclude from the file. Ensure that you have the correct permissions to edit the file:</p>
<div class="listingblock">
<div class="content">
<pre>$ chown ospng:ospng /etc/os-diff/config.yaml</pre>
</div>
</div>
<div class="paragraph">
<p>The following example enables the default Identity service (keystone) to be included in the <code>/etc/os-diff/config.yaml</code> file:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml"># service name and file location
services:
  # Service name
  keystone:
    # Bool to enable/disable a service (not implemented yet)
    enable: true
    # Pod name, in both OCP and podman context.
    # It could be strict match or will only just grep the podman_name
    # and work with all the pods which matched with pod_name.
    # To enable/disable use strict_pod_name_match: true/false
    podman_name: keystone
    pod_name: keystone
    container_name: keystone-api
    # pod options
    # strict match for getting pod id in TripleO and podman context
    strict_pod_name_match: false
    # Path of the config files you want to analyze.
    # It could be whatever path you want:
    # /etc/&lt;service_name&gt; or /etc or /usr/share/&lt;something&gt; or even /
    # @TODO: need to implement loop over path to support multiple paths such as:
    # - /etc
    # - /usr/share
    path:
      - /etc/
      - /etc/keystone
      - /etc/keystone/keystone.conf
      - /etc/keystone/logging.conf</code></pre>
</div>
</div>
<div class="paragraph">
<p>Repeat this step for each RHOSP service that you want to disable or enable.</p>
</div>
</li>
<li>
<p>If you use non-containerized services, such as the <code>ovs-external-ids</code>, pull the configuration or the command output. For example:</p>
<div class="listingblock">
<div class="content">
<pre>services:
  ovs_external_ids:
    hosts: <i class="conum" data-value="1"></i><b>(1)</b>
      - standalone
    service_command: "ovs-vsctl list Open_vSwitch . | grep external_ids | awk -F ': ' '{ print $2; }'" <i class="conum" data-value="2"></i><b>(2)</b>
    cat_output: true <i class="conum" data-value="3"></i><b>(3)</b>
    path:
      - ovs_external_ids.json
    config_mapping: <i class="conum" data-value="4"></i><b>(4)</b>
      ovn-bridge-mappings: edpm_ovn_bridge_mappings <i class="conum" data-value="5"></i><b>(5)</b>
      ovn-bridge: edpm_ovn_bridge
      ovn-encap-type: edpm_ovn_encap_type
      ovn-monitor-all: ovn_monitor_all
      ovn-remote-probe-interval: edpm_ovn_remote_probe_interval
      ovn-ofctrl-wait-before-clear: edpm_ovn_ofctrl_wait_before_clear</pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
You must correctly configure an SSH configuration file or equivalent for non-standard services, such as OVS. The <code>ovs_external_ids</code> service does not run in a container, and the OVS data is stored on each host of your cloud, for example, <code>controller_1/controller_2/</code>, and so on.
</td>
</tr>
</table>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>The list of hosts, for example, <code>compute-1</code>, <code>compute-2</code>.</td>
</tr>
<tr>
<td><i class="conum" data-value="2"></i><b>2</b></td>
<td>The command that runs against the hosts.</td>
</tr>
<tr>
<td><i class="conum" data-value="3"></i><b>3</b></td>
<td>Os-diff gets the output of the command and stores the output in a file that is specified by the key path.</td>
</tr>
<tr>
<td><i class="conum" data-value="4"></i><b>4</b></td>
<td>Provides a mapping between, in this example, the data plane custom resource definition and the <code>ovs-vsctl</code> output.</td>
</tr>
<tr>
<td><i class="conum" data-value="5"></i><b>5</b></td>
<td>The <code>edpm_ovn_bridge_mappings</code> variable must be a list of strings, for example, <code>["datacentre:br-ex"]</code>.
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>Compare the values:</p>
<div class="listingblock">
<div class="content">
<pre>$ os-diff diff ovs_external_ids.json edpm.crd --crd --service ovs_external_ids</pre>
</div>
</div>
<div class="paragraph">
<p>For example, to check the <code>/etc/yum.conf</code> on every host, you must put the following statement in the <code>config.yaml</code> file. The following example uses a file called <code>yum_config</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>services:
  yum_config:
    hosts:
      - undercloud
      - controller_1
      - compute_1
      - compute_2
    service_command: "cat /etc/yum.conf"
    cat_output: true
    path:
      - yum.conf</pre>
</div>
</div>
</li>
</ol>
</div></td>
</tr>
</table>
</div>
</li>
<li>
<p>Pull the configuration:</p>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>The following command pulls all the configuration files that are included in the <code>/etc/os-diff/config.yaml</code> file. You can configure os-diff to update this file automatically according to your running environment by using the <code>--update</code> or <code>--update-only</code> option. These options set the podman information into the <code>config.yaml</code> for all running containers. The podman information can be useful later, when all the Red&#160;Hat OpenStack Platform services are turned off.</p>
</div>
<div class="paragraph">
<p>Note that when the <code>config.yaml</code> file is populated automatically you must provide the configuration paths manually for each service.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml"># will only update the /etc/os-diff/config.yaml
os-diff pull --update-only</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml"># will update the /etc/os-diff/config.yaml and pull configuration
os-diff pull --update</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml"># will update the /etc/os-diff/config.yaml and pull configuration
os-diff pull</code></pre>
</div>
</div>
<div class="paragraph">
<p>The configuration is pulled and stored by default in the following directory:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>/tmp/tripleo/</pre>
</div>
</div>
</li>
</ol>
</div>
<div class="ulist">
<div class="title">Verification</div>
<ul>
<li>
<p>Verify that you have a directory for each service configuration in your local path:</p>
<div class="listingblock">
<div class="content">
<pre>   tmp/
     tripleo/
       glance/
       keystone/</pre>
</div>
</div>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="rolling-back-control-plane-adoption_adopt-control-plane">Rolling back the control plane adoption</h3>
<div class="paragraph">
<p>If you encountered a problem and are unable to complete the adoption of the Red&#160;Hat OpenStack Platform (RHOSP) control plane services, you can roll back the control plane adoption.</p>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
Do not attempt the rollback if you altered the data plane nodes in any way.
You can only roll back the control plane adoption if you altered the control plane.
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>During the control plane adoption, services on the RHOSP control plane are stopped but not removed. The databases on the RHOSP control plane are not edited during the adoption procedure. The Red&#160;Hat OpenStack Services on OpenShift (RHOSO) control plane receives a copy of the original control plane databases. The rollback procedure assumes that the data plane has not yet been modified by the adoption procedure, and it is still connected to the RHOSP control plane.</p>
</div>
<div class="paragraph">
<p>The rollback procedure consists of the following steps:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Restoring the functionality of the RHOSP control plane.</p>
</li>
<li>
<p>Removing the partially or fully deployed RHOSO control plane.</p>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>To restore the source cloud to a working state, start the RHOSP
control plane services that you previously stopped during the adoption
procedure:</p>
<div class="listingblock">
<div class="content">
<pre>ServicesToStart=("tripleo_horizon.service"
                 "tripleo_keystone.service"
                 "tripleo_barbican_api.service"
                 "tripleo_barbican_worker.service"
                 "tripleo_barbican_keystone_listener.service"
                 "tripleo_cinder_api.service"
                 "tripleo_cinder_api_cron.service"
                 "tripleo_cinder_scheduler.service"
                 "tripleo_cinder_volume.service"
                 "tripleo_cinder_backup.service"
                 "tripleo_glance_api.service"
                 "tripleo_manila_api.service"
                 "tripleo_manila_api_cron.service"
                 "tripleo_manila_scheduler.service"
                 "tripleo_neutron_api.service"
                 "tripleo_placement_api.service"
                 "tripleo_nova_api_cron.service"
                 "tripleo_nova_api.service"
                 "tripleo_nova_conductor.service"
                 "tripleo_nova_metadata.service"
                 "tripleo_nova_scheduler.service"
                 "tripleo_nova_vnc_proxy.service"
                 "tripleo_aodh_api.service"
                 "tripleo_aodh_api_cron.service"
                 "tripleo_aodh_evaluator.service"
                 "tripleo_aodh_listener.service"
                 "tripleo_aodh_notifier.service"
                 "tripleo_ceilometer_agent_central.service"
                 "tripleo_ceilometer_agent_compute.service"
                 "tripleo_ceilometer_agent_ipmi.service"
                 "tripleo_ceilometer_agent_notification.service"
                 "tripleo_ovn_cluster_north_db_server.service"
                 "tripleo_ovn_cluster_south_db_server.service"
                 "tripleo_ovn_cluster_northd.service"
                 "tripleo_octavia_api.service"
                 "tripleo_octavia_health_manager.service"
                 "tripleo_octavia_rsyslog.service"
                 "tripleo_octavia_driver_agent.service"
                 "tripleo_octavia_housekeeping.service"
                 "tripleo_octavia_worker.service")

PacemakerResourcesToStart=("galera-bundle"
                           "haproxy-bundle"
                           "rabbitmq-bundle"
                           "openstack-cinder-volume"
                           "openstack-cinder-backup"
                           "openstack-manila-share")

echo "Starting systemd OpenStack services"
for service in ${ServicesToStart[*]}; do
    for i in {1..3}; do
        SSH_CMD=CONTROLLER${i}_SSH
        if [ ! -z "${!SSH_CMD}" ]; then
            if ${!SSH_CMD} sudo systemctl is-enabled $service &amp;&gt; /dev/null; then
                echo "Starting the $service in controller $i"
                ${!SSH_CMD} sudo systemctl start $service
            fi
        fi
    done
done

echo "Checking systemd OpenStack services"
for service in ${ServicesToStart[*]}; do
    for i in {1..3}; do
        SSH_CMD=CONTROLLER${i}_SSH
        if [ ! -z "${!SSH_CMD}" ]; then
            if ${!SSH_CMD} sudo systemctl is-enabled $service &amp;&gt; /dev/null; then
                if ! ${!SSH_CMD} systemctl show $service | grep ActiveState=active &gt;/dev/null; then
                    echo "ERROR: Service $service is not running on controller $i"
                else
                    echo "OK: Service $service is running in controller $i"
                fi
            fi
        fi
    done
done

echo "Starting pacemaker OpenStack services"
for i in {1..3}; do
    SSH_CMD=CONTROLLER${i}_SSH
    if [ ! -z "${!SSH_CMD}" ]; then
        echo "Using controller $i to run pacemaker commands"
        for resource in ${PacemakerResourcesToStart[*]}; do
            if ${!SSH_CMD} sudo pcs resource config $resource &amp;&gt;/dev/null; then
                echo "Starting $resource"
                ${!SSH_CMD} sudo pcs resource enable $resource
            else
                echo "Service $resource not present"
            fi
        done
        break
    fi
done

echo "Checking pacemaker OpenStack services"
for i in {1..3}; do
    SSH_CMD=CONTROLLER${i}_SSH
    if [ ! -z "${!SSH_CMD}" ]; then
        echo "Using controller $i to run pacemaker commands"
        for resource in ${PacemakerResourcesToStop[*]}; do
            if ${!SSH_CMD} sudo pcs resource config $resource &amp;&gt;/dev/null; then
                if ${!SSH_CMD} sudo pcs resource status $resource | grep Started &gt;/dev/null; then
                    echo "OK: Service $resource is started"
                else
                    echo "ERROR: Service $resource is stopped"
                fi
            fi
        done
        break
    fi
done</pre>
</div>
</div>
</li>
<li>
<p>If the Ceph NFS service is running on the deployment as a Shared File Systems service (manila) back end, you must restore the Pacemaker order and colocation constraints for the <code>openstack-manila-share</code> service:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo pcs constraint order start ceph-nfs then openstack-manila-share kind=Optional id=order-ceph-nfs-openstack-manila-share-Optional
$ sudo pcs constraint colocation add openstack-manila-share with ceph-nfs score=INFINITY id=colocation-openstack-manila-share-ceph-nfs-INFINITY</pre>
</div>
</div>
</li>
<li>
<p>Verify that the source cloud is operational again, for example, you
can run <code>openstack</code> CLI commands such as <code>openstack server list</code>, or check that you can access the Dashboard service (horizon).</p>
</li>
<li>
<p>Remove the partially or fully deployed control plane so that you can attempt the adoption again later:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc delete --ignore-not-found=true --wait=false openstackcontrolplane/openstack
$ oc patch openstackcontrolplane openstack --type=merge --patch '
metadata:
  finalizers: []
' || true

while oc get pod | grep rabbitmq-server-0; do
    sleep 2
done
while oc get pod | grep openstack-galera-0; do
    sleep 2
done

$ oc delete --ignore-not-found=true --wait=false pod mariadb-copy-data
$ oc delete --ignore-not-found=true --wait=false pvc mariadb-data
$ oc delete --ignore-not-found=true --wait=false pod ovn-copy-data
$ oc delete --ignore-not-found=true secret osp-secret</pre>
</div>
</div>
</li>
</ol>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
After you restore the RHOSP control plane services, their internal
state might have changed. Before you retry the adoption procedure, verify that all the control plane resources are removed and that there are no leftovers which could affect the following adoption procedure attempt. You must not use previously created copies of the database contents in another adoption attempt. You must make a new copy of the latest state of the original source database contents. For more information about making new copies of the database, see <a href="#migrating-databases-to-the-control-plane_configuring-network">Migrating databases to the control plane</a>.
</td>
</tr>
</table>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="adopting-data-plane_adopt-control-plane">Adopting the data plane</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Adopting the Red&#160;Hat OpenStack Services on OpenShift (RHOSO) data plane involves the following steps:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Stop any remaining services on the Red&#160;Hat OpenStack Platform (RHOSP) 17.1 control plane.</p>
</li>
<li>
<p>Deploy the required custom resources.</p>
</li>
<li>
<p>Perform a fast-forward upgrade on Compute services from RHOSP 17.1 to RHOSO 18.0.</p>
</li>
<li>
<p>Adopt Networker services to the RHOSO data plane.</p>
</li>
</ol>
</div>
<div class="admonitionblock warning">
<table>
<tr>
<td class="icon">
<i class="fa icon-warning" title="Warning"></i>
</td>
<td class="content">
After the RHOSO control plane manages the newly deployed data plane, you must not re-enable services on the RHOSP 17.1 control plane and data plane. If you re-enable services, workloads are managed by two control planes or two data planes, resulting in data corruption, loss of control of existing workloads, inability to start new workloads, or other issues.
</td>
</tr>
</table>
</div>
<div class="sect2">
<h3 id="stopping-infrastructure-management-and-compute-services_data-plane">Stopping infrastructure management and Compute services</h3>
<div class="paragraph">
<p>You must stop cloud database nodes, and messaging nodes on the Red&#160;Hat OpenStack Platform 17.1 control plane. Do not stop nodes that are running the Compute, Storage, Networker or Controller(if running <code>OVN Controller Gateway agent</code> network agent) roles on the control plane.</p>
</div>
<div class="paragraph">
<p>The following procedure applies to a single node standalone director deployment. You must remove conflicting repositories and packages from your Compute hosts, so that you can install libvirt packages when these hosts are adopted as data plane nodes, where modular libvirt daemons are no longer running in podman containers.</p>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>Define the shell variables. Replace the following example values with values that apply to your environment:</p>
<div class="listingblock">
<div class="content">
<pre>CONTROLLER1_SSH="ssh -i &lt;path_to_SSH_key&gt; root@&lt;controller-1 IP&gt;"
# ...
# ... <i class="conum" data-value="1"></i><b>(1)</b>
EDPM_PRIVATEKEY_PATH="&lt;path_to_SSH_key&gt;" <i class="conum" data-value="2"></i><b>(2)</b></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>In the <code>CONTROLLER&lt;X&gt;_SSH</code> settings, provide SSH connection details for all Controller nodes, including cell Controller nodes, of the source director cloud.</td>
</tr>
<tr>
<td><i class="conum" data-value="2"></i><b>2</b></td>
<td>Replace <code>&lt;path_to_SSH_key&gt;</code> with the path to your SSH key.</td>
</tr>
</table>
</div>
</li>
</ul>
</div>
<div class="ulist">
<div class="title">Procedure</div>
<ul>
<li>
<p>Remove the conflicting repositories and packages from all Compute hosts:</p>
<div class="listingblock">
<div class="content">
<pre>PacemakerResourcesToStop=(
                "galera-bundle"
                "haproxy-bundle"
                "rabbitmq-bundle")

echo "Stopping pacemaker services"
for i in {1..3}; do
    SSH_CMD=CONTROLLER${i}_SSH
    if [ ! -z "${!SSH_CMD}" ]; then
        echo "Using controller $i to run pacemaker commands"
        for resource in ${PacemakerResourcesToStop[*]}; do
            if ${!SSH_CMD} sudo pcs resource config $resource; then
                ${!SSH_CMD} sudo pcs resource disable $resource
            fi
        done
        break
    fi
done</pre>
</div>
</div>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="adopting-compute-services-to-the-data-plane_data-plane">Adopting Compute services to the RHOSO data plane</h3>
<div class="paragraph">
<p>Adopt your Compute (nova) services to the Red&#160;Hat OpenStack Services on OpenShift (RHOSO) data plane.</p>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>You have stopped the remaining control plane nodes, repositories, and packages on the Compute service (nova) hosts. For more information, see <a href="#stopping-infrastructure-management-and-compute-services_data-plane">Stopping infrastructure management and Compute services</a>.</p>
</li>
<li>
<p>You have configured the Ceph back end for the <code>NovaLibvirt</code> service. For more information, see <a href="#configuring-a-ceph-backend_migrating-databases">Configuring a Ceph back end</a>.</p>
</li>
<li>
<p>You have configured IP Address Management (IPAM):</p>
<div class="listingblock">
<div class="content">
<pre>$ oc apply -f - &lt;&lt;EOF
apiVersion: network.openstack.org/v1beta1
kind: NetConfig
metadata:
  name: netconfig
spec:
  networks:
  - name: ctlplane
    dnsDomain: ctlplane.example.com
    subnets:
    - name: subnet1
      allocationRanges:
      - end: 192.168.122.120
        start: 192.168.122.100
      - end: 192.168.122.200
        start: 192.168.122.150
      cidr: 192.168.122.0/24
      gateway: 192.168.122.1
  - name: internalapi
    dnsDomain: internalapi.example.com
    subnets:
    - name: subnet1
      allocationRanges:
      - end: 172.17.0.250
        start: 172.17.0.100
      cidr: 172.17.0.0/24
      vlan: 20
  - name: External
    dnsDomain: external.example.com
    subnets:
    - name: subnet1
      allocationRanges:
      - end: 10.0.0.250
        start: 10.0.0.100
      cidr: 10.0.0.0/24
      gateway: 10.0.0.1
  - name: storage
    dnsDomain: storage.example.com
    subnets:
    - name: subnet1
      allocationRanges:
      - end: 172.18.0.250
        start: 172.18.0.100
      cidr: 172.18.0.0/24
      vlan: 21
  - name: storagemgmt
    dnsDomain: storagemgmt.example.com
    subnets:
    - name: subnet1
      allocationRanges:
      - end: 172.20.0.250
        start: 172.20.0.100
      cidr: 172.20.0.0/24
      vlan: 23
  - name: tenant
    dnsDomain: tenant.example.com
    subnets:
    - name: subnet1
      allocationRanges:
      - end: 172.19.0.250
        start: 172.19.0.100
      cidr: 172.19.0.0/24
      vlan: 22
EOF</pre>
</div>
</div>
</li>
<li>
<p>If <code>neutron-sriov-nic-agent</code> is running on your Compute service nodes, ensure that the physical device mappings match the values that are defined in the <code>OpenStackDataPlaneNodeSet</code> custom resource (CR). For more information, see <a href="#pulling-configuration-from-tripleo-deployment_adopt-control-plane">Pulling the configuration from a director deployment</a>.</p>
</li>
<li>
<p>You have defined the shell variables to run the script that runs the upgrade:</p>
<div class="listingblock">
<div class="content">
<pre>$ CEPH_FSID=$(oc get secret ceph-conf-files -o json | jq -r '.data."ceph.conf"' | base64 -d | grep fsid | sed -e 's/fsid = //'

$ alias openstack="oc exec -t openstackclient -- openstack"

$ DEFAULT_CELL_NAME="cell3" <i class="conum" data-value="1"></i><b>(1)</b>
$ RENAMED_CELLS="cell1 cell2 $DEFAULT_CELL_NAME"

$ declare -A COMPUTES_CELL1
$ export COMPUTES_CELL1=( <i class="conum" data-value="2"></i><b>(2)</b>
  ["standalone.localdomain"]="192.168.122.100" <i class="conum" data-value="3"></i><b>(3)</b>
  # &lt;compute1&gt; <i class="conum" data-value="4"></i><b>(4)</b>
  # &lt;compute2&gt;
  # &lt;compute3&gt;
)
$ declare -A COMPUTES_CELL2
$ export COMPUTES_CELL2=(
  # ...
)
$ declare -A COMPUTES_CELL3
$ export COMPUTES_CELL3=(
  # ... <i class="conum" data-value="5"></i><b>(5)</b>
)
# ...

$ declare -A COMPUTES_API_CELL1
$ export COMPUTES_API_CELL1=( <i class="conum" data-value="6"></i><b>(6)</b>
  ["standalone.localdomain"]="172.17.0.100"
  # ...
)
# ...

$ NODESETS=""
$ for CELL in $(echo $RENAMED_CELLS); do
  ref="COMPUTES_$(echo ${CELL}|tr '[:lower:]' '[:upper:]')"
  eval names=\${!${ref}[@]}
  [ -z "$names" ] &amp;&amp; continue
  NODESETS="'openstack-${CELL}', $NODESETS" <i class="conum" data-value="7"></i><b>(7)</b>
done
$ NODESETS="[${NODESETS%,*}]"</pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>The source cloud <code>default</code> cell acquires a new <code>DEFAULT_CELL_NAME</code> on the destination cloud after adoption.
In a multi-cell adoption scenario, you can retain the original name, <code>default</code>, or create a new cell default name by providing the incremented index of the last cell in the source cloud. For example, if the incremented index of the last cell is <code>cell5</code>, the new cell default name is <code>cell6</code>.</td>
</tr>
<tr>
<td><i class="conum" data-value="2"></i><b>2</b></td>
<td>For each cell, update the <code>&lt;["standalone.localdomain"]="x.x.x.x"&gt;</code> value and the and <code>COMPUTES_CELL&lt;X&gt;</code> value with the names and IP addresses of the Compute service nodes that are connected to the <code>ctlplane</code> and <code>internalapi</code> networks. Do not specify a real FQDN defined for each network. Always use the same hostname for each connected network of a Compute node. Provide the IP addresses and the names of the hosts on the remaining networks of the source cloud as needed. Or you can manually adjust the files that you generate in step 9 of this procedure.</td>
</tr>
<tr>
<td><i class="conum" data-value="3"></i><b>3</b></td>
<td>If your deployment has a custom DNS domain, specify it in the FQDN value of the nodes. This value is used in the data plane node set <code>spec.nodes.&lt;NODE NAME&gt;.hostName</code>.</td>
</tr>
<tr>
<td><i class="conum" data-value="4"></i><b>4</b></td>
<td>Assign all Compute service nodes from the source cloud <code>cell1</code> cell into <code>COMPUTES_CELL1</code>, and so on. Replace <code>&lt;compute1&gt;</code>, <code>&lt;compute2&gt;</code>, and <code>&lt;compute3&gt;</code> with the names of your Compute service nodes.</td>
</tr>
<tr>
<td><i class="conum" data-value="5"></i><b>5</b></td>
<td>Assign all Compute service nodes from the source cloud <code>default</code> cell into <code>COMPUTES_CELL&lt;X&gt;</code> and <code>COMPUTES_API_CELL&lt;X&gt;`</code>, where <code>&lt;X&gt;</code> is the <code>DEFAULT_CELL_NAME</code> environment variable value. In this example, the <code>DEFAULT_CELL_NAME</code> environment variable value equals <code>cell3</code>.</td>
</tr>
<tr>
<td><i class="conum" data-value="6"></i><b>6</b></td>
<td>For each cell, update the <code>&lt;["standalone.localdomain"]="192.168.122.100"&gt;</code> value and the <code>COMPUTES_API_CELL</code> value with the names and IP addresses of the Compute service nodes that are connected to the <code>ctlplane</code> and <code>internalapi</code> networks. Do not specify a real FQDN defined for each network. Use the same host name for each of its connected networks. Provide the IP addresses and the names of the hosts on the remaining networks of the source cloud as needed. Or you can manually adjust the files that you generate in step 9 of this procedure.</td>
</tr>
<tr>
<td><i class="conum" data-value="7"></i><b>7</b></td>
<td>Cells that do not contain Compute nodes are omitted from this template because no node sets are created for the cells.
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>If you deployed the source cloud with a <code>default</code> cell, and want to rename it during adoption, define the new name that you want to use, as shown in the following example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>$ DEFAULT_CELL_NAME="cell1"
$ RENAMED_CELLS="cell1"</pre>
</div>
</div>
</td>
</tr>
</table>
</div></td>
</tr>
</table>
</div>
</li>
</ul>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Do not set a value for the <code>CEPH_FSID</code> parameter if the local storage back end is configured by the Compute service for libvirt. The storage back end must match the source cloud storage back end. You cannot change the storage back end during adoption.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Create an SSH authentication secret for the data plane nodes:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc apply -f - &lt;&lt;EOF
apiVersion: v1
kind: Secret
metadata:
    name: dataplane-adoption-secret
data:
    ssh-privatekey: |
$(cat &lt;path_to_SSH_key&gt; | base64 | sed 's/^/        /')
EOF</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Replace <code>&lt;path_to_SSH_key&gt;</code> with the path to your SSH key.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Generate an ssh key-pair <code>nova-migration-ssh-key</code> secret:</p>
<div class="listingblock">
<div class="content">
<pre>$ cd "$(mktemp -d)"
ssh-keygen -f ./id -t ecdsa-sha2-nistp521 -N ''
oc get secret nova-migration-ssh-key || oc create secret generic nova-migration-ssh-key \
  --from-file=ssh-privatekey=id \
  --from-file=ssh-publickey=id.pub \
  --type kubernetes.io/ssh-auth
rm -f id*
cd -</pre>
</div>
</div>
</li>
<li>
<p>If TLS Everywhere is enabled, set <code>LIBVIRT_PASSWORD</code> to match the existing RHOSP deployment password:</p>
<div class="listingblock">
<div class="content">
<pre>declare -A TRIPLEO_PASSWORDS
TRIPLEO_PASSWORDS[default]="$HOME/overcloud-passwords.yaml"
LIBVIRT_PASSWORD=$(cat ${TRIPLEO_PASSWORDS[default]} | grep ' LibvirtTLSPassword:' | awk -F ': ' '{ print $2; }')
LIBVIRT_PASSWORD_BASE64=$(echo -n "$LIBVIRT_PASSWORD" | base64)</pre>
</div>
</div>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>Create libvirt-secret when TLS-e is enabled:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc apply -f - &lt;&lt;EOF
apiVersion: v1
kind: Secret
metadata:
  name: libvirt-secret
type: Opaque
data:
  LibvirtPassword: ${LIBVIRT_PASSWORD_BASE64}
EOF</pre>
</div>
</div>
</li>
</ol>
</div>
</li>
<li>
<p>Create a configuration map to use for all cells to configure a local storage back end for libvirt:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc apply -f - &lt;&lt;EOF
apiVersion: v1
kind: ConfigMap
metadata:
  name: nova-cells-global-config
data: <i class="conum" data-value="1"></i><b>(1)</b>
  99-nova-compute-cells-workarounds.conf: | <i class="conum" data-value="2"></i><b>(2)</b>
    [workarounds]
    disable_compute_service_check_for_ffu=true
EOF</pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>The <code>data</code> resources in the <code>ConfigMap</code> provide the configuration files for all the cells.</td>
</tr>
<tr>
<td><i class="conum" data-value="2"></i><b>2</b></td>
<td>There is a requirement to index the <code>&lt;*.conf&gt;</code> files from <em>03</em> to <em>99</em>, based on precedence. A <code>&lt;99-*.conf&gt;</code> file takes the highest precedence, while indexes below <em>03</em> are reserved for internal use.
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
If you adopt a live cloud, you might be required to carry over additional configurations for the default <code>nova</code> data plane services that are stored in the cell1 default <code>nova-extra-config</code> configuration map. Do not delete or overwrite the existing configuration in the <code>cell1</code> default <code>nova-extra-config</code> configuration map that is assigned to <code>nova</code>. Overwriting the configuration can break the data place services that rely on specific contents of the <code>nova-extra-config</code> configuration map.
</td>
</tr>
</table>
</div></td>
</tr>
</table>
</div>
</li>
<li>
<p>Configure a Red Hat Ceph Storage back end for libvirt:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc apply -f - &lt;&lt;EOF
apiVersion: v1
kind: ConfigMap
metadata:
  name: nova-cells-global-config
data:
  99-nova-compute-cells-workarounds.conf: |
    [workarounds]
    disable_compute_service_check_for_ffu=true
  03-ceph-nova.conf: |
    [libvirt]
    images_type=rbd
    images_rbd_pool=vms
    images_rbd_ceph_conf=/etc/ceph/ceph.conf
    images_rbd_glance_store_name=default_backend
    images_rbd_glance_copy_poll_interval=15
    images_rbd_glance_copy_timeout=600
    rbd_user=openstack
    rbd_secret_uuid=$CEPH_FSID
EOF</pre>
</div>
</div>
</li>
<li>
<p>Create the data plane services for Compute service cells to enable pre-upgrade workarounds, and to configure the Compute services for your chosen storage back end:</p>
<div class="listingblock">
<div class="content">
<pre>for CELL in $(echo $RENAMED_CELLS); do
 $ oc apply -f - &lt;&lt;EOF
---
apiVersion: dataplane.openstack.org/v1beta1
kind: OpenStackDataPlaneService
metadata:
  name: nova-$CELL
spec:
  dataSources: <i class="conum" data-value="1"></i><b>(1)</b>
    - secretRef:
        name: nova-$CELL-compute-config <i class="conum" data-value="2"></i><b>(2)</b>
    - secretRef:
        name: nova-migration-ssh-key <i class="conum" data-value="3"></i><b>(3)</b>
    - configMapRef:
        name: nova-cells-global-config
  playbook: osp.edpm.nova
  caCerts: combined-ca-bundle
  edpmServiceType: nova
  containerImageFields:
  - NovaComputeImage
  - EdpmIscsidImage
EOF
  done</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>If TLS Everywhere is enabled, append the following content to the <code>OpenStackDataPlaneService</code> CR:</p>
<div class="listingblock">
<div class="content">
<pre>  tlsCerts:
    contents:
      - dnsnames
      - ips
    networks:
      - ctlplane
    issuer: osp-rootca-issuer-internal
    edpmRoleServiceName: nova
  caCerts: combined-ca-bundle
  edpmServiceType: nova</pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>To enable a local metadata service for cell&lt;X&gt;, append a <code>spec.dataSources.secretRef</code> to reference an additional auto-generated <code>nova-cell&lt;X&gt;-metadata-neutron-config</code> secret. You should also set
<code>spec.nova.template.cellTemplates.cell&lt;X&gt;.metadataServiceTemplate.enable</code> in the <code>OpenStackControlPlane/openstack</code> CR, as described in <a href="#adopting-the-compute-service_data-plane">Adopting the Compute service</a>. You can configure a single top-level metadata, or define the metadata per cell.</td>
</tr>
<tr>
<td><i class="conum" data-value="2"></i><b>2</b></td>
<td>The secret <code>nova-cell&lt;X&gt;-compute-config</code> auto-generates for each <code>cell&lt;X&gt;</code>.</td>
</tr>
<tr>
<td><i class="conum" data-value="3"></i><b>3</b></td>
<td>You must append the <code>nova-cell&lt;X&gt;-compute-config</code> and <code>nova-migration-ssh-key</code> references for each custom <code>OpenStackDataPlaneService</code> CR that is related to the Compute service.
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>When creating your data plane services for Compute service cells, review the following considerations:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>In this example, the same <code>nova-migration-ssh-key</code> key is shared across cells. However, you should use different keys for different cells.</p>
</li>
<li>
<p>For simple configuration overrides, you do not need a custom data plane service. However, to reconfigure the cell, <code>cell1</code>,
the safest option is to create a custom service and a dedicated configuration map for it.</p>
</li>
<li>
<p>The cell, <code>cell1</code>, is already managed with the default <code>OpenStackDataPlaneService</code> CR called <code>nova</code> and its <code>nova-extra-config</code> configuration map. Do not change the default data plane service <code>nova</code> definition. The changes are lost when the RHOSO operator is updated with OLM.</p>
</li>
<li>
<p>When a cell spans multiple node sets, give the custom <code>OpenStackDataPlaneService</code> resources a name that relates to the node set, for example, <code>nova-cell1-nfv</code> and <code>nova-cell1-enterprise</code>. The auto-generated configuration maps are then named <code>nova-cell1-nfv-extra-config</code> and <code>nova-cell1-enterprise-extra-config</code>.</p>
</li>
<li>
<p>Different configurations for nodes in multiple node sets of the same cell are also supported, but are not covered in this guide.</p>
</li>
</ul>
</div>
</td>
</tr>
</table>
</div></td>
</tr>
</table>
</div>
</li>
</ul>
</div>
</li>
<li>
<p>Create a secret for the subscription manager:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc create secret generic subscription-manager \
--from-literal rhc_auth='{"login": {"username": "&lt;subscription_manager_username&gt;", "password": "&lt;subscription_manager_password&gt;"}}'</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Replace <code>&lt;subscription_manager_username&gt;</code> with the applicable username.</p>
</li>
<li>
<p>Replace <code>&lt;subscription_manager_password&gt;</code> with the applicable password.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Create a secret for the Red Hat registry:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc create secret generic redhat-registry \
--from-literal edpm_container_registry_logins='{"registry.redhat.io": {"&lt;registry_username&gt;": "&lt;registry_password&gt;"}}'</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Replace <code>&lt;registry_username&gt;</code> with the applicable username.</p>
</li>
<li>
<p>Replace <code>&lt;registry_password&gt;</code> with the applicable password.</p>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
You do not need to reference the <code>subscription-manager</code> secret in the <code>dataSources</code> field of the <code>OpenStackDataPlaneService</code> CR.
The secret is already passed in with a node-specific <code>OpenStackDataPlaneNodeSet</code> CR in the <code>ansibleVarsFrom</code> property in the <code>nodeTemplate</code> field.
</td>
</tr>
</table>
</div>
</li>
</ul>
</div>
</li>
<li>
<p>Create the data plane node set definitions for each cell:</p>
<div class="listingblock">
<div class="content">
<pre>$ declare -A names
$ for CELL in $(echo $RENAMED_CELLS); do
  ref="COMPUTES_$(echo ${CELL}|tr '[:lower:]' '[:upper:]')"
  eval names=\${!${ref}[@]}
  ref_api="COMPUTES_API_$(echo ${CELL}|tr '[:lower:]' '[:upper:]')"
  [ -z "$names" ] &amp;&amp; continue
  ind=0
  rm -f computes-$CELL
  for compute in $names; do
    ip="${ref}['$compute']"
    ip_api="${ref_api}['$compute']"
    cat &gt;&gt; computes-$CELL &lt;&lt; EOF
    ${compute}:
      hostName: $compute <i class="conum" data-value="1"></i><b>(1)</b>
      ansible:
        ansibleHost: $compute
      networks: <i class="conum" data-value="2"></i><b>(2)</b>
      - defaultRoute: true
        fixedIP: ${!ip}
        name: ctlplane
        subnetName: subnet1
      - name: internalapi
        subnetName: subnet1
        fixedIP: ${!ip_api}
      - name: storage
        subnetName: subnet1
      - name: tenant
        subnetName: subnet1
EOF
    ind=$(( ind + 1 ))
  done

  test -f computes-$CELL || continue
  cat &gt; nodeset-${CELL}.yaml &lt;&lt;EOF
apiVersion: dataplane.openstack.org/v1beta1
kind: OpenStackDataPlaneNodeSet
metadata:
  name: openstack-$CELL <i class="conum" data-value="3"></i><b>(3)</b>
spec:
  tlsEnabled: false <i class="conum" data-value="4"></i><b>(4)</b>
  networkAttachments:
      - ctlplane
  preProvisioned: true
  services:
    - redhat
    - bootstrap
    - download-cache
    - configure-network
    - validate-network
    - install-os
    - configure-os
    - ssh-known-hosts
    - run-os
    - reboot-os
    - install-certs
    - ovn
    - neutron-metadata
    - libvirt
    - nova-$CELL
    - telemetry <i class="conum" data-value="5"></i><b>(5)</b>
  env:
    - name: ANSIBLE_CALLBACKS_ENABLED
      value: "profile_tasks"
    - name: ANSIBLE_FORCE_COLOR
      value: "True"
    - name: ANSIBLE_VERBOSITY
      value: 3
  nodeTemplate:
    ansibleSSHPrivateKeySecret: dataplane-adoption-secret
    ansible:
      ansibleUser: root
      ansibleVarsFrom:
      - secretRef:
          name: subscription-manager
      - secretRef:
          name: redhat-registry
      ansibleVars:
        rhc_release: 9.2
        rhc_repositories:
            - {name: "*", state: disabled}
            - {name: "rhel-9-for-x86_64-baseos-eus-rpms", state: enabled}
            - {name: "rhel-9-for-x86_64-appstream-eus-rpms", state: enabled}
            - {name: "rhel-9-for-x86_64-highavailability-eus-rpms", state: enabled}
            - {name: "rhoso-18.0-for-rhel-9-x86_64-rpms", state: enabled}
            - {name: "fast-datapath-for-rhel-9-x86_64-rpms", state: enabled}
            - {name: "rhceph-7-tools-for-rhel-9-x86_64-rpms", state: enabled}
        edpm_bootstrap_release_version_package: []
        # edpm_network_config
        # Default nic config template for a EDPM node
        # These vars are edpm_network_config role vars
        edpm_network_config_template: |
           ---
           {% set mtu_list = [ctlplane_mtu] %}
           {% for network in nodeset_networks %}
           {{ mtu_list.append(lookup('vars', networks_lower[network] ~ '_mtu')) }}
           {%- endfor %}
           {% set min_viable_mtu = mtu_list | max %}
           network_config:
           - type: ovs_bridge
             name: {{ neutron_physical_bridge_name }}
             mtu: {{ min_viable_mtu }}
             use_dhcp: false
             dns_servers: {{ ctlplane_dns_nameservers }}
             domain: {{ dns_search_domains }}
             addresses:
             - ip_netmask: {{ ctlplane_ip }}/{{ ctlplane_cidr }}
             routes: {{ ctlplane_host_routes }}
             members:
             - type: interface
               name: nic1
               mtu: {{ min_viable_mtu }}
               # force the MAC address of the bridge to this interface
               primary: true
           {% for network in nodeset_networks %}
             - type: vlan
               mtu: {{ lookup('vars', networks_lower[network] ~ '_mtu') }}
               vlan_id: {{ lookup('vars', networks_lower[network] ~ '_vlan_id') }}
               addresses:
               - ip_netmask:
                   {{ lookup('vars', networks_lower[network] ~ '_ip') }}/{{ lookup('vars', networks_lower[network] ~ '_cidr') }}
               routes: {{ lookup('vars', networks_lower[network] ~ '_host_routes') }}
           {% endfor %}

        edpm_network_config_nmstate: false
        # Control resolv.conf management by NetworkManager
        # false = disable NetworkManager resolv.conf update (default)
        # true = enable NetworkManager resolv.conf update
        edpm_bootstrap_network_resolvconf_update: false
        edpm_network_config_hide_sensitive_logs: false
        #
        # These vars are for the network config templates themselves and are
        # considered EDPM network defaults.
        neutron_physical_bridge_name: br-ctlplane <i class="conum" data-value="6"></i><b>(6)</b>
        neutron_public_interface_name: eth0

        # edpm_nodes_validation
        edpm_nodes_validation_validate_controllers_icmp: false
        edpm_nodes_validation_validate_gateway_icmp: false

        # edpm ovn-controller configuration
        edpm_ovn_bridge_mappings: &lt;bridge_mappings&gt; <i class="conum" data-value="7"></i><b>(7)</b>
        edpm_ovn_bridge: br-int
        edpm_ovn_encap_type: geneve
        ovn_monitor_all: true
        edpm_ovn_remote_probe_interval: 60000
        edpm_ovn_ofctrl_wait_before_clear: 8000

        timesync_ntp_servers:
        - hostname: clock.redhat.com
        - hostname: clock2.redhat.com

        edpm_bootstrap_command: |
          # FIXME: perform dnf upgrade for other packages in EDPM ansible
          # here we only ensuring that decontainerized libvirt can start
          dnf -y upgrade openstack-selinux
          rm -f /run/virtlogd.pid

        gather_facts: false
        # edpm firewall, change the allowed CIDR if needed
        edpm_sshd_configure_firewall: true
        edpm_sshd_allowed_ranges: ['192.168.122.0/24']

        # Do not attempt OVS major upgrades here
        edpm_ovs_packages:
        - openvswitch3.3
        edpm_default_mounts: <i class="conum" data-value="8"></i><b>(8)</b>
          - path: /dev/hugepages&lt;size&gt;
            opts: pagesize=&lt;size&gt;
            fstype: hugetlbfs
            group: hugetlbfs
  nodes:
EOF
  cat computes-$CELL &gt;&gt; nodeset-${CELL}.yaml
done</pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>If your deployment has a custom DNS Domain, specify the FQDN for the node.</td>
</tr>
<tr>
<td><i class="conum" data-value="2"></i><b>2</b></td>
<td>The network composition must match the source cloud configuration to avoid data plane connectivity downtime. The <code>ctlplane</code> network must come first. The commands only retain IP addresses for the hosts on the <code>ctlplane</code> and <code>internalapi</code> networks. Repeat this step for other isolated networks, or update the resulting files manually.</td>
</tr>
<tr>
<td><i class="conum" data-value="3"></i><b>3</b></td>
<td>Use node sets names, such as <code>openstack-cell1</code>, <code>openstack-cell2</code>. Only create node sets for cells that contain Compute nodes.</td>
</tr>
<tr>
<td><i class="conum" data-value="4"></i><b>4</b></td>
<td>If TLS Everywhere is enabled, change <code>tlsEnabled</code> to <code>true</code>.</td>
</tr>
<tr>
<td><i class="conum" data-value="5"></i><b>5</b></td>
<td>If you are not adopting telemetry services, omit it from the services list.</td>
</tr>
<tr>
<td><i class="conum" data-value="6"></i><b>6</b></td>
<td>The bridge name and other OVN and Networking service-specific values must match the source cloud configuration to avoid data plane connectivity downtime.</td>
</tr>
<tr>
<td><i class="conum" data-value="7"></i><b>7</b></td>
<td>Replace <code>&lt;bridge_mappings&gt;</code> with the value of the bridge mappings in your configuration, for example, <code>"datacentre:br-ctlplane"</code>.</td>
</tr>
<tr>
<td><i class="conum" data-value="8"></i><b>8</b></td>
<td>To configure huge pages, replace <code>&lt;size&gt;</code> with the size of the page. To configure multi-sized huge pages, create more items in the list. Note that the mount points must match the source cloud configuration.
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Ensure that you use the same <code>ovn-controller</code> settings in the <code>OpenStackDataPlaneNodeSet</code> CR that you used in the Compute service nodes before adoption. This configuration is stored in the <code>external_ids</code> column in the <code>Open_vSwitch</code> table in the Open vSwitch database:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>$ ovs-vsctl list Open .
...
external_ids        : {hostname=standalone.localdomain, ovn-bridge=br-int, ovn-bridge-mappings=&lt;bridge_mappings&gt;, ovn-chassis-mac-mappings="datacentre:1e:0a:bb:e6:7c:ad", ovn-encap-ip="172.19.0.100", ovn-encap-tos="0", ovn-encap-type=geneve, ovn-match-northd-version=False, ovn-monitor-all=True, ovn-ofctrl-wait-before-clear="8000", ovn-openflow-probe-interval="60", ovn-remote="tcp:ovsdbserver-sb.openstack.svc:6642", ovn-remote-probe-interval="60000", rundir="/var/run/openvswitch", system-id="2eec68e6-aa21-4c95-a868-31aeafc11736"}
...</pre>
</div>
</div>
</td>
</tr>
</table>
</div></td>
</tr>
</table>
</div>
</li>
<li>
<p>Deploy the <code>OpenStackDataPlaneNodeSet</code> CRs for each Compute cell:</p>
<div class="listingblock">
<div class="content">
<pre>$ for CELL in $(echo $RENAMED_CELLS); do
  test -f nodeset-${CELL}.yaml || continue
$ oc apply -f nodeset-${CELL}.yaml
done</pre>
</div>
</div>
</li>
<li>
<p>If you use a Red Hat Ceph Storage back end for Block Storage service (cinder), prepare the adopted data plane workloads:</p>
<div class="listingblock">
<div class="content">
<pre>$ for CELL in $(echo $RENAMED_CELLS); do
  test -f nodeset-${CELL}.yaml || continue
$ oc patch osdpns/openstack-$CELL --type=merge --patch "
  spec:
    services:
      - redhat
      - bootstrap
      - download-cache
      - configure-network
      - validate-network
      - install-os
      - configure-os
      - ssh-known-hosts
      - run-os
      - reboot-os
      - ceph-client
      - install-certs
      - ovn
      - neutron-metadata
      - libvirt
      - nova-$CELL
      - telemetry
    nodeTemplate:
      extraMounts:
      - extraVolType: Ceph
        volumes:
        - name: ceph
          secret:
            secretName: ceph-conf-files
        mounts:
        - name: ceph
          mountPath: "/etc/ceph"
          readOnly: true
  "
done</pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Ensure that you use the same list of services from the original <code>OpenStackDataPlaneNodeSet</code> CR, except for the <code>ceph-client</code> and <code>ceph-hci-pre</code> services.
</td>
</tr>
</table>
</div>
</li>
<li>
<p>Optional: Enable <code>neutron-sriov-nic-agent</code> in the <code>OpenStackDataPlaneNodeSet</code> CR:</p>
<div class="listingblock">
<div class="content">
<pre>$ for CELL in $(echo $RENAMED_CELLS); do
  test -f nodeset-${CELL}.yaml || continue
$ oc patch openstackdataplanenodeset openstack-$CELL --type='json' --patch='[
  {
    "op": "add",
    "path": "/spec/services/-",
    "value": "neutron-sriov"
  }, {
    "op": "add",
    "path": "/spec/nodeTemplate/ansible/ansibleVars/edpm_neutron_sriov_agent_SRIOV_NIC_physical_device_mappings",
    "value": "dummy_sriov_net:dummy-dev"
  }, {
    "op": "add",
    "path": "/spec/nodeTemplate/ansible/ansibleVars/edpm_neutron_sriov_agent_SRIOV_NIC_resource_provider_bandwidths",
    "value": "dummy-dev:40000000:40000000"
  }, {
    "op": "add",
    "path": "/spec/nodeTemplate/ansible/ansibleVars/edpm_neutron_sriov_agent_SRIOV_NIC_resource_provider_hypervisors",
    "value": "dummy-dev:standalone.localdomain"
  }]'
  done</pre>
</div>
</div>
</li>
<li>
<p>Optional: Enable <code>neutron-dhcp</code> in the <code>OpenStackDataPlaneNodeSet</code> CR:</p>
<div class="listingblock">
<div class="content">
<pre>$ for CELL in $(echo $RENAMED_CELLS); do
  test -f nodeset-${CELL}.yaml || continue
$ oc patch openstackdataplanenodeset openstack-$CELL --type='json' --patch='[
  {
    "op": "add",
    "path": "/spec/services/-",
    "value": "neutron-dhcp"
  }]'
done</pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>To use <code>neutron-dhcp</code> with OVN for the Bare Metal Provisioning service (ironic), you must set the <code>disable_ovn_dhcp_for_baremetal_ports</code> configuration option for the Networking service (neutron)  to <code>true</code>.  You can set this configuration in the <code>NeutronAPI</code> spec:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>..
spec:
  serviceUser: neutron
   ...
      customServiceConfig: |
          [DEFAULT]
          dhcp_agent_notification = True
          [ovn]
          disable_ovn_dhcp_for_baremetal_ports = true</pre>
</div>
</div>
</td>
</tr>
</table>
</div>
</li>
<li>
<p>Run the pre-adoption validation:</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>Create the validation service:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc apply -f - &lt;&lt;EOF
apiVersion: dataplane.openstack.org/v1beta1
kind: OpenStackDataPlaneService
metadata:
  name: pre-adoption-validation
spec:
  playbook: osp.edpm.pre_adoption_validation
EOF</pre>
</div>
</div>
</li>
<li>
<p>Create a <code>OpenStackDataPlaneDeployment</code> CR that runs only the validation:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc apply -f - &lt;&lt;EOF
apiVersion: dataplane.openstack.org/v1beta1
kind: OpenStackDataPlaneDeployment
metadata:
  name: openstack-pre-adoption
spec:
  nodeSets: $NODESETS
  servicesOverride:
  - pre-adoption-validation
EOF</pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
If you created different migration SSH keys for different <code>OpenStackDataPlaneService</code> CRs, you should also define a separate <code>OpenStackDataPlaneDeployment</code> CR for each node set or node sets that represent a cell.
</td>
</tr>
</table>
</div>
</li>
<li>
<p>When the validation is finished, confirm that the status of the Ansible EE pods is <code>Completed</code>:</p>
<div class="listingblock">
<div class="content">
<pre>$ watch oc get pod -l app=openstackansibleee</pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre>$ oc logs -l app=openstackansibleee -f --max-log-requests 20</pre>
</div>
</div>
</li>
<li>
<p>Wait for the deployment to reach the <code>Ready</code> status:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc wait --for condition=Ready openstackdataplanedeployment/openstack-pre-adoption --timeout=10m</pre>
</div>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
<div class="paragraph">
<p>If any openstack-pre-adoption validations fail, you must reference the Ansible logs to determine which ones were unsuccessful, and then try the following troubleshooting options:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>If the hostname validation failed, check that the hostname of the data plane
node is correctly listed in the <code>OpenStackDataPlaneNodeSet</code> CR.</p>
</li>
<li>
<p>If the kernel argument check failed, ensure that the kernel argument configuration in the <code>edpm_kernel_args</code> and <code>edpm_kernel_hugepages</code> variables in the <code>OpenStackDataPlaneNodeSet</code> CR is the same as the kernel argument configuration that you used in the Red&#160;Hat OpenStack Platform (RHOSP) 17.1 node.</p>
</li>
<li>
<p>If the tuned profile check failed, ensure that the
<code>edpm_tuned_profile</code> variable in the <code>OpenStackDataPlaneNodeSet</code> CR is configured
to use the same profile as the one set on the RHOSP 17.1 node.</p>
</li>
</ul>
</div>
</td>
</tr>
</table>
</div>
</li>
</ol>
</div>
</li>
<li>
<p>Remove the remaining director services:</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>Create an <code>OpenStackDataPlaneService</code> CR to clean up the data plane services you are adopting:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc apply -f - &lt;&lt;EOF
apiVersion: dataplane.openstack.org/v1beta1
kind: OpenStackDataPlaneService
metadata:
  name: tripleo-cleanup
spec:
  playbook: osp.edpm.tripleo_cleanup
EOF</pre>
</div>
</div>
</li>
<li>
<p>Create the <code>OpenStackDataPlaneDeployment</code> CR to run the clean-up:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc apply -f - &lt;&lt;EOF
apiVersion: dataplane.openstack.org/v1beta1
kind: OpenStackDataPlaneDeployment
metadata:
  name: tripleo-cleanup
spec:
  nodeSets: $NODESETS
  servicesOverride:
  - tripleo-cleanup
EOF</pre>
</div>
</div>
</li>
</ol>
</div>
</li>
<li>
<p>When the clean-up is finished, deploy the <code>OpenStackDataPlaneDeployment</code> CR:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc apply -f - &lt;&lt;EOF
apiVersion: dataplane.openstack.org/v1beta1
kind: OpenStackDataPlaneDeployment
metadata:
  name: openstack
spec:
  nodeSets: $NODESETS
EOF</pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
If you have other node sets to deploy, such as Networker nodes, you can
add them in the <code>nodeSets</code> list in this step, or create separate <code>OpenStackDataPlaneDeployment</code> CRs later. You cannot add new node sets to an <code>OpenStackDataPlaneDeployment</code> CR after deployment.
</td>
</tr>
</table>
</div>
</li>
</ol>
</div>
<div class="olist arabic">
<div class="title">Verification</div>
<ol class="arabic">
<li>
<p>Confirm that all the Ansible EE pods reach a <code>Completed</code> status:</p>
<div class="listingblock">
<div class="content">
<pre>$ watch oc get pod -l app=openstackansibleee</pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre>$ oc logs -l app=openstackansibleee -f --max-log-requests 20</pre>
</div>
</div>
</li>
<li>
<p>Wait for the data plane node sets to reach the <code>Ready</code> status:</p>
<div class="listingblock">
<div class="content">
<pre>$ for CELL in $(echo $RENAMED_CELLS); do
$ oc wait --for condition=Ready osdpns/openstack-$CELL --timeout=30m
done</pre>
</div>
</div>
</li>
<li>
<p>Verify that the Networking service (neutron) agents are running:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc exec openstackclient -- openstack network agent list
+--------------------------------------+------------------------------+------------------------+-------------------+-------+-------+----------------------------+
| ID                                   | Agent Type                   | Host                   | Availability Zone | Alive | State | Binary                     |
+--------------------------------------+------------------------------+------------------------+-------------------+-------+-------+----------------------------+
| 174fc099-5cc9-4348-b8fc-59ed44fcfb0e | DHCP agent                   | standalone.localdomain | nova              | :-)   | UP    | neutron-dhcp-agent         |
| 10482583-2130-5b0d-958f-3430da21b929 | OVN Metadata agent           | standalone.localdomain |                   | :-)   | UP    | neutron-ovn-metadata-agent |
| a4f1b584-16f1-4937-b2b0-28102a3f6eaa | OVN Controller agent         | standalone.localdomain |                   | :-)   | UP    | ovn-controller             |
+--------------------------------------+------------------------------+------------------------+-------------------+-------+-------+----------------------------+</pre>
</div>
</div>
</li>
</ol>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>After you remove all the services from the director cell controllers, you can decomission the cell controllers.
To create new cell Compute nodes, you re-provision the decomissioned controllers as new data plane hosts and add them to the node sets of corresponding or new cells.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="ulist">
<div class="title">Next steps</div>
<ul>
<li>
<p>You must perform a fast-forward upgrade on your Compute services. For more information, see <a href="#performing-a-fast-forward-upgrade-on-compute-services_data-plane">Performing a fast-forward upgrade on Compute services</a>.</p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="performing-a-fast-forward-upgrade-on-compute-services_data-plane">Performing a fast-forward upgrade on Compute services</h3>
<div class="paragraph">
<p>You must upgrade the Compute services from Red&#160;Hat OpenStack Platform 17.1 to Red&#160;Hat OpenStack Services on OpenShift (RHOSO) 18.0 on the control plane and data plane by completing the following tasks:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Update the cell1 Compute data plane services version.</p>
</li>
<li>
<p>Remove pre-fast-forward upgrade workarounds from the Compute control plane services and Compute data plane services.</p>
</li>
<li>
<p>Run Compute database online migrations to update live data.</p>
</li>
</ul>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>Define the shell variables necessary to apply the fast-forward upgrade commands for each Compute service cell.</p>
<div class="listingblock">
<div class="content">
<pre>DEFAULT_CELL_NAME="cell1"
RENAMED_CELLS="$DEFAULT_CELL_NAME"

declare -A PODIFIED_DB_ROOT_PASSWORD
for CELL in $(echo "super $RENAMED_CELLS"); do
  PODIFIED_DB_ROOT_PASSWORD[$CELL]=$(oc get -o json secret/osp-secret | jq -r .data.DbRootPassword | base64 -d)
done</pre>
</div>
</div>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Wait for the Compute service data plane services version to update for all the cells:</p>
<div class="listingblock">
<div class="content">
<pre>$ for CELL in $(echo $RENAMED_CELLS); do
$ oc exec openstack-$CELL-galera-0 -c galera -- mysql -rs -uroot -p"${PODIFIED_DB_ROOT_PASSWORD[$CELL]}" \
    -e "select a.version from nova_${CELL}.services a join nova_${CELL}.services b where a.version!=b.version and a.binary='nova-compute' and a.deleted=0;"
done</pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>The query returns an empty result when the update is completed. No downtime is expected for virtual machine workloads.</p>
</div>
<div class="paragraph">
<p>Review any errors in the nova Compute agent logs on the data plane, and the <code>nova-conductor</code> journal records on the control plane.</p>
</div>
</td>
</tr>
</table>
</div>
</li>
<li>
<p>Patch the <code>OpenStackControlPlane</code> CR to remove the pre-fast-forward upgrade workarounds from the Compute control plane services:</p>
<div class="listingblock">
<div class="content">
<pre>$ rm -f celltemplates
$ for CELL in $(echo $RENAMED_CELLS); do
$ cat &gt;&gt; celltemplates &lt;&lt; EOF
        ${CELL}:
          metadataServiceTemplate:
            customServiceConfig: |
              [workarounds]
              disable_compute_service_check_for_ffu=false
          conductorServiceTemplate:
            customServiceConfig: |
              [workarounds]
              disable_compute_service_check_for_ffu=false
EOF
done

$ cat &gt; oscp-patch.yaml &lt;&lt; EOF
spec:
  nova:
    template:
      apiServiceTemplate:
        customServiceConfig: |
          [workarounds]
          disable_compute_service_check_for_ffu=false
      metadataServiceTemplate:
        customServiceConfig: |
          [workarounds]
          disable_compute_service_check_for_ffu=false
      schedulerServiceTemplate:
        customServiceConfig: |
          [workarounds]
          disable_compute_service_check_for_ffu=false
      cellTemplates:
        cell0:
          conductorServiceTemplate:
            customServiceConfig: |
              [workarounds]
              disable_compute_service_check_for_ffu=false
EOF
$ cat celltemplates &gt;&gt; oscp-patch.yaml</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>If you are adopting the Compute service with the Bare Metal Provisioning service (ironic), append the following <code>novaComputeTemplates</code> in the <code>cell&lt;X&gt;</code> section of the Compute service CR patch:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">        cell&lt;X&gt;:
          novaComputeTemplates:
            &lt;hostname&gt;: <i class="conum" data-value="1"></i><b>(1)</b>
              customServiceConfig: |
                [DEFAULT]
                host = &lt;hostname&gt;
                [workarounds]
                disable_compute_service_check_for_ffu=true
              computeDriver: ironic.IronicDriver
        ...</code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>Replace <code>&lt;hostname&gt;</code> with the hostname of the node that is running the <code>ironic</code> Compute driver in the source cloud of <code>cell&lt;X&gt;</code>.</td>
</tr>
</table>
</div>
</li>
</ul>
</div>
</li>
<li>
<p>Apply the patch file:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc patch openstackcontrolplane openstack --type=merge --patch-file=oscp-patch.yaml</pre>
</div>
</div>
</li>
<li>
<p>Wait until the Compute control plane services CRs are ready:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc wait --for condition=Ready --timeout=300s Nova/nova</pre>
</div>
</div>
</li>
<li>
<p>Complete the steps in <a href="#adopting-compute-services-to-the-data-plane_data-plane">Adopting Compute services to the RHOSO data plane</a>.</p>
</li>
<li>
<p>Remove the pre-fast-forward upgrade workarounds from the Compute data plane services:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc patch cm nova-cells-global-config --type=json -p='[{"op": "replace", "path": "/data/99-nova-compute-cells-workarounds.conf", "value": "[workarounds]\n"}]'
$ for CELL in $(echo $RENAMED_CELLS); do
$ oc get Openstackdataplanenodeset openstack-${CELL} || continue
$ oc apply -f - &lt;&lt;EOF
---
apiVersion: dataplane.openstack.org/v1beta1
kind: OpenStackDataPlaneDeployment
metadata:
  name: openstack-nova-compute-ffu-$CELL
spec:
  nodeSets:
    - openstack-${CELL}
  servicesOverride:
    - nova-${CELL}
backoffLimit: 3
EOF
done</pre>
</div>
</div>
</li>
<li>
<p>Wait for the Compute data plane services to be ready for all the cells:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc wait --for condition=Ready openstackdataplanedeployments --all --timeout=5m</pre>
</div>
</div>
</li>
<li>
<p>Run Compute database online migrations to complete the upgrade:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc exec -it nova-cell0-conductor-0 -- nova-manage db online_data_migrations
$ for CELL in $(echo $RENAMED_CELLS); do
$ oc exec -it nova-${CELL}-conductor-0 -- nova-manage db online_data_migrations
done</pre>
</div>
</div>
</li>
<li>
<p>Discover the Compute hosts in the cells:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc rsh nova-cell0-conductor-0 nova-manage cell_v2 discover_hosts --verbose</pre>
</div>
</div>
</li>
<li>
<p>Verify if the existing test VM instance is running:</p>
<div class="listingblock">
<div class="content">
<pre>${BASH_ALIASES[openstack]} server --os-compute-api-version 2.48 show --diagnostics test 2&gt;&amp;1 || echo FAIL</pre>
</div>
</div>
</li>
<li>
<p>Verify if the Compute services can stop the existing test VM instance:</p>
<div class="listingblock">
<div class="content">
<pre>${BASH_ALIASES[openstack]} server list -c Name -c Status -f value | grep -qF "test ACTIVE" &amp;&amp; ${BASH_ALIASES[openstack]} server stop test || echo PASS
${BASH_ALIASES[openstack]} server list -c Name -c Status -f value | grep -qF "test SHUTOFF" || echo FAIL
${BASH_ALIASES[openstack]} server --os-compute-api-version 2.48 show --diagnostics test 2&gt;&amp;1 || echo PASS</pre>
</div>
</div>
</li>
<li>
<p>Verify if the Compute services can start the existing test VM instance:</p>
<div class="listingblock">
<div class="content">
<pre>${BASH_ALIASES[openstack]} server list -c Name -c Status -f value | grep -qF "test SHUTOFF" &amp;&amp; ${BASH_ALIASES[openstack]} server start test || echo PASS
${BASH_ALIASES[openstack]} server list -c Name -c Status -f value | grep -qF "test ACTIVE" &amp;&amp; \
  ${BASH_ALIASES[openstack]} server --os-compute-api-version 2.48 show --diagnostics test --fit-width -f json | jq -r '.state' | grep running || echo FAIL</pre>
</div>
</div>
</li>
</ol>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
After the data plane adoption, the Compute hosts continue to run Red Hat Enterprise Linux (RHEL) 9.2. To take advantage of RHEL 9.4, perform a minor update procedure after finishing the adoption procedure.
</td>
</tr>
</table>
</div>
</div>
<div class="sect2">
<h3 id="adopting-networker-services-to-the-data-plane_data-plane">Adopting Networker services to the RHOSO data plane</h3>
<div class="paragraph">
<p>Adopt the Networker services in your existing Red&#160;Hat OpenStack Platform deployment to the Red&#160;Hat OpenStack Services on OpenShift (RHOSO) data plane. The <code>Networker</code> services could be running on <code>Conroller</code> nodes or dedicated <code>Networker</code> nodes. You decide which services you want to run on the Networker nodes, and create a separate <code>OpenStackDataPlaneNodeSet</code> custom resource (CR) for the Networker nodes. You might also decide to implement the following options if they apply to your environment:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Depending on your topology, you might need to run the <code>neutron-metadata</code> service on the nodes, specifically when you want to serve metadata to SR-IOV ports that are hosted on Compute nodes.</p>
</li>
<li>
<p>If you want to continue running OVN gateway services on Networker nodes, keep <code>ovn</code> service in the list to deploy.</p>
</li>
<li>
<p>Optional: You can run the <code>neutron-dhcp</code> service on your Networker nodes instead of your Compute nodes. You might not need to use <code>neutron-dhcp</code> with OVN, unless your deployment uses DHCP relays, or advanced DHCP options that are supported by dnsmasq but not by the OVN DHCP implementation.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Adopt each Controller or Networker node in your existing Red&#160;Hat OpenStack Platform deployment to the Red&#160;Hat OpenStack Services on OpenShift (RHOSO) when your node is set as an OVN chassis gateway. Any node with
parameter set to <code>enable-chassis-as-gw</code> is considered OVN gateway chassis. In this case, such nodes will become edpm networker nodes after adoption.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Check for the nodes where <code>OVN Controller Gateway agent</code> agents are running. The list of agents varies depending on the services you enabled:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc exec openstackclient -- openstack network agent list
+--------------------------------------+------------------------------+--------------------------+-------------------+-------+-------+----------------------------+
| ID                                   | Agent Type                   | Host                     | Availability Zone | Alive | State | Binary                     |
+--------------------------------------+------------------------------+--------------------------+-------------------+-------+-------+----------------------------+
| e5075ee0-9dd9-4f0a-a42a-6bbdf1a6111c | OVN Controller Gateway agent | controller-0.localdomain |                   | XXX   | UP    | ovn-controller             |
| f3112349-054c-403a-b00a-e219238192b8 | OVN Controller agent         | compute-0.localdomain    |                   | XXX   | UP    | ovn-controller             |
| af9dae2d-1c1c-55a8-a743-f84719f6406d | OVN Metadata agent           | compute-0.localdomain    |                   | XXX   | UP    | neutron-ovn-metadata-agent |
| 51a11df8-a66e-47a2-aec0-52eb8589626c | OVN Controller Gateway agent | controller-1.localdomain |                   | XXX   | UP    | ovn-controller             |
| bb817e5e-7832-410a-9e67-934dac8c602f | OVN Controller Gateway agent | controller-2.localdomain |                   | XXX   | UP    | ovn-controller             |
+--------------------------------------+------------------------------+--------------------------+-------------------+-------+-------+----------------------------+</pre>
</div>
</div>
</li>
</ol>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>Define the shell variable. Based on above agent list output,
controller-0, controller-1, controller-2 are our target
hosts. If you have both <code>Controller</code> and <code>Networker</code> nodes running
networker services then add all those hosts below.</p>
<div class="listingblock">
<div class="content">
<pre>declare -A networkers
networkers+=(
  ["controller-0.localdomain"]="192.168.122.100"
  ["controller-1.localdomain"]="192.168.122.101"
  ["controller-2.localdomain"]="192.168.122.102"
  # ...
)</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Replace <code>["&lt;node-name&gt;"]="192.168.122.100"</code> with the name and IP address of the corresponding Networker or Controller node as per your environment.</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Deploy the <code>OpenStackDataPlaneNodeSet</code> CR for your nodes:</p>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
You can reuse most of the <code>nodeTemplate</code> section from the <code>OpenStackDataPlaneNodeSet</code> CR that is designated for your Compute nodes. You can omit some of the variables because of the limited set of services that are running on the Networker nodes.
</td>
</tr>
</table>
</div>
<div class="listingblock">
<div class="content">
<pre>$ oc apply -f - &lt;&lt;EOF
apiVersion: dataplane.openstack.org/v1beta1
kind: OpenStackDataPlaneNodeSet
metadata:
  name: openstack-networker
spec:
  tlsEnabled: false <i class="conum" data-value="1"></i><b>(1)</b>
  networkAttachments:
      - ctlplane
  preProvisioned: true
  services:
    - redhat
    - bootstrap
    - download-cache
    - configure-network
    - validate-network
    - install-os
    - configure-os
    - ssh-known-hosts
    - run-os
    - install-certs
    - ovn
  env:
    - name: ANSIBLE_CALLBACKS_ENABLED
      value: "profile_tasks"
    - name: ANSIBLE_FORCE_COLOR
      value: "True"
  nodes:
    controller-0:
      hostName: controller-0
      ansible:
        ansibleHost: ${networkers[controller-0.localdomain]}
      networks:
      - defaultRoute: true
        fixedIP: ${networkers[controller-0.localdomain]}
        name: ctlplane
        subnetName: subnet1
      - name: internalapi
        subnetName: subnet1
      - name: storage
        subnetName: subnet1
      - name: tenant
        subnetName: subnet1
    controller-1:
      hostName: controller-1
      ansible:
        ansibleHost: ${networkers[controller-1.localdomain]}
      networks:
      - defaultRoute: true
        fixedIP: ${networkers[controller-1.localdomain]}
        name: ctlplane
        subnetName: subnet1
      - name: internalapi
        subnetName: subnet1
      - name: storage
        subnetName: subnet1
      - name: tenant
        subnetName: subnet1
    controller-2:
      hostName: controller-2
      ansible:
        ansibleHost: ${networkers[controller-2.localdomain]}
      networks:
      - defaultRoute: true
        fixedIP: ${networkers[controller-2.localdomain]}
        name: ctlplane
        subnetName: subnet1
      - name: internalapi
        subnetName: subnet1
      - name: storage
        subnetName: subnet1
      - name: tenant
        subnetName: subnet1
  nodeTemplate:
    ansibleSSHPrivateKeySecret: dataplane-adoption-secret
    ansible:
      ansibleUser: root
      ansibleVarsFrom:
      - secretRef:
          name: subscription-manager
      - secretRef:
          name: redhat-registry
      ansibleVars:
        rhc_release: 9.2
        rhc_repositories:
            - {name: "*", state: disabled}
            - {name: "rhel-9-for-x86_64-baseos-eus-rpms", state: enabled}
            - {name: "rhel-9-for-x86_64-appstream-eus-rpms", state: enabled}
            - {name: "rhel-9-for-x86_64-highavailability-eus-rpms", state: enabled}
            - {name: "rhoso-18.0-for-rhel-9-x86_64-rpms", state: enabled}
            - {name: "fast-datapath-for-rhel-9-x86_64-rpms", state: enabled}
            - {name: "rhceph-7-tools-for-rhel-9-x86_64-rpms", state: enabled}
        edpm_bootstrap_release_version_package: []
        # edpm_network_config
        # Default nic config template for a EDPM node
        # These vars are edpm_network_config role vars
        edpm_network_config_template: |
           ---
           {% set mtu_list = [ctlplane_mtu] %}
           {% for network in nodeset_networks %}
           {{ mtu_list.append(lookup('vars', networks_lower[network] ~ '_mtu')) }}
           {%- endfor %}
           {% set min_viable_mtu = mtu_list | max %}
           network_config:
           - type: ovs_bridge
             name: {{ neutron_physical_bridge_name }}
             mtu: {{ min_viable_mtu }}
             use_dhcp: false
             dns_servers: {{ ctlplane_dns_nameservers }}
             domain: {{ dns_search_domains }}
             addresses:
             - ip_netmask: {{ ctlplane_ip }}/{{ ctlplane_cidr }}
             routes: {{ ctlplane_host_routes }}
             members:
             - type: interface
               name: nic1
               mtu: {{ min_viable_mtu }}
               # force the MAC address of the bridge to this interface
               primary: true
           {% for network in nodeset_networks %}
             - type: vlan
               mtu: {{ lookup('vars', networks_lower[network] ~ '_mtu') }}
               vlan_id: {{ lookup('vars', networks_lower[network] ~ '_vlan_id') }}
               addresses:
               - ip_netmask:
                   {{ lookup('vars', networks_lower[network] ~ '_ip') }}/{{ lookup('vars', networks_lower[network] ~ '_cidr') }}
               routes: {{ lookup('vars', networks_lower[network] ~ '_host_routes') }}
           {% endfor %}

        edpm_network_config_hide_sensitive_logs: false
        #
        # These vars are for the network config templates themselves and are
        # considered EDPM network defaults.
        neutron_physical_bridge_name: br-ctlplane
        neutron_public_interface_name: eth0

        # edpm_nodes_validation
        edpm_nodes_validation_validate_controllers_icmp: false
        edpm_nodes_validation_validate_gateway_icmp: false

        # edpm ovn-controller configuration
        edpm_ovn_bridge_mappings: &lt;bridge_mappings&gt; <i class="conum" data-value="2"></i><b>(2)</b>
        edpm_ovn_bridge: br-int
        edpm_ovn_encap_type: geneve
        ovn_monitor_all: true
        edpm_ovn_remote_probe_interval: 60000
        edpm_ovn_ofctrl_wait_before_clear: 8000

        # serve as a OVN gateway
        edpm_enable_chassis_gw: true <i class="conum" data-value="3"></i><b>(3)</b>

        timesync_ntp_servers:
        - hostname: clock.redhat.com
        - hostname: clock2.redhat.com


        gather_facts: false
        enable_debug: false
        # edpm firewall, change the allowed CIDR if needed
        edpm_sshd_configure_firewall: true
        edpm_sshd_allowed_ranges: ['192.168.122.0/24']
        # SELinux module
        edpm_selinux_mode: enforcing

        # Do not attempt OVS major upgrades here
        edpm_ovs_packages:
        - openvswitch3.3
EOF</pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>If TLS Everywhere is enabled, change <code>spec:tlsEnabled</code> to <code>true</code>.</td>
</tr>
<tr>
<td><i class="conum" data-value="2"></i><b>2</b></td>
<td>Set to the same values that you used in your Red&#160;Hat OpenStack Platform 17.1 deployment.</td>
</tr>
<tr>
<td><i class="conum" data-value="3"></i><b>3</b></td>
<td>Set to <code>true</code> to run <code>ovn-controller</code> in gateway mode.</td>
</tr>
</table>
</div>
</li>
<li>
<p>Ensure that you use the same <code>ovn-controller</code> settings in the <code>OpenStackDataPlaneNodeSet</code> CR that you used in the Networker nodes before adoption. This configuration is stored in the <code>external_ids</code> column in the <code>Open_vSwitch</code> table in the Open vSwitch database:</p>
<div class="listingblock">
<div class="content">
<pre>ovs-vsctl list Open .
...
external_ids        : {hostname=controller-0.localdomain, ovn-bridge=br-int, ovn-bridge-mappings=&lt;bridge_mappings&gt;, ovn-chassis-mac-mappings="datacentre:1e:0a:bb:e6:7c:ad", ovn-cms-options=enable-chassis-as-gw, ovn-encap-ip="172.19.0.100", ovn-encap-tos="0", ovn-encap-type=geneve, ovn-match-northd-version=False, ovn-monitor-all=True, ovn-ofctrl-wait-before-clear="8000", ovn-openflow-probe-interval="60", ovn-remote="tcp:ovsdbserver-sb.openstack.svc:6642", ovn-remote-probe-interval="60000", rundir="/var/run/openvswitch", system-id="2eec68e6-aa21-4c95-a868-31aeafc11736"}
...</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Replace <code>&lt;bridge_mappings&gt;</code> with the value of the bridge mappings in your configuration, for example, <code>"datacentre:br-ctlplane"</code>.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Optional: Enable <code>neutron-metadata</code> in the <code>OpenStackDataPlaneNodeSet</code> CR:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc patch openstackdataplanenodeset &lt;networker_CR_name&gt; --type='json' --patch='[
  {
    "op": "add",
    "path": "/spec/services/-",
    "value": "neutron-metadata"
  }]'</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Replace <code>&lt;networker_CR_name&gt;</code> with the name of the CR that you deployed for your Networker nodes, for example, <code>openstack-networker</code>.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Optional: Enable <code>neutron-dhcp</code> in the <code>OpenStackDataPlaneNodeSet</code> CR:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc patch openstackdataplanenodeset &lt;networker_CR_name&gt; --type='json' --patch='[
  {
    "op": "add",
    "path": "/spec/services/-",
    "value": "neutron-dhcp"
  }]'</pre>
</div>
</div>
</li>
<li>
<p>Run the <code>pre-adoption-validation</code> service for Networker nodes:</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>Create a <code>OpenStackDataPlaneDeployment</code> CR that runs only the validation:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc apply -f - &lt;&lt;EOF
apiVersion: dataplane.openstack.org/v1beta1
kind: OpenStackDataPlaneDeployment
metadata:
  name: openstack-pre-adoption-networker
spec:
  nodeSets:
  - openstack-networker
  servicesOverride:
  - pre-adoption-validation
EOF</pre>
</div>
</div>
</li>
<li>
<p>When the validation is finished, confirm that the status of the Ansible EE pods is <code>Completed</code>:</p>
<div class="listingblock">
<div class="content">
<pre>$ watch oc get pod -l app=openstackansibleee</pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre>$ oc logs -l app=openstackansibleee -f --max-log-requests 20</pre>
</div>
</div>
</li>
<li>
<p>Wait for the deployment to reach the <code>Ready</code> status:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc wait --for condition=Ready openstackdataplanedeployment/openstack-pre-adoption-networker --timeout=10m</pre>
</div>
</div>
</li>
</ol>
</div>
</li>
<li>
<p>Deploy the <code>OpenStackDataPlaneDeployment</code> CR for Networker nodes:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc apply -f - &lt;&lt;EOF
apiVersion: dataplane.openstack.org/v1beta1
kind: OpenStackDataPlaneDeployment
metadata:
  name: openstack-networker
spec:
  nodeSets:
  - openstack-networker
EOF</pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Alternatively, you can include the Networker node set in the <code>nodeSets</code> list before you deploy the main <code>OpenStackDataPlaneDeployment</code> CR. You cannot add new node sets to the <code>OpenStackDataPlaneDeployment</code> CR after deployment.
</td>
</tr>
</table>
</div>
</li>
<li>
<p>Clean up any Networking service (neutron) agents that are no longer running.</p>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
In some cases, agents from the old data plane that are replaced or retired remain in RHOSO. The function these agents provided might be provided by a new agent that is running in RHOSO, or the function might be replaced by other components. For example, DHCP agents might no longer be needed, since OVN DHCP in RHOSO can provide this function.
</td>
</tr>
</table>
</div>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>List the agents:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc exec openstackclient -- openstack network agent list
+--------------------------------------+------------------------------+--------------------------+-------------------+-------+-------+----------------------------+
| ID                                   | Agent Type                   | Host                     | Availability Zone | Alive | State | Binary                     |
+--------------------------------------+------------------------------+--------------------------+-------------------+-------+-------+----------------------------+
| e5075ee0-9dd9-4f0a-a42a-6bbdf1a6111c | OVN Controller Gateway agent | controller-0.localdomain |                   | :-)   | UP    | ovn-controller             |
| 856960f0-5530-46c7-a331-6eadcba362da | DHCP agent                   | controller-1.localdomain | nova              | XXX   | UP    | neutron-dhcp-agent         |
| 8bd22720-789f-45b8-8d7d-006dee862bf9 | DHCP agent                   | controller-2.localdomain | nova              | XXX   | UP    | neutron-dhcp-agent         |
| e584e00d-be4c-4e98-a11a-4ecd87d21be7 | DHCP agent                   | controller-0.localdomain | nova              | XXX   | UP    | neutron-dhcp-agent         |
+--------------------------------------+------------------------------+--------------------------+-------------------+-------+-------+----------------------------+</pre>
</div>
</div>
</li>
<li>
<p>If any agent in the list shows <code>XXX</code> in the <code>Alive</code> field, verify the Host and Agent Type, if the functions of this agent is no longer required, and the agent has been permanently stopped on the Red&#160;Hat OpenStack Platform host. Then, delete the agent:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc exec openstackclient -- openstack network agent &lt;agent_id&gt;</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Replace <code>&lt;agent_id&gt;</code> with the ID of the agent to delete, for example, <code>856960f0-5530-46c7-a331-6eadcba362da</code>.</p>
</li>
</ul>
</div>
</li>
</ol>
</div>
</li>
</ol>
</div>
<div class="olist arabic">
<div class="title">Verification</div>
<ol class="arabic">
<li>
<p>Confirm that all the Ansible EE pods reach a <code>Completed</code> status:</p>
<div class="listingblock">
<div class="content">
<pre>$ watch oc get pod -l app=openstackansibleee</pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre>$ oc logs -l app=openstackansibleee -f --max-log-requests 20</pre>
</div>
</div>
</li>
<li>
<p>Wait for the data plane node set to reach the <code>Ready</code> status:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc wait --for condition=Ready osdpns/&lt;networker_CR_name&gt; --timeout=30m</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Replace <code>&lt;networker_CR_name&gt;</code> with the name of the CR that you deployed for your Networker nodes, for example, <code>openstack-networker</code>.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Verify that the Networking service (neutron) agents are running. The list of agents varies depending on the services you enabled:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc exec openstackclient -- openstack network agent list
+--------------------------------------+------------------------------+--------------------------+-------------------+-------+-------+----------------------------+
| ID                                   | Agent Type                   | Host                     | Availability Zone | Alive | State | Binary                     |
+--------------------------------------+------------------------------+--------------------------+-------------------+-------+-------+----------------------------+
| e5075ee0-9dd9-4f0a-a42a-6bbdf1a6111c | OVN Controller Gateway agent | controller-0.localdomain |                   | :-)   | UP    | ovn-controller             |
| f3112349-054c-403a-b00a-e219238192b8 | OVN Controller agent         | compute-0.localdomain    |                   | :-)   | UP    | ovn-controller             |
| af9dae2d-1c1c-55a8-a743-f84719f6406d | OVN Metadata agent           | compute-0.localdomain    |                   | :-)   | UP    | neutron-ovn-metadata-agent |
| 51a11df8-a66e-47a2-aec0-52eb8589626c | OVN Controller Gateway agent | controller-1.localdomain |                   | :-)   | UP    | ovn-controller             |
| bb817e5e-7832-410a-9e67-934dac8c602f | OVN Controller Gateway agent | controller-2.localdomain |                   | :-)   | UP    | ovn-controller             |
+--------------------------------------+------------------------------+--------------------------+-------------------+-------+-------+----------------------------+</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="migrating-the-object-storage-service_adopt-control-plane">Migrating the Object Storage service to Red&#160;Hat OpenStack Services on OpenShift nodes</h2>
<div class="sectionbody">
<div class="paragraph">
<p>If you are using the Red&#160;Hat OpenStack Platform Object Storage service (swift) as an Object Storage service, you must migrate your Object Storage service to Red&#160;Hat OpenStack Services on OpenShift nodes. If you are using the Object Storage API of the Ceph Object Gateway (RGW), you can skip this chapter.</p>
</div>
<div class="paragraph">
<p>The data migration happens replica by replica. For example, if you have 3 replicas, move them one at a time to ensure that the other 2 replicas are still operational, which enables you to continue to use the Object Storage service during the migration.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Data migration to the new deployment is a long-running process that executes mostly in the background. The Object Storage service replicators move data from old to new nodes, which might take a long time depending on the amount of storage used. To reduce downtime, you can use the old nodes if they are running and continue with adopting other services while waiting for the migration to complete. Performance might be degraded due to the amount of replication traffic in the network.
</td>
</tr>
</table>
</div>
<div class="sect2">
<h3 id="migrating-object-storage-data-to-rhoso-nodes_migrate-object-storage-service">Migrating the Object Storage service data from RHOSP to RHOSO nodes</h3>
<div class="paragraph">
<p>The Object Storage service (swift) migration involves the following steps:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Add new nodes to the Object Storage service rings.</p>
</li>
<li>
<p>Set weights of existing nodes to 0.</p>
</li>
<li>
<p>Rebalance rings by moving one replica.</p>
</li>
<li>
<p>Copy rings to old nodes and restart services.</p>
</li>
<li>
<p>Check replication status and repeat the previous two steps until the old nodes are drained.</p>
</li>
<li>
<p>Remove the old nodes from the rings.</p>
</li>
</ol>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>Adopt the Object Storage service. For more information, see <a href="#adopting-the-object-storage-service_adopt-control-plane">Adopting the Object Storage service</a>.</p>
</li>
<li>
<p>For DNS servers, ensure that all existing nodes are able to resolve the hostnames of the Red Hat OpenShift Container Platform (RHOCP) pods, for example, by using the external IP of the DNSMasq service as the nameserver in <code>/etc/resolv.conf</code>:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc get service dnsmasq-dns -o jsonpath="{.status.loadBalancer.ingress[0].ip}" | $CONTROLLER1_SSH sudo tee /etc/resolv.conf</pre>
</div>
</div>
</li>
<li>
<p>Track the current status of the replication by using the <code>swift-dispersion</code> tool:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc debug --keep-labels=true job/swift-ring-rebalance -- /bin/sh -c 'swift-ring-tool get &amp;&amp; swift-dispersion-populate'</pre>
</div>
</div>
<div class="paragraph">
<p>The command might need a few minutes to complete. It creates 0-byte objects that are distributed across the Object Storage service deployment, and you can use the <code>swift-dispersion-report</code> afterward to show the current replication status:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>$ oc debug --keep-labels=true job/swift-ring-rebalance -- /bin/sh -c 'swift-ring-tool get &amp;&amp; swift-dispersion-report'</pre>
</div>
</div>
<div class="paragraph">
<p>The output of the <code>swift-dispersion-report</code> command looks similar to the following:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>Queried 1024 containers for dispersion reporting, 5s, 0 retries
100.00% of container copies found (3072 of 3072)
Sample represents 100.00% of the container partition space
Queried 1024 objects for dispersion reporting, 4s, 0 retries
There were 1024 partitions missing 0 copies.
100.00% of object copies found (3072 of 3072)
Sample represents 100.00% of the object partition space</pre>
</div>
</div>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Add new nodes by scaling up the SwiftStorage resource from 0 to 3:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc patch openstackcontrolplane openstack --type=merge -p='{"spec":{"swift":{"template":{"swiftStorage":{"replicas": 3}}}}}'</pre>
</div>
</div>
<div class="paragraph">
<p>This command creates three storage instances on the Red Hat OpenShift Container Platform (RHOCP) cluster that use Persistent Volume Claims.</p>
</div>
</li>
<li>
<p>Wait until all three pods are running and the rings include the new devices:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc wait pods --for condition=Ready -l component=swift-storage
$ oc debug --keep-labels=true job/swift-ring-rebalance -- /bin/sh -c 'swift-ring-tool get &amp;&amp; swift-ring-builder object.builder search --device pv'</pre>
</div>
</div>
</li>
<li>
<p>From the current rings, get the storage management IP addresses of the nodes to drain:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc debug --keep-labels=true job/swift-ring-rebalance -- /bin/sh -c 'swift-ring-tool get &amp;&amp; swift-ring-builder object.builder search _' | tail -n +2 | awk '{print $4}' | sort -u</pre>
</div>
</div>
<div class="paragraph">
<p>The output looks similar to the following:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>172.20.0.100
swift-storage-0.swift-storage.openstack.svc
swift-storage-1.swift-storage.openstack.svc
swift-storage-2.swift-storage.openstack.svc</pre>
</div>
</div>
</li>
<li>
<p>Drain the old nodes. In the following example, the old node <code>172.20.0.100</code> is drained:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc debug --keep-labels=true job/swift-ring-rebalance -- /bin/sh -c '
swift-ring-tool get
swift-ring-tool drain 172.20.0.100
swift-ring-tool rebalance
swift-ring-tool push'</pre>
</div>
</div>
<div class="paragraph">
<p>Depending on your deployment, you might have more nodes to include in the command.</p>
</div>
</li>
<li>
<p>Copy and apply the updated rings to the original nodes. Run the
ssh commands for your existing nodes that store the Object Storage service data:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc extract --confirm cm/swift-ring-files
$CONTROLLER1_SSH "tar -C /var/lib/config-data/puppet-generated/swift/etc/swift/ -xzf -" &lt; swiftrings.tar.gz
$CONTROLLER1_SSH "systemctl restart tripleo_swift_*"</pre>
</div>
</div>
</li>
<li>
<p>Track the replication progress by using the <code>swift-dispersion-report</code> tool:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc debug --keep-labels=true job/swift-ring-rebalance -- /bin/sh -c "swift-ring-tool get &amp;&amp; swift-dispersion-report"</pre>
</div>
</div>
<div class="paragraph">
<p>The output shows less than 100% of copies found. Repeat the command until all container and object copies are found:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>Queried 1024 containers for dispersion reporting, 6s, 0 retries
There were 5 partitions missing 1 copy.
99.84% of container copies found (3067 of 3072)
Sample represents 100.00% of the container partition space
Queried 1024 objects for dispersion reporting, 7s, 0 retries
There were 739 partitions missing 1 copy.
There were 285 partitions missing 0 copies.
75.94% of object copies found (2333 of 3072)
Sample represents 100.00% of the object partition space</pre>
</div>
</div>
</li>
<li>
<p>Move the next replica to the new nodes by rebalancing and distributing the rings:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc debug --keep-labels=true job/swift-ring-rebalance -- /bin/sh -c '
swift-ring-tool get
swift-ring-tool rebalance
swift-ring-tool push'

$ oc extract --confirm cm/swift-ring-files
$CONTROLLER1_SSH "tar -C /var/lib/config-data/puppet-generated/swift/etc/swift/ -xzf -" &lt; swiftrings.tar.gz
$CONTROLLER1_SSH "systemctl restart tripleo_swift_*"</pre>
</div>
</div>
<div class="paragraph">
<p>Monitor the <code>swift-dispersion-report</code> output again, wait until all copies are found, and then repeat this step until all your replicas are moved to the new nodes.</p>
</div>
</li>
<li>
<p>Remove the nodes from the rings:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc debug --keep-labels=true job/swift-ring-rebalance -- /bin/sh -c '
swift-ring-tool get
swift-ring-tool remove 172.20.0.100
swift-ring-tool rebalance
swift-ring-tool push'</pre>
</div>
</div>
</li>
</ol>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Even if all replicas are on the new nodes and the <code>swift-dispersion-report</code> command reports 100% of the copies found, there might still be data on the old nodes. The replicators remove this data, but it might take more time.
</td>
</tr>
</table>
</div>
<div class="olist arabic">
<div class="title">Verification</div>
<ol class="arabic">
<li>
<p>Check the disk usage of all disks in the cluster:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc debug --keep-labels=true job/swift-ring-rebalance -- /bin/sh -c 'swift-ring-tool get &amp;&amp; swift-recon -d'</pre>
</div>
</div>
</li>
<li>
<p>Confirm that there are no more <code>\*.db</code> or <code>*.data</code> files in the <code>/srv/node</code> directory on the nodes:</p>
<div class="listingblock">
<div class="content">
<pre>$CONTROLLER1_SSH "find /srv/node/ -type f -name '*.db' -o -name '*.data' | wc -l"</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
<div class="sect2">
<h3 id="troubleshooting-object-storage-migration_migrate-object-storage-service">Troubleshooting the Object Storage service migration</h3>
<div class="paragraph">
<p>You can troubleshoot issues with the Object Storage service (swift) migration.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>If the replication is not working and the <code>swift-dispersion-report</code> is not back to 100% availability, check the replicator progress to help you debug:</p>
<div class="listingblock">
<div class="content">
<pre>$ CONTROLLER1_SSH tail /var/log/containers/swift/swift.log | grep object-server</pre>
</div>
</div>
<div class="paragraph">
<p>The following shows an example of the output:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>Mar 14 06:05:30 standalone object-server[652216]: &lt;f+++++++++ 4e2/9cbea55c47e243994b0b10d8957184e2/1710395823.58025.data
Mar 14 06:05:30 standalone object-server[652216]: Successful rsync of /srv/node/vdd/objects/626/4e2 to swift-storage-1.swift-storage.openstack.svc::object/d1/objects/626 (0.094)
Mar 14 06:05:30 standalone object-server[652216]: Removing partition: /srv/node/vdd/objects/626
Mar 14 06:05:31 standalone object-server[652216]: &lt;f+++++++++ 85f/cf53b5a048e5b19049e05a548cde185f/1710395796.70868.data
Mar 14 06:05:31 standalone object-server[652216]: Successful rsync of /srv/node/vdb/objects/829/85f to swift-storage-2.swift-storage.openstack.svc::object/d1/objects/829 (0.095)
Mar 14 06:05:31 standalone object-server[652216]: Removing partition: /srv/node/vdb/objects/829</pre>
</div>
</div>
</li>
<li>
<p>You can also check the ring consistency and replicator status:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc debug --keep-labels=true job/swift-ring-rebalance -- /bin/sh -c 'swift-ring-tool get &amp;&amp; swift-recon -r --md5'</pre>
</div>
</div>
<div class="paragraph">
<p>The output might show a md5 mismatch until approximately 2 minutes after pushing the new rings. After the 2 minutes, the output looks similar to the following example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>Oldest completion was 2024-03-14 16:53:27 (3 minutes ago) by 172.20.0.100:6000.
Most recent completion was 2024-03-14 16:56:38 (12 seconds ago) by swift-storage-0.swift-storage.openstack.svc:6200.
===============================================================================
[2024-03-14 16:56:50] Checking ring md5sums
4/4 hosts matched, 0 error[s] while checking hosts.</pre>
</div>
</div>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="ceph-migration_adopt-control-plane">Migrating the Red Hat Ceph Storage cluster</h2>
<div class="sectionbody">
<div class="paragraph">
<p>In the context of data plane adoption, where the Red&#160;Hat OpenStack Platform
(RHOSP) services are redeployed in Red Hat OpenShift Container Platform (RHOCP), you migrate a
director-deployed Red Hat Ceph Storage cluster by using a process
called externalizing the Red Hat Ceph Storage cluster.</p>
</div>
<div class="paragraph">
<p>There are two deployment topologies that include an internal Red Hat Ceph Storage
cluster:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>RHOSP includes dedicated Red Hat Ceph Storage nodes to host object
storage daemons (OSDs)</p>
</li>
<li>
<p>Hyperconverged Infrastructure (HCI), where Compute and Storage services are
colocated on hyperconverged nodes</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>In either scenario, there are some Red Hat Ceph Storage processes that are deployed on
RHOSP Controller nodes: Red Hat Ceph Storage monitors, Ceph Object Gateway (RGW),
Rados Block Device (RBD), Ceph Metadata Server (MDS), Ceph Dashboard, and NFS
Ganesha. To migrate your Red Hat Ceph Storage cluster, you must decommission the
Controller nodes and move the Red Hat Ceph Storage daemons to a set of target nodes that are
already part of the Red Hat Ceph Storage cluster.</p>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>Complete the tasks in your Red&#160;Hat OpenStack Platform 17.1 environment. For more information, see <a href="#red-hat-ceph-storage-prerequisites_configuring-network">Red Hat Ceph Storage prerequisites</a>.</p>
</li>
</ul>
</div>
<div class="sect2">
<h3 id="ceph-daemon-cardinality_migrating-ceph">Red Hat Ceph Storage daemon cardinality</h3>
<div class="paragraph">
<p>Red Hat Ceph Storage 7 and later applies strict constraints in the way daemons can be colocated within the same node.
For more information, see the Red Hat Knowledgebase article <a href="https://access.redhat.com/articles/1548993">Red Hat Ceph Storage: Supported configurations</a>.
Your topology depends on the available hardware and the amount of Red Hat Ceph Storage services in the Controller nodes that you retire.
The amount of services that you can migrate depends on the amount of available nodes in the cluster. The following diagrams show the distribution of Red Hat Ceph Storage daemons on Red Hat Ceph Storage nodes where at least 3 nodes are required.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The following scenario includes only RGW and RBD, without the Red Hat Ceph Storage dashboard:</p>
<div class="listingblock">
<div class="content">
<pre>|    |                     |             |
|----|---------------------|-------------|
| osd | mon/mgr/crash      | rgw/ingress |
| osd | mon/mgr/crash      | rgw/ingress |
| osd | mon/mgr/crash      | rgw/ingress |</pre>
</div>
</div>
</li>
<li>
<p>With the Red Hat Ceph Storage dashboard, but without Shared File Systems service (manila), at least 4 nodes are required. The Red Hat Ceph Storage dashboard has no failover:</p>
<div class="listingblock">
<div class="content">
<pre>|     |                     |             |
|-----|---------------------|-------------|
| osd | mon/mgr/crash | rgw/ingress       |
| osd | mon/mgr/crash | rgw/ingress       |
| osd | mon/mgr/crash | dashboard/grafana |
| osd | rgw/ingress   | (free)            |</pre>
</div>
</div>
</li>
<li>
<p>With the Red Hat Ceph Storage dashboard and the Shared File Systems service, a minimum of 5 nodes are required, and the Red Hat Ceph Storage dashboard has no failover:</p>
<div class="listingblock">
<div class="content">
<pre>|     |                     |                         |
|-----|---------------------|-------------------------|
| osd | mon/mgr/crash       | rgw/ingress             |
| osd | mon/mgr/crash       | rgw/ingress             |
| osd | mon/mgr/crash       | mds/ganesha/ingress     |
| osd | rgw/ingress         | mds/ganesha/ingress     |
| osd | mds/ganesha/ingress | dashboard/grafana       |</pre>
</div>
</div>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="migrating-ceph-monitoring_migrating-ceph">Migrating the monitoring stack component to new nodes within an existing Red Hat Ceph Storage cluster</h3>
<div class="paragraph">
<p>The Red Hat Ceph Storage Dashboard module adds web-based monitoring and administration to the
Ceph Manager. With director-deployed Red Hat Ceph Storage, the Red Hat Ceph Storage Dashboard is enabled as part of the overcloud deploy and is composed of the following components:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Ceph Manager module</p>
</li>
<li>
<p>Grafana</p>
</li>
<li>
<p>Prometheus</p>
</li>
<li>
<p>Alertmanager</p>
</li>
<li>
<p>Node exporter</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The Red Hat Ceph Storage Dashboard containers are included through <code>tripleo-container-image-prepare</code> parameters, and high availability (HA) relies
on <code>HAProxy</code> and <code>Pacemaker</code> to be deployed on the Red&#160;Hat OpenStack Platform (RHOSP) environment. For an external Red Hat Ceph Storage cluster, HA is not supported.</p>
</div>
<div class="paragraph">
<p>In this procedure, you migrate and relocate the Ceph Monitoring components to free Controller nodes.</p>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>Complete the tasks in your Red&#160;Hat OpenStack Platform 17.1 environment. For more information, see <a href="#red-hat-ceph-storage-prerequisites_configuring-network">Red Hat Ceph Storage prerequisites</a>.</p>
</li>
</ul>
</div>
<div class="sect3">
<h4 id="migrating-monitoring-stack-to-target-nodes_migrating-ceph-monitoring">Migrating the monitoring stack to the target nodes</h4>
<div class="paragraph">
<p>To migrate the monitoring stack to the target nodes, you add the monitoring label to your existing nodes and update the configuration of each daemon. You do not need to migrate node exporters. These daemons are deployed across
the nodes that are part of the Red Hat Ceph Storage cluster (the placement is *).</p>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>Confirm that the firewall rules are in place and the ports are open for a given monitoring stack service.</p>
</li>
</ul>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Depending on the target nodes and the number of deployed or active daemons, you can either relocate the existing containers to the target nodes, or
select a subset of nodes that host the monitoring stack daemons. High availability (HA) is not supported. Reducing the placement with <code>count: 1</code>  allows you to migrate the existing daemons in a Hyperconverged Infrastructure, or hardware-limited, scenario without impacting other services.
</td>
</tr>
</table>
</div>
<div class="sect4">
<h5 id="migrating-existing-daemons-to-target-nodes_migrating-monitoring-stack">Migrating the existing daemons to the target nodes</h5>
<div class="paragraph">
<p>The following procedure is an example of an environment with 3 Red Hat Ceph Storage nodes or ComputeHCI nodes. This scenario extends the monitoring labels to all the Red Hat Ceph Storage or ComputeHCI nodes that are part of the cluster. This means that you keep 3 placements for the target nodes.</p>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Add the monitoring label to all the Red Hat Ceph Storage or ComputeHCI nodes in the cluster:</p>
<div class="listingblock">
<div class="content">
<pre>for item in $(sudo cephadm shell --  ceph orch host ls --format json | jq -r '.[].hostname'); do
    sudo cephadm shell -- ceph orch host label add  $item monitoring;
done</pre>
</div>
</div>
</li>
<li>
<p>Verify that all the hosts on the target nodes have the monitoring label:</p>
<div class="listingblock">
<div class="content">
<pre>[tripleo-admin@controller-0 ~]$ sudo cephadm shell -- ceph orch host ls

HOST                        ADDR           LABELS
cephstorage-0.redhat.local  192.168.24.11  osd monitoring
cephstorage-1.redhat.local  192.168.24.12  osd monitoring
cephstorage-2.redhat.local  192.168.24.47  osd monitoring
controller-0.redhat.local   192.168.24.35  _admin mon mgr monitoring
controller-1.redhat.local   192.168.24.53  mon _admin mgr monitoring
controller-2.redhat.local   192.168.24.10  mon _admin mgr monitoring</pre>
</div>
</div>
</li>
<li>
<p>Remove the labels from the Controller nodes:</p>
<div class="listingblock">
<div class="content">
<pre>$ for i in 0 1 2; do sudo cephadm shell -- ceph orch host label rm "controller-$i.redhat.local" monitoring; done

Removed label monitoring from host controller-0.redhat.local
Removed label monitoring from host controller-1.redhat.local
Removed label monitoring from host controller-2.redhat.local</pre>
</div>
</div>
</li>
<li>
<p>Dump the current monitoring stack spec:</p>
<div class="listingblock">
<div class="content">
<pre>function export_spec {
    local component="$1"
    local target_dir="$2"
    sudo cephadm shell -- ceph orch ls --export "$component" &gt; "$target_dir/$component"
}

SPEC_DIR=${SPEC_DIR:-"$PWD/ceph_specs"}
mkdir -p ${SPEC_DIR}
for m in grafana prometheus alertmanager; do
    export_spec "$m" "$SPEC_DIR"
done</pre>
</div>
</div>
</li>
<li>
<p>For each daemon, edit the current spec and replace the <code>placement.hosts:</code> section with the <code>placement.label:</code> section, for example:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">service_type: grafana
service_name: grafana
placement:
  label: monitoring
networks:
- 172.17.3.0/24
spec:
  port: 3100</code></pre>
</div>
</div>
<div class="paragraph">
<p>This step also applies to Prometheus and Alertmanager specs.</p>
</div>
</li>
<li>
<p>Apply the new monitoring spec to relocate the monitoring stack daemons:</p>
<div class="listingblock">
<div class="content">
<pre>SPEC_DIR=${SPEC_DIR:-"$PWD/ceph_specs"}
function migrate_daemon {
    local component="$1"
    local target_dir="$2"
    sudo cephadm shell -m "$target_dir" -- ceph orch apply -i /mnt/ceph_specs/$component
}
for m in grafana prometheus alertmanager; do
    migrate_daemon  "$m" "$SPEC_DIR"
done</pre>
</div>
</div>
</li>
<li>
<p>Verify that the daemons are deployed on the expected nodes:</p>
<div class="listingblock">
<div class="content">
<pre>[ceph: root@controller-0 /]# ceph orch ps | grep -iE "(prome|alert|grafa)"
alertmanager.cephstorage-2  cephstorage-2.redhat.local  172.17.3.144:9093,9094
grafana.cephstorage-0       cephstorage-0.redhat.local  172.17.3.83:3100
prometheus.cephstorage-1    cephstorage-1.redhat.local  172.17.3.53:9092</pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
After you migrate the monitoring stack, you lose high availability. The monitoring stack daemons no longer have a Virtual IP address and HAProxy anymore. Node exporters are still running on all the nodes.
</td>
</tr>
</table>
</div>
</li>
<li>
<p>Review the Red Hat Ceph Storage configuration to ensure that it aligns with the configuration on the target nodes. In particular, focus on the following configuration entries:</p>
<div class="listingblock">
<div class="content">
<pre>[ceph: root@controller-0 /]# ceph config dump | grep -i dashboard
...
mgr  advanced  mgr/dashboard/ALERTMANAGER_API_HOST  http://172.17.3.83:9093
mgr  advanced  mgr/dashboard/GRAFANA_API_URL        https://172.17.3.144:3100
mgr  advanced  mgr/dashboard/PROMETHEUS_API_HOST    http://172.17.3.83:9092
mgr  advanced  mgr/dashboard/controller-0.ycokob/server_addr  172.17.3.33
mgr  advanced  mgr/dashboard/controller-1.lmzpuc/server_addr  172.17.3.147
mgr  advanced  mgr/dashboard/controller-2.xpdgfl/server_addr  172.17.3.138</pre>
</div>
</div>
</li>
<li>
<p>Verify that the <code>API_HOST/URL</code> of the <code>grafana</code>, <code>alertmanager</code> and <code>prometheus</code> services points to the IP addresses on the storage network of the node where each daemon is relocated:</p>
<div class="listingblock">
<div class="content">
<pre>[ceph: root@controller-0 /]# ceph orch ps | grep -iE "(prome|alert|grafa)"
alertmanager.cephstorage-0  cephstorage-0.redhat.local  172.17.3.83:9093,9094
alertmanager.cephstorage-1  cephstorage-1.redhat.local  172.17.3.53:9093,9094
alertmanager.cephstorage-2  cephstorage-2.redhat.local  172.17.3.144:9093,9094
grafana.cephstorage-0       cephstorage-0.redhat.local  172.17.3.83:3100
grafana.cephstorage-1       cephstorage-1.redhat.local  172.17.3.53:3100
grafana.cephstorage-2       cephstorage-2.redhat.local  172.17.3.144:3100
prometheus.cephstorage-0    cephstorage-0.redhat.local  172.17.3.83:9092
prometheus.cephstorage-1    cephstorage-1.redhat.local  172.17.3.53:9092
prometheus.cephstorage-2    cephstorage-2.redhat.local  172.17.3.144:9092</pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre>[ceph: root@controller-0 /]# ceph config dump
...
...
mgr  advanced  mgr/dashboard/ALERTMANAGER_API_HOST   http://172.17.3.83:9093
mgr  advanced  mgr/dashboard/PROMETHEUS_API_HOST     http://172.17.3.83:9092
mgr  advanced  mgr/dashboard/GRAFANA_API_URL         https://172.17.3.144:3100</pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
The Ceph Dashboard, as the service  provided by the Ceph <code>mgr</code>, is not impacted by the relocation. You might experience an impact when the active <code>mgr</code> daemon is migrated or is force-failed. However, you can define 3 replicas in the Ceph Manager configuration to redirect requests to a different instance.
</td>
</tr>
</table>
</div>
</li>
</ol>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="migrating-ceph-mds_migrating-ceph-monitoring">Migrating Red Hat Ceph Storage MDS to new nodes within the existing cluster</h3>
<div class="paragraph">
<p>You can migrate the MDS daemon when Shared File Systems service (manila), deployed with either a cephfs-native or ceph-nfs back end, is part of the overcloud deployment. The MDS migration is performed by <code>cephadm</code>, and you move the daemons placement from a hosts-based approach to a label-based approach.
This ensures that you can visualize the status of the cluster and where daemons are placed by using the <code>ceph orch host</code> command. You can also have a general view of how the daemons are co-located within a given host, as described in the Red Hat Knowledgebase article <a href="https://access.redhat.com/articles/1548993">Red Hat Ceph Storage: Supported configurations</a>.</p>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>Complete the tasks in your Red&#160;Hat OpenStack Platform 17.1 environment. For more information, see <a href="#red-hat-ceph-storage-prerequisites_configuring-network">Red Hat Ceph Storage prerequisites</a>.</p>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Verify that the Red Hat Ceph Storage cluster is healthy and check the MDS status:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo cephadm shell -- ceph fs ls
name: cephfs, metadata pool: manila_metadata, data pools: [manila_data ]

$ sudo cephadm shell -- ceph mds stat
cephfs:1 {0=mds.controller-2.oebubl=up:active} 2 up:standby

$ sudo cephadm shell -- ceph fs status cephfs

cephfs - 0 clients
======
RANK  STATE         	MDS           	ACTIVITY 	DNS	INOS   DIRS   CAPS
 0	active  mds.controller-2.oebubl  Reqs:	0 /s   696	196	173  	0
  	POOL     	TYPE 	USED  AVAIL
manila_metadata  metadata   152M   141G
  manila_data  	data	3072M   141G
  	STANDBY MDS
mds.controller-0.anwiwd
mds.controller-1.cwzhog</pre>
</div>
</div>
</li>
<li>
<p>Retrieve more detailed information on the Ceph File System (CephFS) MDS status:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo cephadm shell -- ceph fs dump

e8
enable_multiple, ever_enabled_multiple: 1,1
default compat: compat={},rocompat={},incompat={1=base v0.20,2=client writeable ranges,3=default file layouts on dirs,4=dir inode in separate object,5=mds uses versioned encoding,6=dirfrag is stored in omap,8=no anchor table,9=file layout v2,10=snaprealm v2}
legacy client fscid: 1

Filesystem 'cephfs' (1)
fs_name cephfs
epoch   5
flags   12 joinable allow_snaps allow_multimds_snaps
created 2024-01-18T19:04:01.633820+0000
modified    	2024-01-18T19:04:05.393046+0000
tableserver 	0
root	0
session_timeout 60
session_autoclose   	300
max_file_size   1099511627776
required_client_features    	{}
last_failure	0
last_failure_osd_epoch  0
compat  compat={},rocompat={},incompat={1=base v0.20,2=client writeable ranges,3=default file layouts on dirs,4=dir inode in separate object,5=mds uses versioned encoding,6=dirfrag is stored in omap,7=mds uses inline data,8=no anchor table,9=file layout v2,10=snaprealm v2}
max_mds 1
in  	0
up  	{0=24553}
failed
damaged
stopped
data_pools  	[7]
metadata_pool   9
inline_data 	disabled
balancer
standby_count_wanted	1
[mds.mds.controller-2.oebubl{0:24553} state up:active seq 2 addr [v2:172.17.3.114:6800/680266012,v1:172.17.3.114:6801/680266012] compat {c=[1],r=[1],i=[7ff]}]


Standby daemons:

[mds.mds.controller-0.anwiwd{-1:14715} state up:standby seq 1 addr [v2:172.17.3.20:6802/3969145800,v1:172.17.3.20:6803/3969145800] compat {c=[1],r=[1],i=[7ff]}]
[mds.mds.controller-1.cwzhog{-1:24566} state up:standby seq 1 addr [v2:172.17.3.43:6800/2227381308,v1:172.17.3.43:6801/2227381308] compat {c=[1],r=[1],i=[7ff]}]
dumped fsmap epoch 8</pre>
</div>
</div>
</li>
<li>
<p>Check the OSD blocklist and clean up the client list:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo cephadm shell -- ceph osd blocklist ls
..
..
for item in $(sudo cephadm shell -- ceph osd blocklist ls | awk '{print $1}'); do
     sudo cephadm shell -- ceph osd blocklist rm $item;
done</pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>When a file system client is unresponsive or misbehaving, the access to the file system might be forcibly terminated. This process is called eviction. Evicting a CephFS client prevents it from communicating further with MDS daemons and OSD daemons.</p>
</div>
<div class="paragraph">
<p>Ordinarily, a blocklisted client cannot reconnect to the servers; you must unmount and then remount the client. However, permitting a client that was evicted to attempt to reconnect can be useful. Because CephFS uses the RADOS OSD blocklist to control client eviction, you can permit CephFS clients to reconnect by removing them from the blocklist.</p>
</div>
</td>
</tr>
</table>
</div>
</li>
<li>
<p>Get the hosts that are currently part of the Red Hat Ceph Storage cluster:</p>
<div class="listingblock">
<div class="content">
<pre>[ceph: root@controller-0 /]# ceph orch host ls
HOST                        ADDR           LABELS          STATUS
cephstorage-0.redhat.local  192.168.24.25  osd
cephstorage-1.redhat.local  192.168.24.50  osd
cephstorage-2.redhat.local  192.168.24.47  osd
controller-0.redhat.local   192.168.24.24  _admin mgr mon
controller-1.redhat.local   192.168.24.42  mgr _admin mon
controller-2.redhat.local   192.168.24.37  mgr _admin mon
6 hosts in cluster</pre>
</div>
</div>
</li>
<li>
<p>Apply the MDS labels to the target nodes:</p>
<div class="listingblock">
<div class="content">
<pre>for item in $(sudo cephadm shell --  ceph orch host ls --format json | jq -r '.[].hostname'); do
    sudo cephadm shell -- ceph orch host label add  $item mds;
done</pre>
</div>
</div>
</li>
<li>
<p>Verify that all the hosts have the MDS label:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo cephadm shell -- ceph orch host ls

HOST                    	ADDR       	   LABELS
cephstorage-0.redhat.local  192.168.24.11  osd mds
cephstorage-1.redhat.local  192.168.24.12  osd mds
cephstorage-2.redhat.local  192.168.24.47  osd mds
controller-0.redhat.local   192.168.24.35  _admin mon mgr mds
controller-1.redhat.local   192.168.24.53  mon _admin mgr mds
controller-2.redhat.local   192.168.24.10  mon _admin mgr mds</pre>
</div>
</div>
</li>
<li>
<p>Dump the current MDS spec:</p>
<div class="listingblock">
<div class="content">
<pre>$ SPEC_DIR=${SPEC_DIR:-"$PWD/ceph_specs"}
$ mkdir -p ${SPEC_DIR}
$ sudo cephadm shell -- ceph orch ls --export mds &gt; ${SPEC_DIR}/mds</pre>
</div>
</div>
</li>
<li>
<p>Edit the retrieved spec and replace the <code>placement.hosts</code> section with
<code>placement.label</code>:</p>
<div class="listingblock">
<div class="content">
<pre>service_type: mds
service_id: mds
service_name: mds.mds
placement:
  label: mds</pre>
</div>
</div>
</li>
<li>
<p>Use the <code>ceph orchestrator</code> to apply the new MDS spec:</p>
<div class="listingblock">
<div class="content">
<pre>$ SPEC_DIR=${SPEC_DIR:-"$PWD/ceph_specs"}
$ sudo cephadm shell -m ${SPEC_DIR}/mds -- ceph orch apply -i /mnt/mds

Scheduling new mds deployment ...</pre>
</div>
</div>
<div class="paragraph">
<p>This results in an increased number of MDS daemons.</p>
</div>
</li>
<li>
<p>Check the new standby daemons that are temporarily added to the CephFS:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo cephadm shell -- ceph fs dump

Active

standby_count_wanted    1
[mds.mds.controller-0.awzplm{0:463158} state up:active seq 307 join_fscid=1 addr [v2:172.17.3.20:6802/51565420,v1:172.17.3.20:6803/51565420] compat {c=[1],r=[1],i=[7ff]}]


Standby daemons:

[mds.mds.cephstorage-1.jkvomp{-1:463800} state up:standby seq 1 join_fscid=1 addr [v2:172.17.3.135:6820/2075903648,v1:172.17.3.135:6821/2075903648] compat {c=[1],r=[1],i=[7ff]}]
[mds.mds.controller-2.gfrqvc{-1:475945} state up:standby seq 1 addr [v2:172.17.3.114:6800/2452517189,v1:172.17.3.114:6801/2452517189] compat {c=[1],r=[1],i=[7ff]}]
[mds.mds.cephstorage-0.fqcshx{-1:476503} state up:standby seq 1 join_fscid=1 addr [v2:172.17.3.92:6820/4120523799,v1:172.17.3.92:6821/4120523799] compat {c=[1],r=[1],i=[7ff]}]
[mds.mds.cephstorage-2.gnfhfe{-1:499067} state up:standby seq 1 addr [v2:172.17.3.79:6820/2448613348,v1:172.17.3.79:6821/2448613348] compat {c=[1],r=[1],i=[7ff]}]
[mds.mds.controller-1.tyiziq{-1:499136} state up:standby seq 1 addr [v2:172.17.3.43:6800/3615018301,v1:172.17.3.43:6801/3615018301] compat {c=[1],r=[1],i=[7ff]}]</pre>
</div>
</div>
</li>
<li>
<p>To migrate MDS to the target nodes, set the MDS affinity that manages the MDS failover:</p>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
It is possible to elect a dedicated MDS as "active" for a particular file system. To configure this preference, <code>CephFS</code> provides a configuration option for MDS called <code>mds_join_fs</code>, which enforces this affinity.
When failing over MDS daemons, cluster monitors prefer standby daemons with <code>mds_join_fs</code> equal to the file system name with the failed rank. If no standby exists with <code>mds_join_fs</code> equal to the file system name, it chooses an unqualified standby as a replacement.
</td>
</tr>
</table>
</div>
<div class="listingblock">
<div class="content">
<pre>$ sudo cephadm shell -- ceph config set mds.mds.cephstorage-0.fqcshx mds_join_fs cephfs</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Replace <code>mds.mds.cephstorage-0.fqcshx</code> with the daemon deployed on
<code>cephstorage-0</code> that was retrieved from the previous step.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Remove the labels from the Controller nodes and force the MDS failover to the
target node:</p>
<div class="listingblock">
<div class="content">
<pre>$ for i in 0 1 2; do sudo cephadm shell -- ceph orch host label rm "controller-$i.redhat.local" mds; done

Removed label mds from host controller-0.redhat.local
Removed label mds from host controller-1.redhat.local
Removed label mds from host controller-2.redhat.local</pre>
</div>
</div>
<div class="paragraph">
<p>The switch to the target node happens in the background. The new active MDS is the one that you set by using the <code>mds_join_fs</code> command.</p>
</div>
</li>
<li>
<p>Check the result of the failover and the new deployed daemons:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo cephadm shell -- ceph fs dump


standby_count_wanted    1
[mds.mds.cephstorage-0.fqcshx{0:476503} state up:active seq 168 join_fscid=1 addr [v2:172.17.3.92:6820/4120523799,v1:172.17.3.92:6821/4120523799] compat {c=[1],r=[1],i=[7ff]}]


Standby daemons:

[mds.mds.cephstorage-2.gnfhfe{-1:499067} state up:standby seq 1 addr [v2:172.17.3.79:6820/2448613348,v1:172.17.3.79:6821/2448613348] compat {c=[1],r=[1],i=[7ff]}]
[mds.mds.cephstorage-1.jkvomp{-1:499760} state up:standby seq 1 join_fscid=1 addr [v2:172.17.3.135:6820/452139733,v1:172.17.3.135:6821/452139733] compat {c=[1],r=[1],i=[7ff]}]


$ sudo cephadm shell -- ceph orch ls

NAME                     PORTS   RUNNING  REFRESHED  AGE  PLACEMENT
crash                                6/6  10m ago    10d  *
mds.mds                          3/3  10m ago    32m  label:mds


$ sudo cephadm shell -- ceph orch ps | grep mds


mds.mds.cephstorage-0.fqcshx  cephstorage-0.redhat.local                     running (79m)     3m ago  79m    27.2M        -  17.2.6-100.el9cp  1af7b794f353  2a2dc5ba6d57
mds.mds.cephstorage-1.jkvomp  cephstorage-1.redhat.local                     running (79m)     3m ago  79m    21.5M        -  17.2.6-100.el9cp  1af7b794f353  7198b87104c8
mds.mds.cephstorage-2.gnfhfe  cephstorage-2.redhat.local                     running (79m)     3m ago  79m    24.2M        -  17.2.6-100.el9cp  1af7b794f353  f3cb859e2a15</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
<div class="sect2">
<h3 id="migrating-ceph-rgw_migrating-ceph-monitoring">Migrating Red Hat Ceph Storage RGW to external RHEL nodes</h3>
<div class="paragraph">
<p>For Hyperconverged Infrastructure (HCI) or dedicated Storage nodes, you must migrate the Ceph Object Gateway (RGW) daemons that are included in the Red&#160;Hat OpenStack Platform Controller nodes into the existing external Red Hat Enterprise Linux (RHEL) nodes. The external RHEL nodes typically include the Compute nodes for an HCI environment or Red Hat Ceph Storage nodes. Your environment must have Red Hat Ceph Storage 7 or later and be managed by <code>cephadm</code> or Ceph Orchestrator.</p>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>Complete the tasks in your Red&#160;Hat OpenStack Platform 17.1 environment. For more information, see <a href="#red-hat-ceph-storage-prerequisites_configuring-network">Red Hat Ceph Storage prerequisites</a>.</p>
</li>
</ul>
</div>
<div class="sect3">
<h4 id="migrating-the-rgw-backends_migrating-ceph-rgw">Migrating the Red Hat Ceph Storage RGW back ends</h4>
<div class="paragraph">
<p>You must migrate your Ceph Object Gateway (RGW) back ends from your Controller nodes to your Red Hat Ceph Storage nodes. To ensure that you distribute the correct amount of services to your available nodes, you use <code>cephadm</code> labels to refer to a group of nodes where a given daemon type is deployed. For more information about the cardinality diagram, see <a href="#ceph-daemon-cardinality_migrating-ceph">Red Hat Ceph Storage daemon cardinality</a>.
The following procedure assumes that you have three target nodes, <code>cephstorage-0</code>, <code>cephstorage-1</code>, <code>cephstorage-2</code>.</p>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Add the RGW label to the Red Hat Ceph Storage nodes that you want to migrate your RGW back ends to:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo cephadm shell -- ceph orch host label add cephstorage-0 rgw;
$ sudo cephadm shell -- ceph orch host label add cephstorage-1 rgw;
$ sudo cephadm shell -- ceph orch host label add cephstorage-2 rgw;

Added label rgw to host cephstorage-0
Added label rgw to host cephstorage-1
Added label rgw to host cephstorage-2

$ sudo cephadm shell -- ceph orch host ls

HOST       	ADDR       	LABELS      	STATUS
cephstorage-0  192.168.24.54  osd rgw
cephstorage-1  192.168.24.44  osd rgw
cephstorage-2  192.168.24.30  osd rgw
controller-0   192.168.24.45  _admin mon mgr
controller-1   192.168.24.11  _admin mon mgr
controller-2   192.168.24.38  _admin mon mgr

6 hosts in cluster</pre>
</div>
</div>
</li>
<li>
<p>Locate the RGW spec and dump in the spec directory:</p>
<div class="listingblock">
<div class="content">
<pre>$ SPEC_DIR=${SPEC_DIR:-"$PWD/ceph_specs"}
$ mkdir -p ${SPEC_DIR}
$ sudo cephadm shell -- ceph orch ls --export rgw &gt; ${SPEC_DIR}/rgw
$ cat ${SPEC_DIR}/rgw

networks:
- 172.17.3.0/24
placement:
  hosts:
  - controller-0
  - controller-1
  - controller-2
service_id: rgw
service_name: rgw.rgw
service_type: rgw
spec:
  rgw_frontend_port: 8080
  rgw_realm: default
  rgw_zone: default</pre>
</div>
</div>
<div class="paragraph">
<p>This example assumes that <code>172.17.3.0/24</code> is the <code>storage</code> network.</p>
</div>
</li>
<li>
<p>In the <code>placement</code> section, ensure that the <code>label</code> and <code>rgw_frontend_port</code> values are set:</p>
<div class="listingblock">
<div class="content">
<pre>---
networks:
- 172.17.3.0/24<i class="conum" data-value="1"></i><b>(1)</b>
placement:
  label: rgw <i class="conum" data-value="2"></i><b>(2)</b>
service_id: rgw
service_name: rgw.rgw
service_type: rgw
spec:
  rgw_frontend_port: 8090 <i class="conum" data-value="3"></i><b>(3)</b>
  rgw_realm: default
  rgw_zone: default
  rgw_frontend_ssl_certificate: ... <i class="conum" data-value="4"></i><b>(4)</b>
  ssl: true</pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>Add the storage network where the RGW back ends are deployed.</td>
</tr>
<tr>
<td><i class="conum" data-value="2"></i><b>2</b></td>
<td>Replace the Controller nodes with the <code>label: rgw</code> label.</td>
</tr>
<tr>
<td><i class="conum" data-value="3"></i><b>3</b></td>
<td>Change the <code>rgw_frontend_port</code> value to <code>8090</code> to avoid conflicts with the Ceph ingress daemon.</td>
</tr>
<tr>
<td><i class="conum" data-value="4"></i><b>4</b></td>
<td>Optional: if TLS is enabled, add the SSL certificate and key concatenation as described in <a href="https://docs.redhat.com/en/documentation/red_hat_openstack_services_on_openshift/18.0/html/configuring_persistent_storage/assembly_configuring-red-hat-ceph-storage-as-the-backend-for-rhosp-storage#proc_ceph-configure-rgw-with-tls_ceph-back-end">Configuring RGW with TLS for an external Red Hat Ceph Storage cluster</a> in <em>Configuring persistent storage</em>.</td>
</tr>
</table>
</div>
</li>
<li>
<p>Apply the new RGW spec by using the orchestrator CLI:</p>
<div class="listingblock">
<div class="content">
<pre>$ SPEC_DIR=${SPEC_DIR:-"$PWD/ceph_specs"}
$ sudo cephadm shell -m ${SPEC_DIR}/rgw -- ceph orch apply -i /mnt/rgw</pre>
</div>
</div>
<div class="paragraph">
<p>This command triggers the redeploy, for example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>...
osd.9                     	cephstorage-2
rgw.rgw.cephstorage-0.wsjlgx  cephstorage-0  172.17.3.23:8090   starting
rgw.rgw.cephstorage-1.qynkan  cephstorage-1  172.17.3.26:8090   starting
rgw.rgw.cephstorage-2.krycit  cephstorage-2  172.17.3.81:8090   starting
rgw.rgw.controller-1.eyvrzw   controller-1   172.17.3.146:8080  running (5h)
rgw.rgw.controller-2.navbxa   controller-2   172.17.3.66:8080   running (5h)

...
osd.9                     	cephstorage-2
rgw.rgw.cephstorage-0.wsjlgx  cephstorage-0  172.17.3.23:8090  running (19s)
rgw.rgw.cephstorage-1.qynkan  cephstorage-1  172.17.3.26:8090  running (16s)
rgw.rgw.cephstorage-2.krycit  cephstorage-2  172.17.3.81:8090  running (13s)</pre>
</div>
</div>
</li>
<li>
<p>Ensure that the new RGW back ends are reachable on the new ports, so you can enable an ingress daemon on port <code>8080</code> later. Log in to each Red Hat Ceph Storage node that includes RGW and add the <code>iptables</code> rule to allow connections to both 8080 and 8090 ports in the Red Hat Ceph Storage nodes:</p>
<div class="listingblock">
<div class="content">
<pre>$ iptables -I INPUT -p tcp -m tcp --dport 8080 -m conntrack --ctstate NEW -m comment --comment "ceph rgw ingress" -j ACCEPT
$ iptables -I INPUT -p tcp -m tcp --dport 8090 -m conntrack --ctstate NEW -m comment --comment "ceph rgw backends" -j ACCEPT
$ sudo iptables-save
$ sudo systemctl restart iptables</pre>
</div>
</div>
</li>
<li>
<p>If <code>nftables</code> is used in the existing deployment, edit <code>/etc/nftables/tripleo-rules.nft</code>
and add the following content:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml"># 100 ceph_rgw {'dport': ['8080','8090']}
add rule inet filter TRIPLEO_INPUT tcp dport { 8080,8090 } ct state new counter accept comment "100 ceph_rgw"</code></pre>
</div>
</div>
</li>
<li>
<p>Save the file.</p>
</li>
<li>
<p>Restart the <code>nftables</code> service:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo systemctl restart nftables</pre>
</div>
</div>
</li>
<li>
<p>Verify that the rules are applied:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo nft list ruleset | grep ceph_rgw</pre>
</div>
</div>
</li>
<li>
<p>From a Controller node, such as <code>controller-0</code>, try to reach the RGW back ends:</p>
<div class="listingblock">
<div class="content">
<pre>$ curl http://cephstorage-0.storage:8090;</pre>
</div>
</div>
<div class="paragraph">
<p>You should observe the following output:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;ListAllMyBucketsResult xmlns="http://s3.amazonaws.com/doc/2006-03-01/"&gt;&lt;Owner&gt;&lt;ID&gt;anonymous&lt;/ID&gt;&lt;DisplayName&gt;&lt;/DisplayName&gt;&lt;/Owner&gt;&lt;Buckets&gt;&lt;/Buckets&gt;&lt;/ListAllMyBucketsResult&gt;</pre>
</div>
</div>
<div class="paragraph">
<p>Repeat the verification for each node where a RGW daemon is deployed.</p>
</div>
</li>
<li>
<p>If you migrated RGW back ends to the Red Hat Ceph Storage nodes, there is no <code>internalAPI</code> network, except in the case of HCI nodes. You must reconfigure the RGW keystone endpoint to point to the external network that you propagated:</p>
<div class="listingblock">
<div class="content">
<pre>[ceph: root@controller-0 /]# ceph config dump | grep keystone
global   basic rgw_keystone_url  http://172.16.1.111:5000

[ceph: root@controller-0 /]# ceph config set global rgw_keystone_url http://&lt;keystone_endpoint&gt;:5000</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Replace <code>&lt;keystone_endpoint&gt;</code> with the Identity service (keystone) internal endpoint of the service that is deployed in the <code>OpenStackControlPlane</code> CR when you adopt the Identity service. For more information, see <a href="#adopting-the-identity-service_adopt-control-plane">Adopting the Identity service</a>.</p>
</li>
</ul>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="deploying-a-ceph-ingress-daemon_migrating-ceph-rgw">Deploying a Red Hat Ceph Storage ingress daemon</h4>
<div class="paragraph">
<p>To deploy the Ceph ingress daemon, you perform the following actions:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Remove the existing <code>ceph_rgw</code> configuration.</p>
</li>
<li>
<p>Clean up the configuration created by director.</p>
</li>
<li>
<p>Redeploy the Object Storage service (swift).</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>When you deploy the ingress daemon, two new containers are created:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>HAProxy, which you use to reach the back ends.</p>
</li>
<li>
<p>Keepalived, which you use to own the virtual IP address.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>You use the <code>rgw</code> label to distribute the ingress daemon to only the number of nodes that host Ceph Object Gateway (RGW) daemons. For more information about distributing daemons among your nodes, see <a href="#ceph-daemon-cardinality_migrating-ceph">Red Hat Ceph Storage daemon cardinality</a>.</p>
</div>
<div class="paragraph">
<p>After you complete this procedure, you can reach the RGW back end from the ingress daemon and use RGW through the Object Storage service CLI.</p>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Log in to each Controller node and remove the following configuration from the <code>/var/lib/config-data/puppet-generated/haproxy/etc/haproxy/haproxy.cfg</code> file:</p>
<div class="listingblock">
<div class="content">
<pre>listen ceph_rgw
  bind 10.0.0.103:8080 transparent
  mode http
  balance leastconn
  http-request set-header X-Forwarded-Proto https if { ssl_fc }
  http-request set-header X-Forwarded-Proto http if !{ ssl_fc }
  http-request set-header X-Forwarded-Port %[dst_port]
  option httpchk GET /swift/healthcheck
  option httplog
  option forwardfor
   server controller-0.storage.redhat.local 172.17.3.73:8080 check fall 5 inter 2000 rise 2
  server controller-1.storage.redhat.local 172.17.3.146:8080 check fall 5 inter 2000 rise 2
  server controller-2.storage.redhat.local 172.17.3.156:8080 check fall 5 inter 2000 rise 2</pre>
</div>
</div>
</li>
<li>
<p>Restart <code>haproxy-bundle</code> and confirm that it is started:</p>
<div class="listingblock">
<div class="content">
<pre>[root@controller-0 ~]# sudo pcs resource restart haproxy-bundle
haproxy-bundle successfully restarted


[root@controller-0 ~]# sudo pcs status | grep haproxy

  * Container bundle set: haproxy-bundle [undercloud-0.ctlplane.redhat.local:8787/rh-osbs/rhosp17-openstack-haproxy:pcmklatest]:
    * haproxy-bundle-podman-0   (ocf:heartbeat:podman):  Started controller-0
    * haproxy-bundle-podman-1   (ocf:heartbeat:podman):  Started controller-1
    * haproxy-bundle-podman-2   (ocf:heartbeat:podman):  Started controller-2</pre>
</div>
</div>
</li>
<li>
<p>Confirm that no process is connected to port 8080:</p>
<div class="listingblock">
<div class="content">
<pre>[root@controller-0 ~]# ss -antop | grep 8080
[root@controller-0 ~]#</pre>
</div>
</div>
<div class="paragraph">
<p>You can expect the Object Storage service (swift) CLI to fail to establish the connection:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>(overcloud) [root@cephstorage-0 ~]# swift list

HTTPConnectionPool(host='10.0.0.103', port=8080): Max retries exceeded with url: /swift/v1/AUTH_852f24425bb54fa896476af48cbe35d3?format=json (Caused by NewConnectionError('&lt;urllib3.connection.HTTPConnection object at 0x7fc41beb0430&gt;: Failed to establish a new connection: [Errno 111] Connection refused'))</pre>
</div>
</div>
</li>
<li>
<p>Set the required images for both HAProxy and Keepalived:</p>
<div class="listingblock">
<div class="content">
<pre>[ceph: root@controller-0 /]# ceph config set mgr mgr/cephadm/container_image_haproxy registry.redhat.io/rhceph/rhceph-haproxy-rhel9:latest
[ceph: root@controller-0 /]# ceph config set mgr mgr/cephadm/container_image_keepalived registry.redhat.io/rhceph/keepalived-rhel9:latest</pre>
</div>
</div>
</li>
<li>
<p>Create a file called <code>rgw_ingress</code> in <code>controller-0</code>:</p>
<div class="listingblock">
<div class="content">
<pre>$ SPEC_DIR=${SPEC_DIR:-"$PWD/ceph_specs"}
$ vim ${SPEC_DIR}/rgw_ingress</pre>
</div>
</div>
</li>
<li>
<p>Paste the following content into the <code>rgw_ingress</code> file:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">---
service_type: ingress
service_id: rgw.rgw
placement:
  label: rgw
spec:
  backend_service: rgw.rgw
  virtual_ip: 10.0.0.89/24
  frontend_port: 8080
  monitor_port: 8898
  virtual_interface_networks:
    - &lt;external_network&gt;
  ssl_cert: ...</code></pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Replace <code>&lt;external_network&gt;</code> with your external network, for example, <code>10.0.0.0/24</code>. For more information, see <a href="#completing-prerequisites-for-migrating-ceph-rgw_ceph-prerequisites">Completing prerequisites for migrating Red Hat Ceph Storage RGW</a>.</p>
</li>
<li>
<p>If TLS is enabled, add the  SSL certificate and key concatenation as described in <a href="https://docs.redhat.com/en/documentation/red_hat_openstack_services_on_openshift/18.0/html/configuring_persistent_storage/assembly_configuring-red-hat-ceph-storage-as-the-backend-for-rhosp-storage#proc_ceph-configure-rgw-with-tls_ceph-back-end">Configuring RGW with TLS for an external Red Hat Ceph Storage cluster</a> in <em>Configuring persistent storage</em>.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Apply the <code>rgw_ingress</code> spec by using the Ceph orchestrator CLI:</p>
<div class="listingblock">
<div class="content">
<pre>$ SPEC_DIR=${SPEC_DIR:-"$PWD/ceph_specs"}
$ cephadm shell -m ${SPEC_DIR}/rgw_ingress -- ceph orch apply -i /mnt/rgw_ingress</pre>
</div>
</div>
</li>
<li>
<p>Wait until the ingress is deployed and query the resulting endpoint:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo cephadm shell -- ceph orch ls

NAME                 	PORTS            	RUNNING  REFRESHED  AGE  PLACEMENT
crash                                         	6/6  6m ago 	3d   *
ingress.rgw.rgw      	10.0.0.89:8080,8898  	6/6  37s ago	60s  label:rgw
mds.mds                   3/3  6m ago 	3d   controller-0;controller-1;controller-2
mgr                       3/3  6m ago 	3d   controller-0;controller-1;controller-2
mon                       3/3  6m ago 	3d   controller-0;controller-1;controller-2
osd.default_drive_group   15  37s ago	3d   cephstorage-0;cephstorage-1;cephstorage-2
rgw.rgw   ?:8090          3/3  37s ago	4m   label:rgw</pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre>$ curl 10.0.0.89:8080

---
&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;ListAllMyBucketsResult xmlns="http://s3.amazonaws.com/doc/2006-03-01/"&gt;&lt;Owner&gt;&lt;ID&gt;anonymous&lt;/ID&gt;&lt;DisplayName&gt;&lt;/DisplayName&gt;&lt;/Owner&gt;&lt;Buckets&gt;&lt;/Buckets&gt;&lt;/ListAllMyBucketsResult&gt;[ceph: root@controller-0 /]#
</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="updating-the-object-storage-endpoints_migrating-ceph-rgw">Updating the Object Storage service endpoints</h4>
<div class="paragraph">
<p>You must update the Object Storage service (swift) endpoints to point to the new virtual IP address (VIP) that you reserved on the same network that you used to deploy RGW ingress.</p>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>List the current endpoints:</p>
<div class="listingblock">
<div class="content">
<pre>(overcloud) [stack@undercloud-0 ~]$ openstack endpoint list | grep object

| 1326241fb6b6494282a86768311f48d1 | regionOne | swift    	| object-store   | True	| internal  | http://172.17.3.68:8080/swift/v1/AUTH_%(project_id)s |
| 8a34817a9d3443e2af55e108d63bb02b | regionOne | swift    	| object-store   | True	| public	| http://10.0.0.103:8080/swift/v1/AUTH_%(project_id)s  |
| fa72f8b8b24e448a8d4d1caaeaa7ac58 | regionOne | swift    	| object-store   | True	| admin 	| http://172.17.3.68:8080/swift/v1/AUTH_%(project_id)s |</pre>
</div>
</div>
</li>
<li>
<p>Update the endpoints that are pointing to the Ingress VIP:</p>
<div class="listingblock">
<div class="content">
<pre>(overcloud) [stack@undercloud-0 ~]$ openstack endpoint set --url "http://10.0.0.89:8080/swift/v1/AUTH_%(project_id)s" 95596a2d92c74c15b83325a11a4f07a3

(overcloud) [stack@undercloud-0 ~]$ openstack endpoint list | grep object-store
| 6c7244cc8928448d88ebfad864fdd5ca | regionOne | swift    	| object-store   | True	| internal  | http://172.17.3.79:8080/swift/v1/AUTH_%(project_id)s |
| 95596a2d92c74c15b83325a11a4f07a3 | regionOne | swift    	| object-store   | True	| public	| http://10.0.0.89:8080/swift/v1/AUTH_%(project_id)s   |
| e6d0599c5bf24a0fb1ddf6ecac00de2d | regionOne | swift    	| object-store   | True	| admin 	| http://172.17.3.79:8080/swift/v1/AUTH_%(project_id)s |</pre>
</div>
</div>
<div class="paragraph">
<p>Repeat this step for both internal and admin endpoints.</p>
</div>
</li>
<li>
<p>Test the migrated service:</p>
<div class="listingblock">
<div class="content">
<pre>(overcloud) [stack@undercloud-0 ~]$ swift list --debug

DEBUG:swiftclient:Versionless auth_url - using http://10.0.0.115:5000/v3 as endpoint
DEBUG:keystoneclient.auth.identity.v3.base:Making authentication request to http://10.0.0.115:5000/v3/auth/tokens
DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): 10.0.0.115:5000
DEBUG:urllib3.connectionpool:http://10.0.0.115:5000 "POST /v3/auth/tokens HTTP/1.1" 201 7795
DEBUG:keystoneclient.auth.identity.v3.base:{"token": {"methods": ["password"], "user": {"domain": {"id": "default", "name": "Default"}, "id": "6f87c7ffdddf463bbc633980cfd02bb3", "name": "admin", "password_expires_at": null},


...
...
...

DEBUG:swiftclient:REQ: curl -i http://10.0.0.89:8080/swift/v1/AUTH_852f24425bb54fa896476af48cbe35d3?format=json -X GET -H "X-Auth-Token: gAAAAABj7KHdjZ95syP4c8v5a2zfXckPwxFQZYg0pgWR42JnUs83CcKhYGY6PFNF5Cg5g2WuiYwMIXHm8xftyWf08zwTycJLLMeEwoxLkcByXPZr7kT92ApT-36wTfpi-zbYXd1tI5R00xtAzDjO3RH1kmeLXDgIQEVp0jMRAxoVH4zb-DVHUos" -H "Accept-Encoding: gzip"
DEBUG:swiftclient:RESP STATUS: 200 OK
DEBUG:swiftclient:RESP HEADERS: {'content-length': '2', 'x-timestamp': '1676452317.72866', 'x-account-container-count': '0', 'x-account-object-count': '0', 'x-account-bytes-used': '0', 'x-account-bytes-used-actual': '0', 'x-account-storage-policy-default-placement-container-count': '0', 'x-account-storage-policy-default-placement-object-count': '0', 'x-account-storage-policy-default-placement-bytes-used': '0', 'x-account-storage-policy-default-placement-bytes-used-actual': '0', 'x-trans-id': 'tx00000765c4b04f1130018-0063eca1dd-1dcba-default', 'x-openstack-request-id': 'tx00000765c4b04f1130018-0063eca1dd-1dcba-default', 'accept-ranges': 'bytes', 'content-type': 'application/json; charset=utf-8', 'date': 'Wed, 15 Feb 2023 09:11:57 GMT'}
DEBUG:swiftclient:RESP BODY: b'[]'</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
</div>
<div class="sect2">
<h3 id="migrating-ceph-rbd_migrating-ceph-monitoring">Migrating Red Hat Ceph Storage RBD to external RHEL nodes</h3>
<div class="paragraph">
<p>For Hyperconverged Infrastructure (HCI) or dedicated Storage nodes that are
running Red Hat Ceph Storage 7 or later, you must migrate the daemons that are
included in the Red&#160;Hat OpenStack Platform control plane into the existing external Red
Hat Enterprise Linux (RHEL) nodes. The external RHEL nodes typically include
the Compute nodes for an HCI environment or dedicated storage nodes.</p>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>Complete the tasks in your Red&#160;Hat OpenStack Platform 17.1 environment. For more information, see <a href="#red-hat-ceph-storage-prerequisites_configuring-network">Red Hat Ceph Storage prerequisites</a>.</p>
</li>
</ul>
</div>
<div class="sect3">
<h4 id="migrating-ceph-mgr-daemons-to-ceph-nodes_migrating-ceph-rbd">Migrating Ceph Manager daemons to Red Hat Ceph Storage nodes</h4>
<div class="paragraph">
<p>You must migrate your Ceph Manager daemons from the Red&#160;Hat OpenStack Platform (RHOSP) Controller nodes to a set of target nodes. Target nodes are either existing Red Hat Ceph Storage nodes, or RHOSP Compute nodes if Red Hat Ceph Storage is deployed by director with a Hyperconverged Infrastructure (HCI) topology.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
The following procedure uses <code>cephadm</code> and the Ceph Orchestrator to drive the Ceph Manager migration, and the Ceph spec to modify the placement and reschedule the Ceph Manager daemons. Ceph Manager is run in an active/passive state. It also provides many modules, including the Ceph Orchestrator. Every potential module, such as the Ceph Dashboard, that is provided by <code>ceph-mgr</code> is implicitly migrated with Ceph Manager.
</td>
</tr>
</table>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>SSH into the target node and enable the firewall rules that are required to reach a Ceph Manager service:</p>
<div class="listingblock">
<div class="content">
<pre>dports="6800:7300"
ssh heat-admin@&lt;target_node&gt; sudo iptables -I INPUT \
    -p tcp --match multiport --dports $dports -j ACCEPT;</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Replace <code>&lt;target_node&gt;</code> with the hostname of the hosts that are listed in the Red Hat Ceph Storage environment. Run <code>ceph orch host ls</code> to see the list of the hosts.</p>
<div class="paragraph">
<p>Repeat this step for each target node.</p>
</div>
</li>
</ul>
</div>
</li>
<li>
<p>Check that the rules are properly applied to the target node and persist them:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo iptables-save
$ sudo systemctl restart iptables</pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
The default dashboard port for <code>ceph-mgr</code> in a greenfield deployment is 8443. With director-deployed Red Hat Ceph Storage, the default port is 8444 because the service ran on the Controller node, and it was necessary to use this port to avoid a conflict. For adoption, update the dashboard port to 8443 in the <code>ceph-mgr</code> configuration and firewall rules.
</td>
</tr>
</table>
</div>
</li>
<li>
<p>Log in to <code>controller-0</code> and update the dashboard port in the <code>ceph-mgr</code> configuration to 8443:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo cephadm shell
$ ceph config set mgr mgr/dashboard/server_port 8443
$ ceph config set mgr mgr/dashboard/ssl_server_port 8443
$ ceph mgr module disable dashboard
$ ceph mgr module enable dashboard</pre>
</div>
</div>
</li>
<li>
<p>If <code>nftables</code> is used in the existing deployment, edit <code>/etc/nftables/tripleo-rules.nft</code>
and add the following content:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml"># 113 ceph_mgr {'dport': ['6800-7300', 8443]}
add rule inet filter TRIPLEO_INPUT tcp dport { 6800-7300,8443 } ct state new counter accept comment "113 ceph_mgr"</code></pre>
</div>
</div>
</li>
<li>
<p>Save the file.</p>
</li>
<li>
<p>Restart the <code>nftables</code> service:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo systemctl restart nftables</pre>
</div>
</div>
</li>
<li>
<p>Verify that the rules are applied:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo nft list ruleset | grep ceph_mgr</pre>
</div>
</div>
</li>
<li>
<p>Prepare the target node to host the new Ceph Manager daemon, and add the <code>mgr</code>
label to the target node:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo cephadm shell -- ceph orch host label add &lt;target_node&gt; mgr</pre>
</div>
</div>
</li>
<li>
<p>Repeat steps 1-7 for each target node that hosts a Ceph Manager daemon.</p>
</li>
<li>
<p>Get the Ceph Manager spec:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">$ SPEC_DIR=${SPEC_DIR:-"$PWD/ceph_specs"}
$ mkdir -p ${SPEC_DIR}
$ sudo cephadm shell -- ceph orch ls --export mgr &gt; ${SPEC_DIR}/mgr</code></pre>
</div>
</div>
</li>
<li>
<p>Edit the retrieved spec and add the <code>label: mgr</code> section to the <code>placement</code>
section:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">service_type: mgr
service_id: mgr
placement:
  label: mgr</code></pre>
</div>
</div>
</li>
<li>
<p>Save the spec.</p>
</li>
<li>
<p>Apply the spec with <code>cephadm</code> by using the Ceph Orchestrator:</p>
<div class="listingblock">
<div class="content">
<pre>$ SPEC_DIR=${SPEC_DIR:-"$PWD/ceph_specs"}
$ sudo cephadm shell -m ${SPEC_DIR}/mgr -- ceph orch apply -i /mnt/mgr</pre>
</div>
</div>
</li>
</ol>
</div>
<div class="olist arabic">
<div class="title">Verification</div>
<ol class="arabic">
<li>
<p>Verify that the new Ceph Manager daemons are created in the target nodes:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo cephadm shell -- ceph orch ps | grep -i mgr
$ sudo cephadm shell -- ceph -s</pre>
</div>
</div>
<div class="paragraph">
<p>The Ceph Manager daemon count should match the number of hosts where the <code>mgr</code> label is added.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
The migration does not shrink the Ceph Manager daemons. The count grows by
the number of target nodes, and migrating Ceph Monitor daemons to Red Hat Ceph Storage nodes
decommissions the stand-by Ceph Manager instances. For more information, see
<a href="#migrating-mon-from-controller-nodes_migrating-ceph-rbd">Migrating Ceph Monitor daemons to Red Hat Ceph Storage nodes</a>.
</td>
</tr>
</table>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="migrating-mon-from-controller-nodes_migrating-ceph-rbd">Migrating Ceph Monitor daemons to Red Hat Ceph Storage nodes</h4>
<div class="paragraph">
<p>You must move Ceph Monitor daemons from the Red&#160;Hat OpenStack Platform (RHOSP) Controller nodes to a set of target nodes. Target nodes are either existing Red Hat Ceph Storage nodes, or RHOSP Compute nodes if Red Hat Ceph Storage is
deployed by director with a Hyperconverged Infrastructure (HCI) topology. Additional Ceph Monitors are deployed to the target nodes, and they are promoted as <code>_admin</code> nodes that you can use to manage the Red Hat Ceph Storage cluster and perform day 2 operations.</p>
</div>
<div class="paragraph">
<p>To migrate the Ceph Monitor daemons, you must perform the following high-level steps:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><a href="#configuring-target-nodes-for-ceph-monitor-migration_migrating-ceph-mon">Configure the target nodes for Ceph Monitor migration</a>.</p>
</li>
<li>
<p><a href="#draining-the-source-node_migrating-ceph-mon">Drain the source node</a>.</p>
</li>
<li>
<p><a href="#migrating-the-ceph-monitor-ip-address_migrating-ceph-mon">Migrate your Ceph Monitor IP addresses to the target nodes</a>.</p>
</li>
<li>
<p><a href="#redeploying-a-ceph-monitor-on-the-target-node_migrating-ceph-mon">Redeploy the Ceph Monitor on the target node</a>.</p>
</li>
<li>
<p><a href="#verifying-the-cluster-after-ceph-mon-migration_migrating-ceph-mon">Verify that the Red Hat Ceph Storage cluster is healthy</a>.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>Repeat these steps for any additional Controller node that hosts a Ceph Monitor until you migrate all the Ceph Monitor daemons to the target nodes.</p>
</div>
<div class="sect4">
<h5 id="configuring-target-nodes-for-ceph-monitor-migration_migrating-ceph-mon">Configuring target nodes for Ceph Monitor migration</h5>
<div class="paragraph">
<p>Prepare the target Red Hat Ceph Storage nodes for the Ceph Monitor migration by performing the following actions:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Enable firewall rules in a target node and persist them.</p>
</li>
<li>
<p>Create a spec that is based on labels and apply it by using <code>cephadm</code>.</p>
</li>
<li>
<p>Ensure that the Ceph Monitor quorum is maintained during the migration process.</p>
</li>
</ol>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>SSH into the target node and enable the firewall rules that are required to
reach a Ceph Monitor service:</p>
<div class="listingblock">
<div class="content">
<pre>$ for port in 3300 6789; {
    ssh heat-admin@&lt;target_node&gt; sudo iptables -I INPUT \
    -p tcp -m tcp --dport $port -m conntrack --ctstate NEW \
    -j ACCEPT;
}</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Replace <code>&lt;target_node&gt;</code> with the hostname of the node that hosts the new Ceph Monitor.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Check that the rules are properly applied to the target node and persist them:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo iptables-save
$ sudo systemctl restart iptables</pre>
</div>
</div>
</li>
<li>
<p>If <code>nftables</code> is used in the existing deployment, edit <code>/etc/nftables/tripleo-rules.nft</code>
and add the following content:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml"># 110 ceph_mon {'dport': [6789, 3300, '9100']}
add rule inet filter TRIPLEO_INPUT tcp dport { 6789,3300,9100 } ct state new counter accept comment "110 ceph_mon"</code></pre>
</div>
</div>
</li>
<li>
<p>Save the file.</p>
</li>
<li>
<p>Restart the <code>nftables</code> service:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo systemctl restart nftables</pre>
</div>
</div>
</li>
<li>
<p>Verify that the rules are applied:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo nft list ruleset | grep ceph_mon</pre>
</div>
</div>
</li>
<li>
<p>To migrate the existing Ceph Monitors to the target Red Hat Ceph Storage nodes, retrieve the Red Hat Ceph Storage mon spec from the first Ceph Monitor, or the first Controller node:</p>
<div class="listingblock">
<div class="content">
<pre>$ SPEC_DIR=${SPEC_DIR:-"$PWD/ceph_specs"}
$ mkdir -p ${SPEC_DIR}
$ sudo cephadm shell -- ceph orch ls --export mon &gt; ${SPEC_DIR}/mon</pre>
</div>
</div>
</li>
<li>
<p>Add the <code>label:mon</code> section to the <code>placement</code> section:</p>
<div class="listingblock">
<div class="content">
<pre>service_type: mon
service_id: mon
placement:
  label: mon</pre>
</div>
</div>
</li>
<li>
<p>Save the spec.</p>
</li>
<li>
<p>Apply the spec with <code>cephadm</code> by using the Ceph Orchestrator:</p>
<div class="listingblock">
<div class="content">
<pre>$ SPEC_DIR=${SPEC_DIR:-"$PWD/ceph_specs"}
$ sudo cephadm shell -m ${SPEC_DIR}/mon -- ceph orch apply -i /mnt/mon</pre>
</div>
</div>
</li>
<li>
<p>Extend the <code>mon</code> label to the remaining Red Hat Ceph Storage target nodes to ensure that
quorum is maintained during the migration process:</p>
<div class="listingblock">
<div class="content">
<pre>for item in $(sudo cephadm shell --  ceph orch host ls --format json | jq -r '.[].hostname'); do
    sudo cephadm shell -- ceph orch host label add  $item mon;
    sudo cephadm shell -- ceph orch host label add  $item _admin;
done</pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Applying the <code>mon</code> spec allows the existing strategy to use <code>labels</code> instead of <code>hosts</code>.
As a result, any node with the <code>mon</code> label can host a Ceph Monitor daemon.
Perform this step only once to avoid multiple iterations when multiple Ceph Monitors are migrated.
</td>
</tr>
</table>
</div>
</li>
<li>
<p>Check the status of the Red Hat Ceph Storage and the Ceph Orchestrator daemons list.
Ensure that Ceph Monitors are in a quorum and listed by the <code>ceph orch</code> command:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo cephadm shell -- ceph -s
  cluster:
    id:     f6ec3ebe-26f7-56c8-985d-eb974e8e08e3
    health: HEALTH_OK

  services:
    mon: 6 daemons, quorum controller-0,controller-1,controller-2,ceph-0,ceph-1,ceph-2 (age 19m)
    mgr: controller-0.xzgtvo(active, since 32m), standbys: controller-1.mtxohd, controller-2.ahrgsk
    osd: 8 osds: 8 up (since 12m), 8 in (since 18m); 1 remapped pgs

  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   43 MiB used, 400 GiB / 400 GiB avail
    pgs:     1 active+clean</pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre>$ sudo cephadm shell -- ceph orch host ls
HOST              ADDR           LABELS          STATUS
ceph-0        192.168.24.14  osd mon mgr _admin
ceph-1        192.168.24.7   osd mon mgr _admin
ceph-2        192.168.24.8   osd mon mgr _admin
controller-0  192.168.24.15  _admin mgr mon
controller-1  192.168.24.23  _admin mgr mon
controller-2  192.168.24.13  _admin mgr mon</pre>
</div>
</div>
</li>
<li>
<p>Set up a Ceph client on the first Controller node that is used during the rest
of the procedure to interact with Red Hat Ceph Storage. Set up an additional IP address on the
storage network that is used to interact with Red Hat Ceph Storage when the first Controller
node is decommissioned:</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>Back up the content of <code>/etc/ceph</code> in the <code>ceph_client_backup</code> directory.</p>
<div class="listingblock">
<div class="content">
<pre>$ mkdir -p $HOME/ceph_client_backup
$ sudo cp -R /etc/ceph/* $HOME/ceph_client_backup</pre>
</div>
</div>
</li>
<li>
<p>Edit <code>/etc/os-net-config/config.yaml</code> and add <code>- ip_netmask: 172.17.3.200</code>
after the IP address on the VLAN that belongs to the storage network. Replace
<code>172.17.3.200</code> with any other available IP address on the storage network
that can be statically assigned to <code>controller-0</code>.</p>
</li>
<li>
<p>Save the file and refresh the <code>controller-0</code> network configuration:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo os-net-config -c /etc/os-net-config/config.yaml</pre>
</div>
</div>
</li>
<li>
<p>Verify that the IP address is present in the Controller node:</p>
<div class="listingblock">
<div class="content">
<pre>$ ip -o a | grep 172.17.3.200</pre>
</div>
</div>
</li>
<li>
<p>Ping the IP address and confirm that it is reachable:</p>
<div class="listingblock">
<div class="content">
<pre>$ ping -c 3 172.17.3.200</pre>
</div>
</div>
</li>
<li>
<p>Verify that you can interact with the Red Hat Ceph Storage cluster:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo cephadm shell -c $HOME/ceph_client_backup/ceph.conf -k $HOME/ceph_client_backup/ceph.client.admin.keyring -- ceph -s</pre>
</div>
</div>
</li>
</ol>
</div>
</li>
</ol>
</div>
<div class="paragraph">
<div class="title">Next steps</div>
<p>Proceed to the next step <a href="#draining-the-source-node_migrating-ceph-mon">Draining the source node</a>.</p>
</div>
</div>
<div class="sect4">
<h5 id="draining-the-source-node_migrating-ceph-mon">Draining the source node</h5>
<div class="paragraph">
<p>Drain the source node and remove the source node host from the Red Hat Ceph Storage cluster.</p>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>On the source node, back up the <code>/etc/ceph/</code> directory to run <code>cephadm</code> and get a shell for the Red Hat Ceph Storage cluster from the source node:</p>
<div class="listingblock">
<div class="content">
<pre>$ mkdir -p $HOME/ceph_client_backup
$ sudo cp -R /etc/ceph $HOME/ceph_client_backup</pre>
</div>
</div>
</li>
<li>
<p>Identify the active <code>ceph-mgr</code> instance:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo cephadm shell -- ceph mgr stat</pre>
</div>
</div>
</li>
<li>
<p>Fail the <code>ceph-mgr</code> if it is active on the source node:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo cephadm shell -- ceph mgr fail &lt;mgr_instance&gt;</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Replace <code>&lt;mgr_instance&gt;</code> with the Ceph Manager daemon to fail.</p>
</li>
</ul>
</div>
</li>
<li>
<p>From the <code>cephadm</code> shell, remove the labels on the source node:</p>
<div class="listingblock">
<div class="content">
<pre>$ for label in mon mgr _admin; do
    sudo cephadm shell -- ceph orch host label rm &lt;source_node&gt; $label;
done</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Replace <code>&lt;source_node&gt;</code> with the hostname of the source node.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Optional: Ensure that you remove the Ceph Monitor daemon from the source node if it is still running:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo cephadm shell -- ceph orch daemon rm mon.&lt;source_node&gt; --force</pre>
</div>
</div>
</li>
<li>
<p>Drain the source node to remove any leftover daemons:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo cephadm shell -- ceph orch host drain &lt;source_node&gt;</pre>
</div>
</div>
</li>
<li>
<p>Remove the source node host from the Red Hat Ceph Storage cluster:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo cephadm shell -- ceph orch host rm &lt;source_node&gt; --force</pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>The source node is not part of the cluster anymore, and should not appear in
the Red Hat Ceph Storage host list when you run <code>sudo cephadm shell -- ceph orch host ls</code>.
However, if you run <code>sudo podman ps</code> in the source node, the list might show
that both Ceph Monitors and Ceph Managers are still running.</p>
</div>
<div class="listingblock">
<div class="content">
<pre>[root@controller-1 ~]# sudo podman ps
CONTAINER ID  IMAGE                                                                                        COMMAND               CREATED         STATUS             PORTS       NAMES
5c1ad36472bc  registry.redhat.io/ceph/rhceph@sha256:320c364dcc8fc8120e2a42f54eb39ecdba12401a2546763b7bef15b02ce93bc4  -n mon.contro...  35 minutes ago  Up 35 minutes ago              ceph-f6ec3ebe-26f7-56c8-985d-eb974e8e08e3-mon-controller-1
3b14cc7bf4dd  registry.redhat.io/ceph/rhceph@sha256:320c364dcc8fc8120e2a42f54eb39ecdba12401a2546763b7bef15b02ce93bc4  -n mgr.contro...  35 minutes ago  Up 35 minutes ago              ceph-f6ec3ebe-26f7-56c8-985d-eb974e8e08e3-mgr-controller-1-mtxohd</pre>
</div>
</div>
<div class="paragraph">
<p>To clean up the existing containers and remove the <code>cephadm</code> data from the source node, contact Red Hat Support.</p>
</div>
</td>
</tr>
</table>
</div>
</li>
<li>
<p>Confirm that mons are still in quorum:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo cephadm shell -- ceph -s
$ sudo cephadm shell -- ceph orch ps | grep -i mon</pre>
</div>
</div>
</li>
</ol>
</div>
<div class="paragraph">
<div class="title">Next steps</div>
<p>Proceed to the next step <a href="#migrating-the-ceph-monitor-ip-address_migrating-ceph-mon">Migrating the Ceph Monitor IP address</a>.</p>
</div>
</div>
<div class="sect4">
<h5 id="migrating-the-ceph-monitor-ip-address_migrating-ceph-mon">Migrating the Ceph Monitor IP address</h5>
<div class="paragraph">
<p>You must migrate your Ceph Monitor IP addresses to the target Red Hat Ceph Storage nodes. The
IP address migration assumes that the target nodes are originally deployed by
director and that the network configuration is managed by
<code>os-net-config</code>.</p>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Get the original Ceph Monitor IP addresses from <code>$HOME/ceph_client_backup/ceph.conf</code> file on the <code>mon_host</code> line, for example:</p>
<div class="listingblock">
<div class="content">
<pre>mon_host = [v2:172.17.3.60:3300/0,v1:172.17.3.60:6789/0] [v2:172.17.3.29:3300/0,v1:172.17.3.29:6789/0] [v2:172.17.3.53:3300/0,v1:172.17.3.53:6789/0]</pre>
</div>
</div>
</li>
<li>
<p>Match the IP address retrieved in the previous step with the storage network IP addresses on the source node, and find the Ceph Monitor IP address:</p>
<div class="listingblock">
<div class="content">
<pre>[tripleo-admin@controller-0 ~]$ ip -o -4 a | grep 172.17.3
9: vlan30    inet 172.17.3.60/24 brd 172.17.3.255 scope global vlan30\       valid_lft forever preferred_lft forever
9: vlan30    inet 172.17.3.13/32 brd 172.17.3.255 scope global vlan30\       valid_lft forever preferred_lft forever</pre>
</div>
</div>
</li>
<li>
<p>Confirm that the Ceph Monitor IP address is present in the <code>os-net-config</code> configuration that is located in the <code>/etc/os-net-config</code> directory on the source node:</p>
<div class="listingblock">
<div class="content">
<pre>[tripleo-admin@controller-0 ~]$ grep "172.17.3.60" /etc/os-net-config/config.yaml
    - ip_netmask: 172.17.3.60/24</pre>
</div>
</div>
</li>
<li>
<p>Edit the <code>/etc/os-net-config/config.yaml</code> file and remove the <code>ip_netmask</code> line.</p>
</li>
<li>
<p>Save the file and refresh the node network configuration:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo os-net-config -c /etc/os-net-config/config.yaml</pre>
</div>
</div>
</li>
<li>
<p>Verify that the IP address is not present in the source node anymore, for example:</p>
<div class="listingblock">
<div class="content">
<pre>[controller-0]$ ip -o a | grep 172.17.3.60</pre>
</div>
</div>
</li>
<li>
<p>SSH into the target node, for example <code>cephstorage-0</code>, and add the IP address
for the new Ceph Monitor.</p>
</li>
<li>
<p>On the target node, edit <code>/etc/os-net-config/config.yaml</code> and
add the <code>- ip_netmask: 172.17.3.60</code> line that you removed in the source node.</p>
</li>
<li>
<p>Save the file and refresh the node network configuration:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo os-net-config -c /etc/os-net-config/config.yaml</pre>
</div>
</div>
</li>
<li>
<p>Verify that the IP address is present in the target node.</p>
<div class="listingblock">
<div class="content">
<pre>$ ip -o a | grep 172.17.3.60</pre>
</div>
</div>
</li>
<li>
<p>From the Ceph client node, <code>controller-0</code>, ping the IP address that is
migrated to the target node and confirm that it is still reachable:</p>
<div class="listingblock">
<div class="content">
<pre>[controller-0]$ ping -c 3 172.17.3.60</pre>
</div>
</div>
</li>
</ol>
</div>
<div class="paragraph">
<div class="title">Next steps</div>
<p>Proceed to the next step <a href="#redeploying-a-ceph-monitor-on-the-target-node_migrating-ceph-mon">Redeploying the Ceph Monitor on the target node</a>.</p>
</div>
</div>
<div class="sect4">
<h5 id="redeploying-a-ceph-monitor-on-the-target-node_migrating-ceph-mon">Redeploying a Ceph Monitor on the target node</h5>
<div class="paragraph">
<p>You use the IP address that you migrated to the target node to redeploy the
Ceph Monitor on the target node.</p>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>From the Ceph client node, for example <code>controller-0</code>, get the Ceph mon spec:</p>
<div class="listingblock">
<div class="content">
<pre>$ SPEC_DIR=${SPEC_DIR:-"$PWD/ceph_specs"}
$ sudo cephadm shell -- ceph orch ls --export mon &gt; ${SPEC_DIR}/mon</pre>
</div>
</div>
</li>
<li>
<p>Edit the retrieved spec and add the <code>unmanaged: true</code> keyword:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">service_type: mon
service_id: mon
placement:
  label: mon
unmanaged: true</code></pre>
</div>
</div>
</li>
<li>
<p>Save the spec.</p>
</li>
<li>
<p>Apply the spec with <code>cephadm</code> by using the Ceph Orchestrator:</p>
<div class="listingblock">
<div class="content">
<pre>$ SPEC_DIR=${SPEC_DIR:-"$PWD/ceph_specs"}
$ sudo cephadm shell -m ${SPEC_DIR}/mon -- ceph orch apply -i /mnt/mon</pre>
</div>
</div>
<div class="paragraph">
<p>The Ceph Monitor daemons are marked as <code>unmanaged</code>, and you can now redeploy the existing daemon and bind it to the migrated IP address.</p>
</div>
</li>
<li>
<p>Delete the existing Ceph Monitor on the target node:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo cephadm shell -- ceph orch daemon rm mon.&lt;target_node&gt; --force</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Replace <code>&lt;target_node&gt;</code> with the hostname of the target node that is included in the Red Hat Ceph Storage cluster.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Redeploy the new Ceph Monitor on the target node by using the migrated IP address:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo cephadm shell -- ceph orch daemon add mon &lt;target_node&gt;:&lt;ip_address&gt;</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Replace <code>&lt;ip_address&gt;</code> with the IP address of the migrated IP address.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Get the Ceph Monitor spec:</p>
<div class="listingblock">
<div class="content">
<pre>$ SPEC_DIR=${SPEC_DIR:-"$PWD/ceph_specs"}
$ sudo cephadm shell -- ceph orch ls --export mon &gt; ${SPEC_DIR}/mon</pre>
</div>
</div>
</li>
<li>
<p>Edit the retrieved spec and set the <code>unmanaged</code> keyword to <code>false</code>:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">service_type: mon
service_id: mon
placement:
  label: mon
unmanaged: false</code></pre>
</div>
</div>
</li>
<li>
<p>Save the spec.</p>
</li>
<li>
<p>Apply the spec with <code>cephadm</code> by using the Ceph Orchestrator:</p>
<div class="listingblock">
<div class="content">
<pre>$ SPEC_DIR=${SPEC_DIR:-"$PWD/ceph_specs"}
$ sudo cephadm shell -m ${SPEC_DIR}/mon -- ceph orch apply -i /mnt/mon</pre>
</div>
</div>
<div class="paragraph">
<p>The new Ceph Monitor runs on the target node with the original IP address.</p>
</div>
</li>
<li>
<p>Identify the running <code>mgr</code>:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo cephadm shell -- ceph mgr stat</pre>
</div>
</div>
</li>
<li>
<p>Refresh the Ceph Manager information by force-failing it:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo cephadm shell -- ceph mgr fail</pre>
</div>
</div>
</li>
<li>
<p>Refresh the <code>OSD</code> information:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo cephadm shell -- ceph orch reconfig osd.default_drive_group</pre>
</div>
</div>
</li>
</ol>
</div>
<div class="paragraph">
<div class="title">Next steps</div>
<p>Repeat the procedure starting from step <a href="#draining-the-source-node_migrating-ceph-mon">Draining the source node</a> for each node that you want to decommission.
Proceed to the next step <a href="#verifying-the-cluster-after-ceph-mon-migration_migrating-ceph-mon">Verifying the Red Hat Ceph Storage cluster after Ceph Monitor migration</a>.</p>
</div>
</div>
<div class="sect4">
<h5 id="verifying-the-cluster-after-ceph-mon-migration_migrating-ceph-mon">Verifying the Red Hat Ceph Storage cluster after Ceph Monitor migration</h5>
<div class="paragraph">
<p>After you finish migrating your Ceph Monitor daemons to the target nodes, verify that the the Red Hat Ceph Storage cluster is healthy.</p>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Verify that the Red Hat Ceph Storage cluster is healthy:</p>
<div class="listingblock">
<div class="content">
<pre>$ ceph -s
  cluster:
    id:     f6ec3ebe-26f7-56c8-985d-eb974e8e08e3
    health: HEALTH_OK
...
...</pre>
</div>
</div>
</li>
<li>
<p>Verify that the Red Hat Ceph Storage mons are running with the old IP addresses. SSH
into the target nodes and verify that the Ceph Monitor daemons are bound to
the expected IP and port:</p>
<div class="listingblock">
<div class="content">
<pre>$ netstat -tulpn | grep 3300</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="updating-the-cluster-dashboard-configuration_migrating-ceph-rbd">Updating the Red Hat Ceph Storage cluster Ceph Dashboard configuration</h3>
<div class="paragraph">
<p>If the Ceph Dashboard is part of the enabled Ceph Manager modules, you need to
reconfigure the failover settings.</p>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Regenerate the following Red Hat Ceph Storage configuration keys to point to the right
<code>mgr</code> container:</p>
<div class="listingblock">
<div class="content">
<pre>mgr    advanced  mgr/dashboard/controller-0.ycokob/server_addr  172.17.3.33
mgr    advanced  mgr/dashboard/controller-1.lmzpuc/server_addr  172.17.3.147
mgr    advanced  mgr/dashboard/controller-2.xpdgfl/server_addr  172.17.3.138</pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre>$ sudo cephadm shell
$ ceph orch ps | awk '/mgr./ {print $1}'</pre>
</div>
</div>
</li>
<li>
<p>For each retrieved <code>mgr</code> daemon, update the corresponding entry in the Red Hat Ceph Storage
configuration:</p>
<div class="listingblock">
<div class="content">
<pre>$ ceph config set mgr mgr/dashboard/&lt;&gt;/server_addr/&lt;ip addr&gt;</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
</div>
</div>
</div>
<div id="footer">
<div id="footer-text">
Last updated 2025-06-11 20:26:30 UTC
</div>
</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.3/highlight.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.3/languages/yaml.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.3/languages/bash.min.js"></script>
<script>
if (!hljs.initHighlighting.called) {
  hljs.initHighlighting.called = true
  ;[].slice.call(document.querySelectorAll('pre.highlight > code[data-lang]')).forEach(function (el) { hljs.highlightBlock(el) })
}
</script>
</body>
</html>